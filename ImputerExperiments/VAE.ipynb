{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielketron/tpot2_addimputers/env2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted, _check_feature_names_in\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn\n",
    "import sklearn.impute\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import sklearn.compose\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import optimize\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted, _check_feature_names_in\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn\n",
    "import sklearn.impute\n",
    "from torch.autograd.variable import Variable\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import sklearn.compose\n",
    "import math\n",
    "import torch\n",
    "import sklearn.preprocessing\n",
    "import openml\n",
    "import tpot2\n",
    "import sklearn.metrics\n",
    "import sklearn\n",
    "from sklearn.metrics import (roc_auc_score, roc_curve, precision_score, auc, recall_score, precision_recall_curve, \\\n",
    "                             roc_auc_score, accuracy_score, balanced_accuracy_score, f1_score, log_loss,\n",
    "                             f1_score, root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing(X, add_missing = 0.05, missing_type = 'MAR'):\n",
    "    if isinstance(X,np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    missing_mask = X\n",
    "    missing_mask = missing_mask.mask(missing_mask.isna(), True)\n",
    "    missing_mask = missing_mask.mask(missing_mask.notna(), False)\n",
    "    X = X.mask(X.isna(), 0)\n",
    "    T = torch.tensor(X.to_numpy())\n",
    "\n",
    "    match missing_type:\n",
    "        case 'MAR':\n",
    "            out = MAR(T, [add_missing])\n",
    "        case 'MCAR':\n",
    "            out = MCAR(T, [add_missing])\n",
    "        case 'MNAR':\n",
    "            out = MNAR_mask_logistic(T, [add_missing])\n",
    "    \n",
    "    masked_set = pd.DataFrame(out['Mask'].numpy())\n",
    "    missing_combo = (missing_mask | masked_set.isna())\n",
    "    masked_set = masked_set.mask(missing_combo, True)\n",
    "    masked_set.columns = X.columns.values\n",
    "    #masked_set = masked_set.to_numpy()\n",
    "\n",
    "    missing_set = pd.DataFrame(out['Missing'].numpy())\n",
    "    missing_set.columns = X.columns.values\n",
    "    #missing_set = missing_set.to_numpy()\n",
    "\n",
    "    return missing_set, masked_set\n",
    "\n",
    "\"\"\"BEYOND THIS POINT WRITTEN BY Aude Sportisse, Marine Le Morvan and Boris Muzellec - https://rmisstastic.netlify.app/how-to/python/generate_html/how%20to%20generate%20missing%20values\"\"\"\n",
    "\n",
    "def MCAR(X, p_miss):\n",
    "    out = {'X': X.double()}\n",
    "    for p in p_miss: \n",
    "        mask = (torch.rand(X.shape) < p).double()\n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "        model_name = 'Missing'\n",
    "        mask_name = 'Mask'\n",
    "        out[model_name] = X_nas\n",
    "        out[mask_name] = mask\n",
    "    return out\n",
    "\n",
    "def MAR(X,p_miss,p_obs=0.5):\n",
    "    out = {'X': X.double()}\n",
    "    for p in p_miss:\n",
    "        n, d = X.shape\n",
    "        mask = torch.zeros(n, d).bool()\n",
    "        num_no_missing = max(int(p_obs * d), 1)\n",
    "        num_missing = d - num_no_missing\n",
    "        obs_samples = np.random.choice(d, num_no_missing, replace=False)\n",
    "        copy_samples = np.array([i for i in range(d) if i not in obs_samples])\n",
    "        len_obs = len(obs_samples)\n",
    "        len_na = len(copy_samples)\n",
    "        coeffs = torch.randn(len_obs, len_na).double()\n",
    "        Wx = X[:, obs_samples].mm(coeffs)\n",
    "        coeffs /= torch.std(Wx, 0, keepdim=True)\n",
    "        coeffs.double()\n",
    "        len_obs, len_na = coeffs.shape\n",
    "        intercepts = torch.zeros(len_na)\n",
    "        for j in range(len_na):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X[:, obs_samples].mv(coeffs[:, j]) + x).mean().item() - p\n",
    "            intercepts[j] = optimize.bisect(f, -50, 50)\n",
    "        ps = torch.sigmoid(X[:, obs_samples].mm(coeffs) + intercepts)\n",
    "        ber = torch.rand(n, len_na)\n",
    "        mask[:, copy_samples] = ber < ps\n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "        model_name = 'Missing'\n",
    "        mask_name = 'Mask'\n",
    "        out[model_name] = X_nas\n",
    "        out[mask_name] = mask\n",
    "    return out\n",
    "\n",
    "def MNAR_mask_logistic(X, p_miss, p_params =.5, exclude_inputs=True):\n",
    "    \"\"\"\n",
    "    Missing not at random mechanism with a logistic masking model. It implements two mechanisms:\n",
    "    (i) Missing probabilities are selected with a logistic model, taking all variables as inputs. Hence, values that are\n",
    "    inputs can also be missing.\n",
    "    (ii) Variables are split into a set of intputs for a logistic model, and a set whose missing probabilities are\n",
    "    determined by the logistic model. Then inputs are then masked MCAR (hence, missing values from the second set will\n",
    "    depend on masked values.\n",
    "    In either case, weights are random and the intercept is selected to attain the desired proportion of missing values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.DoubleTensor or np.ndarray, shape (n, d)\n",
    "        Data for which missing values will be simulated.\n",
    "        If a numpy array is provided, it will be converted to a pytorch tensor.\n",
    "    p : float\n",
    "        Proportion of missing values to generate for variables which will have missing values.\n",
    "    p_params : float\n",
    "        Proportion of variables that will be used for the logistic masking model (only if exclude_inputs).\n",
    "    exclude_inputs : boolean, default=True\n",
    "        True: mechanism (ii) is used, False: (i)\n",
    "    Returns\n",
    "    -------\n",
    "    mask : torch.BoolTensor or np.ndarray (depending on type of X)\n",
    "        Mask of generated missing values (True if the value is missing).\n",
    "    \"\"\"\n",
    "    out = {'X_init_MNAR': X.double()}\n",
    "    for p in p_miss: \n",
    "        n, d = X.shape\n",
    "        to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "        if not to_torch:\n",
    "            X = torch.from_numpy(X)\n",
    "        mask = torch.zeros(n, d).bool() if to_torch else np.zeros((n, d)).astype(bool)\n",
    "        d_params = max(int(p_params * d), 1) if exclude_inputs else d ## number of variables used as inputs (at least 1)\n",
    "        d_na = d - d_params if exclude_inputs else d ## number of variables masked with the logistic model\n",
    "        ### Sample variables that will be parameters for the logistic regression:\n",
    "        idxs_params = np.random.choice(d, d_params, replace=False) if exclude_inputs else np.arange(d)\n",
    "        idxs_nas = np.array([i for i in range(d) if i not in idxs_params]) if exclude_inputs else np.arange(d)\n",
    "        ### Other variables will have NA proportions selected by a logistic model\n",
    "        ### The parameters of this logistic model are random.\n",
    "        ### Pick coefficients so that W^Tx has unit variance (avoids shrinking)\n",
    "        len_obs = len(idxs_params)\n",
    "        len_na = len(idxs_nas)\n",
    "        coeffs = torch.randn(len_obs, len_na).double()\n",
    "        Wx = X[:, idxs_params].mm(coeffs)\n",
    "        coeffs /= torch.std(Wx, 0, keepdim=True)\n",
    "        coeffs.double()\n",
    "        ### Pick the intercepts to have a desired amount of missing values\n",
    "        len_obs, len_na = coeffs.shape\n",
    "        intercepts = torch.zeros(len_na)\n",
    "        for j in range(len_na):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X[:, idxs_params].mv(coeffs[:, j]) + x).mean().item() - p\n",
    "            intercepts[j] = optimize.bisect(f, -50, 50)\n",
    "        ps = torch.sigmoid(X[:, idxs_params].mm(coeffs) + intercepts)\n",
    "        ber = torch.rand(n, d_na)\n",
    "        mask[:, idxs_nas] = ber < ps\n",
    "        ## If the inputs of the logistic model are excluded from MNAR missingness,\n",
    "        ## mask some values used in the logistic model at random.\n",
    "        ## This makes the missingness of other variables potentially dependent on masked values\n",
    "        if exclude_inputs:\n",
    "            mask[:, idxs_params] = torch.rand(n, d_params) < p\n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "        model_name = 'Missing'\n",
    "        mask_name = 'Mask'\n",
    "        out[model_name] = X_nas\n",
    "        out[mask_name] = mask\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/05y1vdbd3vd52x71qyhklyjw0000gn/T/ipykernel_13410/518150171.py:6: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  missing_mask = missing_mask.mask(missing_mask.notna(), False)\n"
     ]
    }
   ],
   "source": [
    "dataset_file = '/Users/gabrielketron/tpot2_addimputers/tpot2/ImputerExperiments/data/Spam.csv'\n",
    "#%% System Parameters\n",
    "# 1. Mini batch size\n",
    "mb_size = 128\n",
    "# 2. Missing rate\n",
    "p_miss = 0.2\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "# 4. Loss Hyperparameters\n",
    "alpha = 10\n",
    "# 5. Train Rate\n",
    "train_rate = 0.8\n",
    "\n",
    "#%% Data\n",
    "\n",
    "# Data generation\n",
    "Data = np.loadtxt(dataset_file, delimiter=\",\",skiprows=1)\n",
    "X = pd.DataFrame(Data)\n",
    "\n",
    "X_M, mask = add_missing(X, missing_type='MNAR')\n",
    "X_M = X_M.astype(float)\n",
    "mask = mask.astype(float)\n",
    "no, dim = X_M.shape\n",
    "idx = np.random.permutation(no)\n",
    "\n",
    "Train_No = int(no * 0.7)\n",
    "Test_No = no - Train_No\n",
    "    \n",
    "# Train / Test Features\n",
    "trainX = X_M.iloc[:Train_No]\n",
    "testX = X_M.iloc[Train_No:]\n",
    "\n",
    "# Train / Test Missing Indicators\n",
    "trainM = mask.iloc[:Train_No]\n",
    "testM = mask.iloc[Train_No:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEImputer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, iterations=1000, batch_size=128, split_size=5, code_size=5, encoder_hidden_sizes=[128, 64], decoder_hidden_sizes=[128, 64],\n",
    "                    temperature=None, p_miss = 0.2, learning_rate = 0.001, tolerance=0.001):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = iterations\n",
    "        self.split_size = split_size\n",
    "        self.code_size = code_size\n",
    "        self.encoder_hidden_sizes = encoder_hidden_sizes\n",
    "        self.decoder_hidden_sizes = decoder_hidden_sizes\n",
    "        self.test_loss_function = torch.nn.MSELoss()\n",
    "        self.p_miss = p_miss\n",
    "        self.temperature = temperature\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.variable_sizes = [1]*X.shape[1] #list of 1s the same lenght as the features of X\n",
    "        \n",
    "        self.encoder_hidden_sizes = [int(math.floor(X.shape[1]/2)), int(math.floor(X.shape[1]*3/10))]\n",
    "        self.decoder_hidden_sizes = [int(math.floor(X.shape[1]/2)), int(math.floor(X.shape[1]*3/10))]\n",
    "        self.split_size =int(math.floor(X.shape[1]/5))\n",
    "        self.code_size=int(math.floor(X.shape[1]/5))\n",
    "        \n",
    "        #print(self.encoder_hidden_sizes)\n",
    "\n",
    "        features = torch.from_numpy(X.to_numpy()) #X features\n",
    "        features = torch.nan_to_num(features)\n",
    "\n",
    "        num_samples = len(features)\n",
    "        variable_masks = []\n",
    "        for variable_size in self.variable_sizes:\n",
    "            variable_mask = (torch.zeros(num_samples, 1).uniform_(0.0, 1.0) > self.p_miss).float()\n",
    "            if variable_size > 1:\n",
    "                variable_mask = variable_mask.repeat(1, variable_size)\n",
    "            variable_masks.append(variable_mask)\n",
    "        mask = torch.cat(variable_masks, dim=1)\n",
    "\n",
    "        temperature = self.temperature\n",
    "        self.model = self.VAE(self,\n",
    "                        features.shape[1],\n",
    "                        self.split_size,\n",
    "                        self.code_size,\n",
    "                        encoder_hidden_sizes=self.encoder_hidden_sizes,\n",
    "                        decoder_hidden_sizes=self.decoder_hidden_sizes,\n",
    "                        variable_sizes=(None if temperature is None else self.variable_sizes),  # do not use multi-output without temperature\n",
    "                        temperature=temperature\n",
    "                        )\n",
    "        \n",
    "        self.model.train(mode=True)\n",
    "        inverted_mask = 1 - mask\n",
    "        observed = features * mask\n",
    "        missing = torch.randn_like(features)\n",
    "        noisy_features = observed + missing*inverted_mask\n",
    "\n",
    "        if self.learning_rate is not None:\n",
    "            missing = torch.autograd.Variable(missing, requires_grad=True)\n",
    "            self.optim = torch.optim.Adam(self.model.parameters(), weight_decay=0, lr=self.learning_rate)\n",
    "\n",
    "        self.model.train(mode=True)\n",
    "        #pbar = tqdm(range(self.iterations))\n",
    "        for iterations in range(self.iterations):\n",
    "            train_ds = TensorDataset(features.float(), mask.float(), noisy_features.float())\n",
    "            losses = []\n",
    "            for f, m, n in DataLoader(train_ds, batch_size=self.batch_size, shuffle=True):\n",
    "                loss = self.train_batch(f, m, n)\n",
    "                losses.append(loss)\n",
    "                if loss < self.tolerance:\n",
    "                    break\n",
    "            #pbar.set_postfix({'loss': min(losses)})\n",
    "            '''\n",
    "            if iterations % 100 == 0 :\n",
    "                print(f'Epoch {iterations} loss: {loss:.4f}')\n",
    "            '''\n",
    "\n",
    "        self._VAE_params = self.model.state_dict()\n",
    "        return self\n",
    "    \n",
    "    def train_batch(self, features, mask, noisy_features):\n",
    "        self.optim.zero_grad()\n",
    "        #print(features.shape)\n",
    "        #print(noisy_features.shape)\n",
    "        #noise = torch.autograd.Variable(torch.FloatTensor(len(noisy_features), self.p_miss).normal_())\n",
    "        _, reconstructed, mu, log_var = self.model(noisy_features, training=True)\n",
    "        #print(reconstructed.shape)\n",
    "        #print(reconstructed)\n",
    "        # reconstruction of the non-missing values\n",
    "        reconstruction_loss = self.masked_reconstruction_loss_function(reconstructed,\n",
    "                                                                  features,\n",
    "                                                                  mask,\n",
    "                                                                  self.variable_sizes)\n",
    "        missing_loss = self.masked_reconstruction_loss_function(reconstructed, features, 1-mask, self.variable_sizes)\n",
    "        #print(reconstruction_loss)\n",
    "        loss = torch.sqrt(self.test_loss_function((mask * features + (1.0 - mask) * reconstructed), features))\n",
    "        \n",
    "        kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        #print(kld_loss)\n",
    "        observed_loss = reconstruction_loss + kld_loss\n",
    "        #loss = loss.type(torch.float32)\n",
    "        #print(loss)\n",
    "        observed_loss.backward()\n",
    "\n",
    "        self.optim.step()\n",
    "\n",
    "        return observed_loss.detach().numpy()\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        self.model.load_state_dict(self._VAE_params)\n",
    "        self.model.train(mode=False)\n",
    "        self.variable_sizes = [1]*X.shape[1] #list of 1s the same lenght as the features of X\n",
    "\n",
    "        features = torch.from_numpy(X.to_numpy()) #X features\n",
    "        features = torch.nan_to_num(features)\n",
    "        mask = torch.from_numpy(1-np.isnan(X.to_numpy()))\n",
    "        inverted_mask = ~mask\n",
    "        num_samples = len(features)\n",
    "        observed = features * mask\n",
    "        missing = torch.randn_like(features)\n",
    "        noisy_features = observed + missing*inverted_mask\n",
    "        f = features.float()\n",
    "        m = mask.float()\n",
    "        #print(m)\n",
    "        n = noisy_features.float()\n",
    "        #print(n)\n",
    "        with torch.no_grad():\n",
    "            _, reconstructed, _, _ = self.model.forward(n)\n",
    "            #print(reconstructed)\n",
    "            imputed = m*n + (1.0 - m)*reconstructed\n",
    "        return imputed.cpu().numpy()\n",
    "\n",
    "    def reconstruction_loss_function(self, reconstructed, original, variable_sizes, reduction=\"mean\"):\n",
    "        # by default use loss for binary variables\n",
    "        if variable_sizes is None:\n",
    "            return torch.nn.functional.binary_cross_entropy(reconstructed, original, reduction=reduction)\n",
    "        # use the variable sizes when available\n",
    "        else:\n",
    "            loss = 0\n",
    "            start = 0\n",
    "            numerical_size = 0\n",
    "            for variable_size in variable_sizes:\n",
    "                # if it is a categorical variable\n",
    "                if variable_size > 1:\n",
    "                    # add loss from the accumulated continuous variables\n",
    "                    if numerical_size > 0:\n",
    "                        end = start + numerical_size\n",
    "                        batch_reconstructed_variable = reconstructed[:, start:end]\n",
    "                        batch_target = original[:, start:end]\n",
    "                        loss += torch.nn.functional.mse_loss(batch_reconstructed_variable, batch_target, reduction=reduction)\n",
    "                        start = end\n",
    "                        numerical_size = 0\n",
    "                    # add loss from categorical variable\n",
    "                    end = start + variable_size\n",
    "                    batch_reconstructed_variable = reconstructed[:, start:end]\n",
    "                    batch_target = torch.argmax(original[:, start:end], dim=1)\n",
    "                    loss += torch.nn.functional.cross_entropy(batch_reconstructed_variable, batch_target, reduction=reduction)\n",
    "                    start = end\n",
    "                # if not, accumulate numerical variables\n",
    "                else:\n",
    "                    numerical_size += 1\n",
    "\n",
    "            # add loss from the remaining accumulated numerical variables\n",
    "            if numerical_size > 0:\n",
    "                end = start + numerical_size\n",
    "                batch_reconstructed_variable = reconstructed[:, start:end]\n",
    "                batch_target = original[:, start:end]\n",
    "                loss += torch.nn.functional.mse_loss(batch_reconstructed_variable, batch_target, reduction=reduction)\n",
    "\n",
    "            return loss\n",
    "\n",
    "    def masked_reconstruction_loss_function(self, reconstructed, original, mask, variable_sizes):\n",
    "        return self.reconstruction_loss_function(mask * reconstructed,\n",
    "                                            mask * original,\n",
    "                                            variable_sizes,\n",
    "                                            reduction=\"sum\") / torch.sum(mask)\n",
    "\n",
    "    class Encoder(torch.nn.Module):\n",
    "\n",
    "        def __init__(self, VAEImputer, input_size, code_size, hidden_sizes=[], variable_sizes=None):\n",
    "            super(VAEImputer.Encoder, self).__init__()\n",
    "\n",
    "            layers = []\n",
    "\n",
    "            if variable_sizes is None:\n",
    "                previous_layer_size = input_size\n",
    "                #print(type(previous_layer_size))\n",
    "            else:\n",
    "                multi_input_layer = VAEImputer.MultiInput(VAEImputer, variable_sizes)\n",
    "                layers.append(multi_input_layer)\n",
    "                previous_layer_size = multi_input_layer.size\n",
    "                #print(type(previous_layer_size))\n",
    "\n",
    "            layer_sizes = list(hidden_sizes) + [code_size]\n",
    "            hidden_activation = torch.nn.Tanh()\n",
    "\n",
    "            for layer_size in layer_sizes:\n",
    "                #print(layer_size)\n",
    "                layers.append(torch.nn.Linear(previous_layer_size, layer_size))\n",
    "                layers.append(hidden_activation)\n",
    "                previous_layer_size = layer_size\n",
    "\n",
    "            self.hidden_layers = torch.nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            #print(inputs)\n",
    "            return self.hidden_layers(inputs)\n",
    "    \n",
    "    class Decoder(torch.nn.Module):\n",
    "\n",
    "        def __init__(self, VAEImputer, code_size, output_size, hidden_sizes=[], variable_sizes=None, temperature=None):\n",
    "            super(VAEImputer.Decoder, self).__init__()\n",
    "\n",
    "            hidden_activation = torch.nn.Tanh()\n",
    "\n",
    "            previous_layer_size = code_size\n",
    "            hidden_layers = []\n",
    "\n",
    "            for layer_size in hidden_sizes:\n",
    "                hidden_layers.append(torch.nn.Linear(previous_layer_size, layer_size))\n",
    "                hidden_layers.append(hidden_activation)\n",
    "                previous_layer_size = layer_size\n",
    "\n",
    "            if len(hidden_layers) > 0:\n",
    "                self.hidden_layers = torch.nn.Sequential(*hidden_layers)\n",
    "            else:\n",
    "                self.hidden_layers = None\n",
    "\n",
    "            if variable_sizes is None:\n",
    "                self.output_layer = VAEImputer.SingleOutput(VAEImputer, previous_layer_size, output_size, activation=torch.nn.Sigmoid())\n",
    "            else:\n",
    "                self.output_layer = VAEImputer.MultiOutput(VAEImputer, previous_layer_size, variable_sizes, temperature=temperature)\n",
    "\n",
    "        def forward(self, code, training=False):\n",
    "            if self.hidden_layers is None:\n",
    "                hidden = code\n",
    "            else:\n",
    "                hidden = self.hidden_layers(code)\n",
    "\n",
    "            return self.output_layer(hidden, training=training)\n",
    "\n",
    "    class VAE(torch.nn.Module):\n",
    "\n",
    "        def __init__(self, VAEImputer, input_size, split_size, code_size, encoder_hidden_sizes=[], decoder_hidden_sizes=[],\n",
    "                    variable_sizes=None, temperature=None):\n",
    "\n",
    "            super(VAEImputer.VAE, self).__init__()\n",
    "\n",
    "            self.encoder = VAEImputer.Encoder(VAEImputer, input_size, split_size, hidden_sizes=encoder_hidden_sizes, variable_sizes=variable_sizes)\n",
    "            self.decoder = VAEImputer.Decoder(VAEImputer, code_size, input_size, hidden_sizes=decoder_hidden_sizes, variable_sizes=variable_sizes,\n",
    "                                temperature=temperature)\n",
    "\n",
    "            self.mu_layer = torch.nn.Linear(split_size, code_size)\n",
    "            self.log_var_layer = torch.nn.Linear(split_size, code_size)\n",
    "\n",
    "        def forward(self, inputs, training=False):\n",
    "            mu, log_var = self.encode(inputs)\n",
    "            #print(mu)\n",
    "            #print(log_var)\n",
    "            code = self.reparameterize(mu, log_var)\n",
    "            #print(code.shape)\n",
    "            reconstructed = self.decode(code, training=training)\n",
    "            return code, reconstructed, mu, log_var\n",
    "\n",
    "        def encode(self, inputs):\n",
    "            outputs = self.encoder(inputs)\n",
    "            #print(outputs.shape)\n",
    "            #print(outputs)\n",
    "            return self.mu_layer(outputs), self.log_var_layer(outputs)\n",
    "\n",
    "        def decode(self, code, training=False):\n",
    "            return self.decoder(code, training=training)\n",
    "        \n",
    "        def reparameterize(self, mu, log_var):\n",
    "            std = torch.exp(0.5 * log_var)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        \n",
    "    '''\n",
    "    class OutputLayer(torch.nn.Module):\n",
    "        \"\"\"\n",
    "        This is just a simple abstract class for single and multi output layers.\n",
    "        Both need to have the same interface.\n",
    "        \"\"\"\n",
    "\n",
    "        def forward(self, hidden, training=None):\n",
    "            raise NotImplementedError\n",
    "    '''\n",
    "\n",
    "    class SingleOutput(torch.nn.Module):\n",
    "\n",
    "        def __init__(self, VAEImputer, previous_layer_size, output_size, activation=None):\n",
    "            super(VAEImputer.SingleOutput, self).__init__()\n",
    "            if activation is None:\n",
    "                self.model = torch.nn.Linear(previous_layer_size, output_size)\n",
    "            else:\n",
    "                self.model = torch.nn.Sequential(torch.nn.Linear(previous_layer_size, output_size), activation)\n",
    "\n",
    "        def forward(self, hidden, training=False):\n",
    "            return self.model(hidden)\n",
    "    \n",
    "    class MultiOutput(torch.nn.Module):\n",
    "        def __init__(self, VAEImputer, input_size, variable_sizes, temperature=None):\n",
    "            super(VAEImputer.MultiOutput, self).__init__()\n",
    "\n",
    "            self.output_layers = torch.nn.ModuleList()\n",
    "            self.output_activations = torch.nn.ModuleList()\n",
    "\n",
    "            numerical_size = 0\n",
    "            for i, variable_size in enumerate(variable_sizes):\n",
    "                # if it is a categorical variable\n",
    "                if variable_size > 1:\n",
    "                    # first create the accumulated numerical layer\n",
    "                    if numerical_size > 0:\n",
    "                        self.output_layers.append(torch.nn.Linear(input_size, numerical_size))\n",
    "                        self.output_activations.append(VAEImputer.NumericalActivation())\n",
    "                        numerical_size = 0\n",
    "                    # create the categorical layer\n",
    "                    self.output_layers.append(torch.nn.Linear(input_size, variable_size))\n",
    "                    self.output_activations.append(VAEImputer.CategoricalActivation(temperature))\n",
    "                # if not, accumulate numerical variables\n",
    "                else:\n",
    "                    numerical_size += 1\n",
    "\n",
    "            # create the remaining accumulated numerical layer\n",
    "            if numerical_size > 0:\n",
    "                self.output_layers.append(torch.nn.Linear(input_size, numerical_size))\n",
    "                self.output_activations.append(VAEImputer.NumericalActivation())\n",
    "\n",
    "        def forward(self, inputs, training=True, concat=True):\n",
    "            outputs = []\n",
    "            for output_layer, output_activation in zip(self.output_layers, self.output_activations):\n",
    "                logits = output_layer(inputs)\n",
    "                output = output_activation(logits, training=training)\n",
    "                outputs.append(output)\n",
    "\n",
    "            if concat:\n",
    "                return torch.cat(outputs, dim=1)\n",
    "            else:\n",
    "                return outputs\n",
    "\n",
    "\n",
    "    class CategoricalActivation(torch.nn.Module):\n",
    "\n",
    "        def __init__(self, VAEImputer, temperature):\n",
    "            super(VAEImputer.CategoricalActivation, self).__init__()\n",
    "\n",
    "            self.temperature = temperature\n",
    "\n",
    "        def forward(self, logits, training=True):\n",
    "            # gumbel-softmax (training and evaluation)\n",
    "            if self.temperature is not None:\n",
    "                return torch.nn.functional.gumbel_softmax(logits, hard=not training, tau=self.temperature)\n",
    "            # softmax training\n",
    "            elif training:\n",
    "                return torch.nn.functional.softmax(logits, dim=1)\n",
    "            # softmax evaluation\n",
    "            else:\n",
    "                return torch.distributions.OneHotCategorical(logits=logits).sample()\n",
    "\n",
    "\n",
    "    class NumericalActivation(torch.nn.Module):\n",
    "\n",
    "        def __init__(self, VAEImputer):\n",
    "            super(VAEImputer.NumericalActivation, self).__init__()\n",
    "\n",
    "        def forward(self, logits, training=True):\n",
    "            return torch.sigmoid(logits)\n",
    "        \n",
    "    class MultiInput(torch.nn.Module):\n",
    "\n",
    "        def __init__(self, VAEImputer, variable_sizes, min_embedding_size=2, max_embedding_size=50):\n",
    "            super(VAEImputer.MultiInput, self).__init__()\n",
    "\n",
    "            self.has_categorical = False\n",
    "            self.size = 0\n",
    "\n",
    "            embeddings = torch.nn.ParameterList()\n",
    "            for i, variable_size in enumerate(variable_sizes):\n",
    "                # if it is a numerical variable\n",
    "                if variable_size == 1:\n",
    "                    embeddings.append(None)\n",
    "                    self.size += 1\n",
    "                # if it is a categorical variable\n",
    "                else:\n",
    "                    # this is an arbitrary rule of thumb taken from several blog posts\n",
    "                    embedding_size = max(min_embedding_size, min(max_embedding_size, int(variable_size / 2)))\n",
    "\n",
    "                    # the embedding is implemented manually to be able to use one hot encoding\n",
    "                    # PyTorch embedding only accepts as input label encoding\n",
    "                    embedding = torch.nn.Parameter(data=torch.Tensor(variable_size, embedding_size).normal_(), requires_grad=True)\n",
    "\n",
    "                    embeddings.append(embedding)\n",
    "                    self.size += embedding_size\n",
    "                    self.has_categorical = True\n",
    "\n",
    "            if self.has_categorical:\n",
    "                self.variable_sizes = variable_sizes\n",
    "                self.embeddings = embeddings\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            if self.has_categorical:\n",
    "                outputs = []\n",
    "                start = 0\n",
    "                for variable_size, embedding in zip(self.variable_sizes, self.embeddings):\n",
    "                    # extract the variable\n",
    "                    end = start + variable_size\n",
    "                    variable = inputs[:, start:end]\n",
    "\n",
    "                    # numerical variable\n",
    "                    if variable_size == 1:\n",
    "                        # leave the input as it is\n",
    "                        outputs.append(variable)\n",
    "                    # categorical variable\n",
    "                    else:\n",
    "                        output = torch.matmul(variable, embedding).squeeze(1)\n",
    "                        outputs.append(output)\n",
    "\n",
    "                    # move the variable limits\n",
    "                    start = end\n",
    "\n",
    "                # concatenate all the variable outputs\n",
    "                return torch.cat(outputs, dim=1)\n",
    "            else:\n",
    "                return inputs\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot2.builtin_modules\n",
    "\n",
    "\n",
    "gain = tpot2.builtin_modules.imputer.VAEImputer(batch_size=64, iterations=1000)\n",
    "\n",
    "imputed_train = gain.fit_transform(trainX)\n",
    "imputed_test = gain.transform(testX)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(imputed_train))\n",
    "print(type(imputed_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 2.0007007615450614\n",
      "Test RMSE: 1.9998040999184423\n"
     ]
    }
   ],
   "source": [
    "def rmse_loss(ori_data, imputed_data, data_m):\n",
    "        '''Compute RMSE loss between ori_data and imputed_data\n",
    "        Args:\n",
    "            - ori_data: original data without missing values\n",
    "            - imputed_data: imputed data\n",
    "            - data_m: indicator matrix for missingness\n",
    "        Returns:\n",
    "            - rmse: Root Mean Squared Error\n",
    "        '''\n",
    "        #ori_data, norm_parameters = normalization(ori_data)\n",
    "        #imputed_data, _ = normalization(imputed_data, norm_parameters)\n",
    "        # Only for missing values\n",
    "        nominator = np.sum(((1-data_m) * ori_data - (1-data_m) * imputed_data)**2)\n",
    "        denominator = np.sum(1-data_m)\n",
    "        rmse = np.sqrt(nominator/float(denominator))\n",
    "        return rmse\n",
    "\n",
    "\n",
    "print(f'Train RMSE: {rmse_loss(ori_data=X[:Train_No].to_numpy(), imputed_data=imputed_train, data_m=trainM.to_numpy())}')\n",
    "print(f'Test RMSE: {rmse_loss(ori_data=X[Train_No:].to_numpy(), imputed_data=imputed_test, data_m=testM.to_numpy())}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
