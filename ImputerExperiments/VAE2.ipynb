{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielketron/tpot2_addimputers/env2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released 2 days ago.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted, _check_feature_names_in\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn\n",
    "import sklearn.impute\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import sklearn.compose\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import optimize\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted, _check_feature_names_in\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn\n",
    "import sklearn.impute\n",
    "from torch.autograd.variable import Variable\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import sklearn.compose\n",
    "import math\n",
    "import torch\n",
    "import sklearn.preprocessing\n",
    "import openml\n",
    "import tpot2\n",
    "import sklearn.metrics\n",
    "import sklearn\n",
    "from sklearn.metrics import (roc_auc_score, roc_curve, precision_score, auc, recall_score, precision_recall_curve, \\\n",
    "                             roc_auc_score, accuracy_score, balanced_accuracy_score, f1_score, log_loss,\n",
    "                             f1_score, root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing(X, add_missing = 0.05, missing_type = 'MAR'):\n",
    "    if isinstance(X,np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    missing_mask = X\n",
    "    missing_mask = missing_mask.mask(missing_mask.isna(), True)\n",
    "    missing_mask = missing_mask.mask(missing_mask.notna(), False)\n",
    "    X = X.mask(X.isna(), 0)\n",
    "    T = torch.tensor(X.to_numpy())\n",
    "\n",
    "    match missing_type:\n",
    "        case 'MAR':\n",
    "            out = MAR(T, [add_missing])\n",
    "        case 'MCAR':\n",
    "            out = MCAR(T, [add_missing])\n",
    "        case 'MNAR':\n",
    "            out = MNAR_mask_logistic(T, [add_missing])\n",
    "    \n",
    "    masked_set = pd.DataFrame(out['Mask'].numpy())\n",
    "    missing_combo = (missing_mask | masked_set.isna())\n",
    "    masked_set = masked_set.mask(missing_combo, True)\n",
    "    masked_set.columns = X.columns.values\n",
    "    #masked_set = masked_set.to_numpy()\n",
    "\n",
    "    missing_set = pd.DataFrame(out['Missing'].numpy())\n",
    "    missing_set.columns = X.columns.values\n",
    "    #missing_set = missing_set.to_numpy()\n",
    "\n",
    "    return missing_set, masked_set\n",
    "\n",
    "\"\"\"BEYOND THIS POINT WRITTEN BY Aude Sportisse, Marine Le Morvan and Boris Muzellec - https://rmisstastic.netlify.app/how-to/python/generate_html/how%20to%20generate%20missing%20values\"\"\"\n",
    "\n",
    "def MCAR(X, p_miss):\n",
    "    out = {'X': X.double()}\n",
    "    for p in p_miss: \n",
    "        mask = (torch.rand(X.shape) < p).double()\n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "        model_name = 'Missing'\n",
    "        mask_name = 'Mask'\n",
    "        out[model_name] = X_nas\n",
    "        out[mask_name] = mask\n",
    "    return out\n",
    "\n",
    "def MAR(X,p_miss,p_obs=0.5):\n",
    "    out = {'X': X.double()}\n",
    "    for p in p_miss:\n",
    "        n, d = X.shape\n",
    "        mask = torch.zeros(n, d).bool()\n",
    "        num_no_missing = max(int(p_obs * d), 1)\n",
    "        num_missing = d - num_no_missing\n",
    "        obs_samples = np.random.choice(d, num_no_missing, replace=False)\n",
    "        copy_samples = np.array([i for i in range(d) if i not in obs_samples])\n",
    "        len_obs = len(obs_samples)\n",
    "        len_na = len(copy_samples)\n",
    "        coeffs = torch.randn(len_obs, len_na).double()\n",
    "        Wx = X[:, obs_samples].mm(coeffs)\n",
    "        coeffs /= torch.std(Wx, 0, keepdim=True)\n",
    "        coeffs.double()\n",
    "        len_obs, len_na = coeffs.shape\n",
    "        intercepts = torch.zeros(len_na)\n",
    "        for j in range(len_na):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X[:, obs_samples].mv(coeffs[:, j]) + x).mean().item() - p\n",
    "            intercepts[j] = optimize.bisect(f, -50, 50)\n",
    "        ps = torch.sigmoid(X[:, obs_samples].mm(coeffs) + intercepts)\n",
    "        ber = torch.rand(n, len_na)\n",
    "        mask[:, copy_samples] = ber < ps\n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "        model_name = 'Missing'\n",
    "        mask_name = 'Mask'\n",
    "        out[model_name] = X_nas\n",
    "        out[mask_name] = mask\n",
    "    return out\n",
    "\n",
    "def MNAR_mask_logistic(X, p_miss, p_params =.5, exclude_inputs=True):\n",
    "    \"\"\"\n",
    "    Missing not at random mechanism with a logistic masking model. It implements two mechanisms:\n",
    "    (i) Missing probabilities are selected with a logistic model, taking all variables as inputs. Hence, values that are\n",
    "    inputs can also be missing.\n",
    "    (ii) Variables are split into a set of intputs for a logistic model, and a set whose missing probabilities are\n",
    "    determined by the logistic model. Then inputs are then masked MCAR (hence, missing values from the second set will\n",
    "    depend on masked values.\n",
    "    In either case, weights are random and the intercept is selected to attain the desired proportion of missing values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.DoubleTensor or np.ndarray, shape (n, d)\n",
    "        Data for which missing values will be simulated.\n",
    "        If a numpy array is provided, it will be converted to a pytorch tensor.\n",
    "    p : float\n",
    "        Proportion of missing values to generate for variables which will have missing values.\n",
    "    p_params : float\n",
    "        Proportion of variables that will be used for the logistic masking model (only if exclude_inputs).\n",
    "    exclude_inputs : boolean, default=True\n",
    "        True: mechanism (ii) is used, False: (i)\n",
    "    Returns\n",
    "    -------\n",
    "    mask : torch.BoolTensor or np.ndarray (depending on type of X)\n",
    "        Mask of generated missing values (True if the value is missing).\n",
    "    \"\"\"\n",
    "    out = {'X_init_MNAR': X.double()}\n",
    "    for p in p_miss: \n",
    "        n, d = X.shape\n",
    "        to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "        if not to_torch:\n",
    "            X = torch.from_numpy(X)\n",
    "        mask = torch.zeros(n, d).bool() if to_torch else np.zeros((n, d)).astype(bool)\n",
    "        d_params = max(int(p_params * d), 1) if exclude_inputs else d ## number of variables used as inputs (at least 1)\n",
    "        d_na = d - d_params if exclude_inputs else d ## number of variables masked with the logistic model\n",
    "        ### Sample variables that will be parameters for the logistic regression:\n",
    "        idxs_params = np.random.choice(d, d_params, replace=False) if exclude_inputs else np.arange(d)\n",
    "        idxs_nas = np.array([i for i in range(d) if i not in idxs_params]) if exclude_inputs else np.arange(d)\n",
    "        ### Other variables will have NA proportions selected by a logistic model\n",
    "        ### The parameters of this logistic model are random.\n",
    "        ### Pick coefficients so that W^Tx has unit variance (avoids shrinking)\n",
    "        len_obs = len(idxs_params)\n",
    "        len_na = len(idxs_nas)\n",
    "        coeffs = torch.randn(len_obs, len_na).double()\n",
    "        Wx = X[:, idxs_params].mm(coeffs)\n",
    "        coeffs /= torch.std(Wx, 0, keepdim=True)\n",
    "        coeffs.double()\n",
    "        ### Pick the intercepts to have a desired amount of missing values\n",
    "        len_obs, len_na = coeffs.shape\n",
    "        intercepts = torch.zeros(len_na)\n",
    "        for j in range(len_na):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X[:, idxs_params].mv(coeffs[:, j]) + x).mean().item() - p\n",
    "            intercepts[j] = optimize.bisect(f, -50, 50)\n",
    "        ps = torch.sigmoid(X[:, idxs_params].mm(coeffs) + intercepts)\n",
    "        ber = torch.rand(n, d_na)\n",
    "        mask[:, idxs_nas] = ber < ps\n",
    "        ## If the inputs of the logistic model are excluded from MNAR missingness,\n",
    "        ## mask some values used in the logistic model at random.\n",
    "        ## This makes the missingness of other variables potentially dependent on masked values\n",
    "        if exclude_inputs:\n",
    "            mask[:, idxs_params] = torch.rand(n, d_params) < p\n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "        model_name = 'Missing'\n",
    "        mask_name = 'Mask'\n",
    "        out[model_name] = X_nas\n",
    "        out[mask_name] = mask\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/05y1vdbd3vd52x71qyhklyjw0000gn/T/ipykernel_5096/518150171.py:6: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  missing_mask = missing_mask.mask(missing_mask.notna(), False)\n"
     ]
    }
   ],
   "source": [
    "dataset_file = '/Users/gabrielketron/tpot2_addimputers/tpot2/ImputerExperiments/data/Spam.csv'\n",
    "#%% System Parameters\n",
    "# 1. Mini batch size\n",
    "mb_size = 128\n",
    "# 2. Missing rate\n",
    "p_miss = 0.2\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "# 4. Loss Hyperparameters\n",
    "alpha = 10\n",
    "# 5. Train Rate\n",
    "train_rate = 0.8\n",
    "\n",
    "#%% Data\n",
    "\n",
    "# Data generation\n",
    "Data = np.loadtxt(dataset_file, delimiter=\",\",skiprows=1)\n",
    "X = pd.DataFrame(Data)\n",
    "\n",
    "X_M, mask = add_missing(X, add_missing=0.3, missing_type='MNAR')\n",
    "X_M = X_M.astype(float)\n",
    "mask = mask.astype(float)\n",
    "no, dim = X_M.shape\n",
    "idx = np.random.permutation(no)\n",
    "\n",
    "Train_No = int(no * 0.5)\n",
    "Test_No = no - Train_No\n",
    "    \n",
    "# Train / Test Features\n",
    "trainX = X_M.iloc[:Train_No]\n",
    "testX = X_M.iloc[Train_No:]\n",
    "\n",
    "# Train / Test Missing Indicators\n",
    "trainM = mask.iloc[:Train_No]\n",
    "testM = mask.iloc[Train_No:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEImputer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, iterations=1000, batch_size=128, split_size=5, code_size=5, encoder_hidden_sizes=[128, 64], decoder_hidden_sizes=[128, 64],\n",
    "                    temperature=None, p_miss = 0.2, learning_rate = 0.001, tolerance=0.001, random_state=None):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = iterations\n",
    "        self.split_size = split_size\n",
    "        self.code_size = code_size\n",
    "        self.encoder_hidden_sizes = encoder_hidden_sizes\n",
    "        self.decoder_hidden_sizes = decoder_hidden_sizes\n",
    "        self.test_loss_function = torch.nn.MSELoss()\n",
    "        self.p_miss = p_miss\n",
    "        self.temperature = temperature\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.random_state=random_state\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "            )\n",
    "        torch.set_default_device(self.device)\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        torch.set_grad_enabled(True)\n",
    "        if self.random_state is not None:\n",
    "            torch.manual_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "    class Encoder():\n",
    "        def __init__(self, VAEImputer, input_size):\n",
    "            super(VAEImputer.Encoder, self).__init__()\n",
    "            self.E_W1 = torch.nn.init.xavier_normal_(torch.empty((VAEImputer.encoder_hidden_sizes[0], input_size), requires_grad=True, device=VAEImputer.device))    # Data + Hint as inputs\n",
    "            self.E_b1 = torch.zeros((VAEImputer.encoder_hidden_sizes[0]),requires_grad=True, device=VAEImputer.device)\n",
    "\n",
    "            self.E_W2 = torch.nn.init.xavier_normal_(torch.empty((VAEImputer.encoder_hidden_sizes[1], VAEImputer.encoder_hidden_sizes[0],),requires_grad=True, device=VAEImputer.device))\n",
    "            self.E_b2 = torch.zeros((VAEImputer.encoder_hidden_sizes[1]),requires_grad=True, device=VAEImputer.device)\n",
    "\n",
    "            self.E_W3 = torch.nn.init.xavier_normal_(torch.empty((VAEImputer.code_size, VAEImputer.encoder_hidden_sizes[1]),requires_grad=True, device=VAEImputer.device))\n",
    "            self.E_b3 = torch.zeros((VAEImputer.code_size), requires_grad=True, device=VAEImputer.device)   \n",
    "        \n",
    "            self.E_W4 = torch.nn.init.xavier_normal_(torch.empty((VAEImputer.split_size, VAEImputer.code_size),requires_grad=True, device=VAEImputer.device))\n",
    "            self.E_b4 = torch.zeros((VAEImputer.split_size), requires_grad=True, device=VAEImputer.device)   \n",
    "\n",
    "            self.E_W5 = torch.nn.init.xavier_normal_(torch.empty((VAEImputer.split_size, VAEImputer.code_size),requires_grad=True, device=VAEImputer.device))\n",
    "            self.E_b5 = torch.zeros((VAEImputer.split_size), requires_grad=True, device=VAEImputer.device)  \n",
    "\n",
    "        def forward(self, x):\n",
    "            l1  = torch.nn.functional.linear(input=x, weight=self.E_W1, bias=self.E_b1)\n",
    "            out1 = torch.nn.functional.tanh(l1)\n",
    "            l2 = torch.nn.functional.linear(input=out1, weight=self.E_W2, bias=self.E_b2)\n",
    "            out2 = torch.nn.functional.tanh(l2)\n",
    "            l3 = torch.nn.functional.linear(input=out2, weight=self.E_W3, bias=self.E_b3)\n",
    "            out3 = torch.nn.functional.tanh(l3)\n",
    "            mean = torch.nn.functional.linear(input=out3, weight=self.E_W4, bias=self.E_b4)\n",
    "            log_var = torch.nn.functional.linear(input=out3, weight=self.E_W5, bias=self.E_b5)\n",
    "            return mean, log_var\n",
    "\n",
    "        def parameters(self):\n",
    "            params = [self.E_W1, self.E_b1, self.E_W2, self.E_b2, self.E_W3, self.E_b3, self.E_W4, self.E_b4, self.E_W5, self.E_b5]\n",
    "            return params\n",
    "        \n",
    "        def load_state(self, params):\n",
    "            self.E_W1 = params[0]\n",
    "            self.E_b1 = params[1]\n",
    "            self.E_W2 = params[2]\n",
    "            self.E_b2 = params[3]\n",
    "            self.E_W3 = params[4]\n",
    "            self.E_b3 = params[5]\n",
    "            self.E_W4 = params[6]\n",
    "            self.E_b4 = params[7]\n",
    "            self.E_W5 = params[8]\n",
    "            self.E_b5 = params[9]\n",
    "\n",
    "    class Decoder():\n",
    "        def __init__(self, VAEImputer, input_size):\n",
    "            super(VAEImputer.Decoder, self).__init__()\n",
    "            self.D_W1 = torch.nn.init.xavier_normal_(torch.empty((VAEImputer.decoder_hidden_sizes[0], VAEImputer.split_size), requires_grad=True, device=VAEImputer.device))    # Data + Hint as inputs\n",
    "            self.D_b1 = torch.zeros((VAEImputer.decoder_hidden_sizes[0]),requires_grad=True, device=VAEImputer.device)\n",
    "\n",
    "            self.D_W2 = torch.nn.init.xavier_normal_(torch.empty((VAEImputer.decoder_hidden_sizes[1], VAEImputer.decoder_hidden_sizes[0]),requires_grad=True, device=VAEImputer.device))\n",
    "            self.D_b2 = torch.zeros((VAEImputer.decoder_hidden_sizes[1]),requires_grad=True, device=VAEImputer.device)\n",
    "\n",
    "            self.D_W3 = torch.nn.init.xavier_normal_(torch.empty((input_size, VAEImputer.decoder_hidden_sizes[1]),requires_grad=True, device=VAEImputer.device))\n",
    "            self.D_b3 = torch.zeros((input_size), requires_grad=True, device=VAEImputer.device)   \n",
    "\n",
    "        def forward(self, x):\n",
    "            l1  = torch.nn.functional.linear(input=x, weight=self.D_W1, bias=self.D_b1)\n",
    "            out1 = torch.nn.functional.tanh(l1)\n",
    "            l2 = torch.nn.functional.linear(input=out1, weight=self.D_W2, bias=self.D_b2)\n",
    "            out2 = torch.nn.functional.tanh(l2)\n",
    "            l3 = torch.nn.functional.linear(input=out2, weight=self.D_W3, bias=self.D_b3)\n",
    "            x_hat = torch.nn.functional.sigmoid(l3)\n",
    "            return x_hat\n",
    "\n",
    "        def parameters(self):\n",
    "            params = [self.D_W1, self.D_b1, self.D_W2, self.D_b2, self.D_W3, self.D_b3]\n",
    "            return params\n",
    "        \n",
    "        def load_state(self, params):\n",
    "            self.D_W1 = params[0]\n",
    "            self.D_b1 = params[1]\n",
    "            self.D_W2 = params[2]\n",
    "            self.D_b2 = params[3]\n",
    "            self.D_W3 = params[4]\n",
    "            self.D_b3 = params[5]\n",
    "            \n",
    "    class VAE():\n",
    "        def __init__(self, VAEImputer, input_size):\n",
    "            super(VAEImputer.VAE, self).__init__()\n",
    "            self.encoder = VAEImputer.Encoder(VAEImputer, input_size)\n",
    "            self.decoder = VAEImputer.Decoder(VAEImputer, input_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            mu, log_var = self.encoder.forward(x)\n",
    "            code = self.reparameterize(mu, log_var)\n",
    "            reconstucted = self.decoder.forward(code)\n",
    "            return code, reconstucted, mu, log_var\n",
    "\n",
    "        def reparameterize(self, mu, log_var):\n",
    "            std = torch.exp(0.5 * log_var)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "            \n",
    "        def parameters(self):\n",
    "            params = self.encoder.parameters() + self.decoder.parameters()\n",
    "            return params\n",
    "        \n",
    "        def load_state(self, params):\n",
    "            self.encoder.load_state(params[0:10]) \n",
    "            self.decoder.load_state(params[10:]) \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.variable_sizes = [1]*X.shape[1] #list of 1s the same lenght as the features of X\n",
    "        \n",
    "        self.encoder_hidden_sizes = [int(math.floor(X.shape[1]/2)), int(math.floor(X.shape[1]*3/10))]\n",
    "        self.decoder_hidden_sizes = [int(math.floor(X.shape[1]*3/10)), int(math.floor(X.shape[1]/2))]\n",
    "        self.split_size =int(math.floor(X.shape[1]/5))\n",
    "        self.code_size=int(math.floor(X.shape[1]/5))\n",
    "        \n",
    "        #print(self.encoder_hidden_sizes)\n",
    "\n",
    "        features = torch.from_numpy(X.to_numpy()) #X features\n",
    "        features = torch.nan_to_num(features)\n",
    "        features = features.to(dtype=torch.float32)\n",
    "        features = features.to(device=self.device)\n",
    "        \n",
    "\n",
    "        num_samples = len(features)\n",
    "        variable_masks = []\n",
    "        for variable_size in self.variable_sizes:\n",
    "            variable_mask = (torch.zeros(num_samples, 1).uniform_(0.0, 1.0) > self.p_miss).float()\n",
    "            if variable_size > 1:\n",
    "                variable_mask = variable_mask.repeat(1, variable_size)\n",
    "            variable_masks.append(variable_mask)\n",
    "        mask = torch.cat(variable_masks, dim=1)\n",
    "\n",
    "        temperature = self.temperature\n",
    "        self.model = self.VAE(VAEImputer=self, input_size=features.shape[1])\n",
    "        \n",
    "        inverted_mask = 1 - mask\n",
    "        observed = features * mask\n",
    "        missing = torch.randn_like(features)\n",
    "        noisy_features = observed + missing*inverted_mask\n",
    "\n",
    "        if self.learning_rate is not None:\n",
    "            missing = torch.autograd.Variable(missing, requires_grad=True)\n",
    "            self.optim = torch.optim.Adam(self.model.parameters(), weight_decay=0, lr=self.learning_rate)\n",
    "\n",
    "        #pbar = tqdm(range(self.iterations))\n",
    "        for iterations in range(self.iterations):\n",
    "            train_ds = torch.utils.data.TensorDataset(features.float(), mask.float(), noisy_features.float())\n",
    "            losses = [np.inf]\n",
    "            for f, m, n in torch.utils.data.DataLoader(train_ds, \n",
    "                                                       batch_size=self.batch_size,\n",
    "                                                         shuffle=True, \n",
    "                                                         generator=torch.Generator(device=self.device)):\n",
    "                loss = self.train_batch(f, m, n)\n",
    "                temp_loss = losses[-1]\n",
    "                \n",
    "                if temp_loss - loss < self.tolerance:\n",
    "                    break\n",
    "                \n",
    "                losses.append(loss)\n",
    "            #pbar.set_postfix({'loss': min(losses)})\n",
    "            \n",
    "            if iterations % 100 == 0 :\n",
    "                print(f'Epoch {iterations} loss: {loss:.4f}')\n",
    "            \n",
    "\n",
    "        self._VAE_params = self.model.parameters()\n",
    "        #print(len(self._VAE_params))\n",
    "        return self\n",
    "    \n",
    "    def train_batch(self, features, mask, noisy_features):\n",
    "        self.optim.zero_grad()\n",
    "        #print(features.shape)\n",
    "        #print(noisy_features.shape)\n",
    "        #noise = torch.autograd.Variable(torch.FloatTensor(len(noisy_features), self.p_miss).normal_())\n",
    "        _, reconstructed, mu, log_var = self.model.forward(noisy_features)\n",
    "        #print(reconstructed.shape)\n",
    "        #print(reconstructed)\n",
    "        # reconstruction of the non-missing values\n",
    "        reconstruction_loss = self.masked_reconstruction_loss_function(reconstructed,\n",
    "                                                                  features,\n",
    "                                                                  mask,\n",
    "                                                                  self.variable_sizes)\n",
    "        missing_loss = self.masked_reconstruction_loss_function(reconstructed, features, 1-mask, self.variable_sizes)\n",
    "        #print(reconstruction_loss)\n",
    "        loss = torch.sqrt(self.test_loss_function((mask * features + (1.0 - mask) * reconstructed), features))\n",
    "        \n",
    "        kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        #print(kld_loss)\n",
    "        observed_loss = reconstruction_loss + kld_loss\n",
    "        #loss = loss.type(torch.float32)\n",
    "        #print(loss)\n",
    "        observed_loss.backward()\n",
    "\n",
    "        self.optim.step()\n",
    "\n",
    "        return observed_loss.cpu().detach().numpy()\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        self.model.load_state(self._VAE_params)\n",
    "        self.variable_sizes = [1]*X.shape[1] #list of 1s the same lenght as the features of X\n",
    "        features = torch.from_numpy(X.to_numpy()) #X features\n",
    "        features = torch.nan_to_num(features)\n",
    "        mask = torch.from_numpy(1-np.isnan(X.to_numpy()))\n",
    "        inverted_mask = ~mask\n",
    "        num_samples = len(features)\n",
    "        observed = features * mask\n",
    "        missing = torch.randn_like(features)\n",
    "        noisy_features = observed + missing*inverted_mask\n",
    "        \n",
    "        f = features.to(dtype=torch.float32)\n",
    "        f = f.to(device=self.device)\n",
    "        \n",
    "        m = mask.to(dtype=torch.float32)\n",
    "        m = m.to(device=self.device)\n",
    "        #print(m)\n",
    "        n = noisy_features.to(dtype=torch.float32)\n",
    "        n = n.to(device=self.device)\n",
    "        #print(n)\n",
    "        with torch.no_grad():\n",
    "            _, reconstructed, _, _ = self.model.forward(n)\n",
    "            #print(reconstructed)\n",
    "            imputed = m*n + (1.0 - m)*reconstructed\n",
    "        return imputed.cpu().numpy()\n",
    "\n",
    "    def reconstruction_loss_function(self, reconstructed, original, variable_sizes, reduction=\"mean\"):\n",
    "        # by default use loss for binary variables\n",
    "        if variable_sizes is None:\n",
    "            return torch.nn.functional.binary_cross_entropy(reconstructed, original, reduction=reduction)\n",
    "        # use the variable sizes when available\n",
    "        else:\n",
    "            loss = 0\n",
    "            start = 0\n",
    "            numerical_size = 0\n",
    "            for variable_size in variable_sizes:\n",
    "                # if it is a categorical variable\n",
    "                if variable_size > 1:\n",
    "                    # add loss from the accumulated continuous variables\n",
    "                    if numerical_size > 0:\n",
    "                        end = start + numerical_size\n",
    "                        batch_reconstructed_variable = reconstructed[:, start:end]\n",
    "                        batch_target = original[:, start:end]\n",
    "                        loss += torch.nn.functional.mse_loss(batch_reconstructed_variable, batch_target, reduction=reduction)\n",
    "                        start = end\n",
    "                        numerical_size = 0\n",
    "                    # add loss from categorical variable\n",
    "                    end = start + variable_size\n",
    "                    batch_reconstructed_variable = reconstructed[:, start:end]\n",
    "                    batch_target = torch.argmax(original[:, start:end], dim=1)\n",
    "                    loss += torch.nn.functional.cross_entropy(batch_reconstructed_variable, batch_target, reduction=reduction)\n",
    "                    start = end\n",
    "                # if not, accumulate numerical variables\n",
    "                else:\n",
    "                    numerical_size += 1\n",
    "\n",
    "            # add loss from the remaining accumulated numerical variables\n",
    "            if numerical_size > 0:\n",
    "                end = start + numerical_size\n",
    "                batch_reconstructed_variable = reconstructed[:, start:end]\n",
    "                batch_target = original[:, start:end]\n",
    "                loss += torch.nn.functional.mse_loss(batch_reconstructed_variable, batch_target, reduction=reduction)\n",
    "\n",
    "            return loss\n",
    "\n",
    "    def masked_reconstruction_loss_function(self, reconstructed, original, mask, variable_sizes):\n",
    "        return self.reconstruction_loss_function(mask * reconstructed,\n",
    "                                            mask * original,\n",
    "                                            variable_sizes,\n",
    "                                            reduction=\"sum\") / torch.sum(mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot2.builtin_modules\n",
    "\n",
    "\n",
    "gain = tpot2.builtin_modules.imputer.VAEImputer(batch_size=64, iterations=10000)\n",
    "\n",
    "imputed_train = gain.fit_transform(trainX)\n",
    "imputed_test = gain.transform(testX)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(imputed_train))\n",
    "print(type(imputed_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 2.002874479439409\n",
      "Test RMSE: 1.9971598844886538\n"
     ]
    }
   ],
   "source": [
    "def rmse_loss(ori_data, imputed_data, data_m):\n",
    "        '''Compute RMSE loss between ori_data and imputed_data\n",
    "        Args:\n",
    "            - ori_data: original data without missing values\n",
    "            - imputed_data: imputed data\n",
    "            - data_m: indicator matrix for missingness\n",
    "        Returns:\n",
    "            - rmse: Root Mean Squared Error\n",
    "        '''\n",
    "        #ori_data, norm_parameters = normalization(ori_data)\n",
    "        #imputed_data, _ = normalization(imputed_data, norm_parameters)\n",
    "        # Only for missing values\n",
    "        nominator = np.sum(((1-data_m) * ori_data - (1-data_m) * imputed_data)**2)\n",
    "        denominator = np.sum(1-data_m)\n",
    "        rmse = np.sqrt(nominator/float(denominator))\n",
    "        return rmse\n",
    "\n",
    "\n",
    "print(f'Train RMSE: {rmse_loss(ori_data=X[:Train_No].to_numpy(), imputed_data=imputed_train, data_m=trainM.to_numpy())}')\n",
    "print(f'Test RMSE: {rmse_loss(ori_data=X[Train_No:].to_numpy(), imputed_data=imputed_test, data_m=testM.to_numpy())}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
