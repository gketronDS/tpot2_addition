{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def create_one_type_dictionary(variable_type, variables):\n",
    "    return dict([(variable, variable_type) for variable in variables])\n",
    "\n",
    "def types_to_sorted_lists(variable_types, variables=None):\n",
    "    if variables is None:\n",
    "        variables = variable_types.keys()\n",
    "\n",
    "    binary_variables = []\n",
    "    categorical_variables = []\n",
    "    numerical_variables = []\n",
    "    for variable in variables:\n",
    "        variable_type = variable_types[variable]\n",
    "        if variable_type == \"categorical\":\n",
    "            categorical_variables.append(variable)\n",
    "        elif variable_type == \"binary\":\n",
    "            binary_variables.append(variable)\n",
    "        elif variable_type == \"numerical\":\n",
    "            numerical_variables.append(variable)\n",
    "        else:\n",
    "            raise Exception(\"Invalid type: '{}'.\".format(variable_type))\n",
    "\n",
    "    binary_variables = sorted(binary_variables)\n",
    "    categorical_variables = sorted(categorical_variables)\n",
    "    numerical_variables = sorted(numerical_variables)\n",
    "    return binary_variables, categorical_variables, numerical_variables\n",
    "\n",
    "def create_metadata(variables, variable_types, categorical_values={}, num_samples=None, classes=None):\n",
    "    binary_variables, categorical_variables, numerical_variables = types_to_sorted_lists(variable_types, variables)\n",
    "\n",
    "    feature_number = 0\n",
    "    value_to_index = {}\n",
    "    index_to_value = []\n",
    "    variable_sizes = []\n",
    "    variable_types = []\n",
    "\n",
    "    for variable in categorical_variables:\n",
    "        variable_types.append(\"categorical\")\n",
    "        values = sorted(categorical_values[variable])\n",
    "        variable_sizes.append(len(values))\n",
    "        value_to_index[variable] = {}\n",
    "        for value in values:\n",
    "            index_to_value.append((variable, value))\n",
    "            value_to_index[variable][value] = feature_number\n",
    "            feature_number += 1\n",
    "\n",
    "    for variable in binary_variables:\n",
    "        variable_types.append(\"categorical\")\n",
    "        values = [0, 1]\n",
    "        variable_sizes.append(2)\n",
    "        value_to_index[variable] = {}\n",
    "        for value in values:\n",
    "            index_to_value.append((variable, value))\n",
    "            value_to_index[variable][value] = feature_number\n",
    "            feature_number += 1\n",
    "\n",
    "    for variable in numerical_variables:\n",
    "        variable_types.append(\"numerical\")\n",
    "        variable_sizes.append(1)\n",
    "        value_to_index[variable] = feature_number\n",
    "        feature_number += 1\n",
    "\n",
    "    num_features = feature_number\n",
    "\n",
    "    metadata = {\n",
    "        \"variables\": binary_variables + categorical_variables + numerical_variables,\n",
    "        \"variable_sizes\": variable_sizes,\n",
    "        \"variable_types\": variable_types,\n",
    "        \"index_to_value\": index_to_value,\n",
    "        \"value_to_index\": value_to_index,\n",
    "        \"num_features\": num_features,\n",
    "    }\n",
    "\n",
    "    if num_samples is not None:\n",
    "        metadata[\"num_samples\"] = num_samples\n",
    "\n",
    "    if classes is not None:\n",
    "        metadata[\"classes\"] = classes\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def create_class_to_index(classes):\n",
    "    return dict([(c, i) for i, c in enumerate(classes)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "def create_directories_if_needed(directory_path):\n",
    "    if not os.path.exists(directory_path):\n",
    "        try:\n",
    "            os.makedirs(directory_path)\n",
    "        except OSError as e:\n",
    "            if e.errno != os.errno.EEXIST:\n",
    "                raise\n",
    "    return directory_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] input features labels metadata\n",
      "ipykernel_launcher.py: error: the following arguments are required: input, features, labels, metadata\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielketron/tpot2_addimputers/env2/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "VARIABLES = [\n",
    "    \"x-box\",\n",
    "    \"y-box\",\n",
    "    \"width\",\n",
    "    \"high\",\n",
    "    \"onpix\",\n",
    "    \"x-bar\",\n",
    "    \"y-bar\",\n",
    "    \"x2bar\",\n",
    "    \"y2bar\",\n",
    "    \"xybar\",\n",
    "    \"x2ybr\",\n",
    "    \"xy2br\",\n",
    "    \"x-ege\",\n",
    "    \"xegvy\",\n",
    "    \"y-ege\",\n",
    "    \"yegvx\",\n",
    "]\n",
    "\n",
    "NUM_SAMPLES = [\n",
    "    789,  # A\n",
    "    766,  # B\n",
    "    736,  # C\n",
    "    805,  # D\n",
    "    768,  # E\n",
    "    775,  # F\n",
    "    773,  # G\n",
    "    734,  # H\n",
    "    755,  # I\n",
    "    747,  # J\n",
    "    739,  # K\n",
    "    761,  # L\n",
    "    792,  # M\n",
    "    783,  # N\n",
    "    753,  # O\n",
    "    803,  # P\n",
    "    783,  # Q\n",
    "    758,  # R\n",
    "    748,  # S\n",
    "    796,  # T\n",
    "    813,  # U\n",
    "    764,  # V\n",
    "    752,  # W\n",
    "    787,  # X\n",
    "    786,  # Y\n",
    "    734,  # Z\n",
    "]\n",
    "\n",
    "CLASSES = [\n",
    "    \"A\",\n",
    "    \"B\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "    \"E\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"J\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"O\",\n",
    "    \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"U\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "]\n",
    "\n",
    "CLASS_TO_INDEX = create_class_to_index(CLASSES)\n",
    "\n",
    "\n",
    "def letter_recognition_transform(input_path, features_path, labels_path, metadata_path):\n",
    "    metadata = create_metadata(VARIABLES,\n",
    "                               create_one_type_dictionary(\"numerical\", VARIABLES),\n",
    "                               {},\n",
    "                               sum(NUM_SAMPLES),\n",
    "                               CLASSES)\n",
    "\n",
    "    input_file = open(input_path, \"r\")\n",
    "\n",
    "    features = np.zeros((metadata[\"num_samples\"], metadata[\"num_features\"]), dtype=np.float32)\n",
    "    labels = np.zeros(metadata[\"num_samples\"], dtype=np.int32)\n",
    "\n",
    "    # transform\n",
    "    i = 0\n",
    "    line = input_file.readline()\n",
    "    while line != \"\":\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        values = line.split(\",\")\n",
    "\n",
    "        assert len(values) - 1 == len(VARIABLES), str((len(values) - 1, len(VARIABLES)))\n",
    "\n",
    "        for j, value in enumerate(values[1:]):\n",
    "            value = float(value)\n",
    "            features[i, j] = value\n",
    "\n",
    "        labels[i] = CLASS_TO_INDEX[values[0]]\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        line = input_file.readline()\n",
    "\n",
    "    # scale\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "    scaler.fit_transform(features)\n",
    "\n",
    "    assert i == metadata[\"num_samples\"]\n",
    "\n",
    "    for class_index in range(len(NUM_SAMPLES)):\n",
    "        num_samples_class = (labels == class_index).sum()\n",
    "        assert num_samples_class == NUM_SAMPLES[class_index]\n",
    "\n",
    "    print(\"Total samples: \", features.shape[0])\n",
    "    print(\"Features: \", features.shape[1])\n",
    "\n",
    "    np.save(features_path, features)\n",
    "    np.save(labels_path, labels)\n",
    "\n",
    "    input_file.close()\n",
    "\n",
    "    metadata[\"features_min\"] = scaler.data_min_.tolist()\n",
    "    metadata[\"features_max\"] = scaler.data_max_.tolist()\n",
    "\n",
    "    with open(metadata_path, \"w\") as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    options_parser = argparse.ArgumentParser(\n",
    "        description=\"Transform the Letter Recognition data into feature matrices.\"\n",
    "                    + \" Dataset: https://archive.ics.uci.edu/ml/datasets/Letter+Recognition\"\n",
    "    )\n",
    "\n",
    "    options_parser.add_argument(\"input\", type=str, help=\"Input Letter Recognition data in text format.\")\n",
    "    options_parser.add_argument(\"features\", type=str, help=\"Output features in numpy array format.\")\n",
    "    options_parser.add_argument(\"labels\", type=str, help=\"Output labels in numpy array format.\")\n",
    "    options_parser.add_argument(\"metadata\", type=str, help=\"Metadata in json format.\")\n",
    "\n",
    "    options = options_parser.parse_args(args=args)\n",
    "\n",
    "    letter_recognition_transform(options.input, options.features, options.labels, options.metadata)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "(17, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     10\u001b[0m     transform_main(args\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/letter-recognition/letter-recognition.data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/letter-recognition/features.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/letter-recognition/labels.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/letter-recognition/metadata.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     ])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m wget\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/letter-recognition/letter-recognition.data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtransform_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/letter-recognition/letter-recognition.data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/letter-recognition/features.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/letter-recognition/labels.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/letter-recognition/metadata.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 220\u001b[0m, in \u001b[0;36mtransform_main\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    216\u001b[0m options_parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetadata in json format.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    218\u001b[0m options \u001b[38;5;241m=\u001b[39m options_parser\u001b[38;5;241m.\u001b[39mparse_args(args\u001b[38;5;241m=\u001b[39margs)\n\u001b[0;32m--> 220\u001b[0m \u001b[43mbreast_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 166\u001b[0m, in \u001b[0;36mbreast_transform\u001b[0;34m(input_path, features_path, labels_path, metadata_path)\u001b[0m\n\u001b[1;32m    163\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m values \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(VARIABLES), \u001b[38;5;28mstr\u001b[39m((\u001b[38;5;28mlen\u001b[39m(values), \u001b[38;5;28mlen\u001b[39m(VARIABLES)))\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values[\u001b[38;5;241m2\u001b[39m:]):\n\u001b[1;32m    169\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(value)\n",
      "\u001b[0;31mAssertionError\u001b[0m: (17, 32)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    create_directories_if_needed(\"data/letter-recognition\")\n",
    "\n",
    "    wget.download(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\",\n",
    "        \"data/letter-recognition/letter-recognition.data\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    transform_main(args=[\n",
    "        \"data/letter-recognition/letter-recognition.data\",\n",
    "        \"data/letter-recognition/features.npy\",\n",
    "        \"data/letter-recognition/labels.npy\",\n",
    "        \"data/letter-recognition/metadata.json\"\n",
    "    ])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 39644\n",
    "\n",
    "ORIGINAL_TYPES = {\n",
    "    \"n_tokens_title\": \"numerical\",  # Number of words in the title\n",
    "    \"n_tokens_content\": \"numerical\",  # Number of words in the content\n",
    "    \"n_unique_tokens\": \"numerical\",  # Rate of unique words in the content\n",
    "    \"n_non_stop_words\": \"numerical\",  # Rate of non-stop words in the content\n",
    "    \"n_non_stop_unique_tokens\": \"numerical\",  # Rate of unique non-stop words in the content\n",
    "    \"num_hrefs\": \"numerical\",  # Number of links\n",
    "    \"num_self_hrefs\": \"numerical\",  # Number of links to other articles published by Mashable\n",
    "    \"num_imgs\": \"numerical\",  # Number of images\n",
    "    \"num_videos\": \"numerical\",  # Number of videos\n",
    "    \"average_token_length\": \"numerical\",  # Average length of the words in the content\n",
    "    \"num_keywords\": \"numerical\",  # Number of keywords in the metadata\n",
    "    \"data_channel_is_lifestyle\": \"categorical_split\",  # Is data channel 'Lifestyle'?\n",
    "    \"data_channel_is_entertainment\": \"categorical_split\",  # Is data channel 'Entertainment'?\n",
    "    \"data_channel_is_bus\": \"categorical_split\",  # Is data channel 'Business'?\n",
    "    \"data_channel_is_socmed\": \"categorical_split\",  # Is data channel 'Social Media'?\n",
    "    \"data_channel_is_tech\": \"categorical_split\",  # Is data channel 'Tech'?\n",
    "    \"data_channel_is_world\": \"categorical_split\",  # Is data channel 'World'?\n",
    "    \"kw_min_min\": \"numerical\",  # Worst keyword (min. shares)\n",
    "    \"kw_max_min\": \"numerical\",  # Worst keyword (max. shares)\n",
    "    \"kw_avg_min\": \"numerical\",  # Worst keyword (avg. shares)\n",
    "    \"kw_min_max\": \"numerical\",  # Best keyword (min. shares)\n",
    "    \"kw_max_max\": \"numerical\",  # Best keyword (max. shares)\n",
    "    \"kw_avg_max\": \"numerical\",  # Best keyword (avg. shares)\n",
    "    \"kw_min_avg\": \"numerical\",  # Avg. keyword (min. shares)\n",
    "    \"kw_max_avg\": \"numerical\",  # Avg. keyword (max. shares)\n",
    "    \"kw_avg_avg\": \"numerical\",  # Avg. keyword (avg. shares)\n",
    "    \"self_reference_min_shares\": \"numerical\",  # Min. shares of referenced articles in Mashable\n",
    "    \"self_reference_max_shares\": \"numerical\",  # Max. shares of referenced articles in Mashable\n",
    "    \"self_reference_avg_sharess\": \"numerical\",  # Avg. shares of referenced articles in Mashable\n",
    "    \"weekday_is_monday\": \"categorical_split\",  # Was the article published on a Monday?\n",
    "    \"weekday_is_tuesday\": \"categorical_split\",  # Was the article published on a Tuesday?\n",
    "    \"weekday_is_wednesday\": \"categorical_split\",  # Was the article published on a Wednesday?\n",
    "    \"weekday_is_thursday\": \"categorical_split\",  # Was the article published on a Thursday?\n",
    "    \"weekday_is_friday\": \"categorical_split\",  # Was the article published on a Friday?\n",
    "    \"weekday_is_saturday\": \"categorical_split\",  # Was the article published on a Saturday?\n",
    "    \"weekday_is_sunday\": \"categorical_split\",  # Was the article published on a Sunday?\n",
    "    \"is_weekend\": \"binary\",  # Was the article published on the weekend?\n",
    "    \"LDA_00\": \"numerical\",  # Closeness to LDA topic 0\n",
    "    \"LDA_01\": \"numerical\",  # Closeness to LDA topic 1\n",
    "    \"LDA_02\": \"numerical\",  # Closeness to LDA topic 2\n",
    "    \"LDA_03\": \"numerical\",  # Closeness to LDA topic 3\n",
    "    \"LDA_04\": \"numerical\",  # Closeness to LDA topic 4\n",
    "    \"global_subjectivity\": \"numerical\",  # Text subjectivity\n",
    "    \"global_sentiment_polarity\": \"numerical\",  # Text sentiment polarity\n",
    "    \"global_rate_positive_words\": \"numerical\",  # Rate of positive words in the content\n",
    "    \"global_rate_negative_words\": \"numerical\",  # Rate of negative words in the content\n",
    "    \"rate_positive_words\": \"numerical\",  # Rate of positive words among non-neutral tokens\n",
    "    \"rate_negative_words\": \"numerical\",  # Rate of negative words among non-neutral tokens\n",
    "    \"avg_positive_polarity\": \"numerical\",  # Avg. polarity of positive words\n",
    "    \"min_positive_polarity\": \"numerical\",  # Min. polarity of positive words\n",
    "    \"max_positive_polarity\": \"numerical\",  # Max. polarity of positive words\n",
    "    \"avg_negative_polarity\": \"numerical\",  # Avg. polarity of negative  words\n",
    "    \"min_negative_polarity\": \"numerical\",  # Min. polarity of negative  words\n",
    "    \"max_negative_polarity\": \"numerical\",  # Max. polarity of negative  words\n",
    "    \"title_subjectivity\": \"numerical\",  # Title subjectivity\n",
    "    \"title_sentiment_polarity\": \"numerical\",  # Title polarity\n",
    "    \"abs_title_subjectivity\": \"numerical\",  # Absolute subjectivity level\n",
    "    \"abs_title_sentiment_polarity\": \"numerical\",  # Absolute polarity level\n",
    "}\n",
    "\n",
    "CAN_BE_EMPTY = {\n",
    "    \"data_channel\": True,\n",
    "    \"weekday\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def read_binary(value):\n",
    "    return int(float(value.strip()))\n",
    "\n",
    "\n",
    "def online_news_popularity_transform(input_path, features_path, labels_path, metadata_path):\n",
    "    variables = []\n",
    "    types = {}\n",
    "    values = {}\n",
    "\n",
    "    for original_variable, original_type in ORIGINAL_TYPES.items():\n",
    "        if \"_is_\" in original_variable:\n",
    "            index = original_variable.index(\"_is_\")\n",
    "            variable = original_variable[:index]\n",
    "            value = original_variable[index + 4:]\n",
    "            if variable not in types:\n",
    "                assert variable not in values\n",
    "                types[variable] = \"categorical\"\n",
    "                if CAN_BE_EMPTY[variable]:\n",
    "                    values[variable] = [\"none\"]\n",
    "                else:\n",
    "                    values[variable] = []\n",
    "                variables.append(variable)\n",
    "            values[variable].append(value)\n",
    "        else:\n",
    "            variables.append(original_variable)\n",
    "            types[original_variable] = original_type\n",
    "\n",
    "    metadata = create_metadata(variables, types, values, NUM_SAMPLES)\n",
    "\n",
    "    input_file = open(input_path, \"r\")\n",
    "    reader = csv.DictReader(input_file)\n",
    "\n",
    "    reader.fieldnames = [variable.strip() for variable in reader.fieldnames]\n",
    "\n",
    "    features = np.zeros((metadata[\"num_samples\"], metadata[\"num_features\"]), dtype=np.float32)\n",
    "    labels = np.zeros(metadata[\"num_samples\"], dtype=np.float32)\n",
    "\n",
    "    # transform\n",
    "    for i, row in enumerate(reader):\n",
    "        # the categorical variables are already one hot encoded\n",
    "        for j, variable in enumerate(metadata[\"variables\"]):\n",
    "            if types[variable] == \"numerical\":\n",
    "                value = float(row[variable])\n",
    "                features[i, metadata[\"value_to_index\"][variable]] = value\n",
    "            elif types[variable] == \"categorical\":\n",
    "                value = None\n",
    "                for possible_value in values[variable]:\n",
    "                    if possible_value == \"none\":\n",
    "                        continue\n",
    "                    real_variable = \"{}_is_{}\".format(variable, possible_value)\n",
    "                    if read_binary(row[real_variable]) == 1:\n",
    "                        if value is None:\n",
    "                            value = possible_value\n",
    "                        else:\n",
    "                            raise Exception(\"'{}' was already defined\".format(variable))\n",
    "                if value is None:\n",
    "                    if \"none\" in values[variable]:\n",
    "                        value = \"none\"\n",
    "                    else:\n",
    "                        for possible_value in values[variable]:\n",
    "                            if possible_value == \"none\":\n",
    "                                continue\n",
    "                            real_variable = \"{}_is_{}\".format(variable, possible_value)\n",
    "                            print(possible_value, real_variable, read_binary(row[real_variable]))\n",
    "                        raise Exception(\"'{}' has no valid value\".format(variable))\n",
    "                features[i, metadata[\"value_to_index\"][variable][value]] = 1.0\n",
    "            elif types[variable] == \"binary\":\n",
    "                value = read_binary(row[variable])\n",
    "                assert value in [0, 1], \"'{}' is not a valid value for '{}'\".format(value, variable)\n",
    "                features[i, metadata[\"value_to_index\"][variable][value]] = 1.0\n",
    "            else:\n",
    "                raise Exception(\"Unknown variable type.\")\n",
    "\n",
    "        labels[i] = row[\"shares\"]\n",
    "\n",
    "    # scale\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "    scaler.fit_transform(features)\n",
    "\n",
    "    label_scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "    label_scaler.fit_transform(labels.reshape(-1, 1))\n",
    "\n",
    "    assert i == metadata[\"num_samples\"] - 1\n",
    "\n",
    "    print(\"Total samples: \", features.shape[0])\n",
    "    print(\"Features: \", features.shape[1])\n",
    "\n",
    "    np.save(features_path, features)\n",
    "    np.save(labels_path, labels)\n",
    "\n",
    "    input_file.close()\n",
    "\n",
    "    metadata[\"features_min\"] = scaler.data_min_.tolist()\n",
    "    metadata[\"features_max\"] = scaler.data_max_.tolist()\n",
    "\n",
    "    metadata[\"labels_min\"] = label_scaler.data_min_.tolist()\n",
    "    metadata[\"labels_max\"] = label_scaler.data_max_.tolist()\n",
    "\n",
    "    with open(metadata_path, \"w\") as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    options_parser = argparse.ArgumentParser(\n",
    "        description=\"Transform the Online News Popularity data into feature matrices.\"\n",
    "                    + \" Dataset: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity\"\n",
    "    )\n",
    "\n",
    "    options_parser.add_argument(\"input\", type=str, help=\"Input Online News Popularity data in text format.\")\n",
    "    options_parser.add_argument(\"features\", type=str, help=\"Output features in numpy array format.\")\n",
    "    options_parser.add_argument(\"labels\", type=str, help=\"Output labels in numpy array format.\")\n",
    "    options_parser.add_argument(\"metadata\", type=str, help=\"Metadata in json format.\")\n",
    "\n",
    "    options = options_parser.parse_args(args=args)\n",
    "\n",
    "    online_news_popularity_transform(options.input, options.features, options.labels, options.metadata)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    create_directories_if_needed(\"data/online-news-popularity\")\n",
    "\n",
    "    wget.download(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip\",\n",
    "        \"data/online-news-popularity/OnlineNewsPopularity.zip\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    with zipfile.ZipFile(\"data/online-news-popularity/OnlineNewsPopularity.zip\", \"r\") as zip_file:\n",
    "        zip_file.extractall(\"data/online-news-popularity/\")\n",
    "\n",
    "    transform_main(args=[\n",
    "        \"data/online-news-popularity/OnlineNewsPopularity/OnlineNewsPopularity.csv\",\n",
    "        \"data/online-news-popularity/features.npy\",\n",
    "        \"data/online-news-popularity/labels.npy\",\n",
    "        \"data/online-news-popularity/metadata.json\"\n",
    "    ])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
