{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted, _check_feature_names_in\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn\n",
    "import sklearn.impute\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import sklearn.compose\n",
    "\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = '/Users/gabrielketron/tpot2_addimputers/tpot2/ImputerExperiments/data/Spam.csv'\n",
    "#%% System Parameters\n",
    "# 1. Mini batch size\n",
    "mb_size = 128\n",
    "# 2. Missing rate\n",
    "p_miss = 0.2\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "# 4. Loss Hyperparameters\n",
    "alpha = 10\n",
    "# 5. Train Rate\n",
    "train_rate = 0.8\n",
    "\n",
    "#%% Data\n",
    "\n",
    "# Data generation\n",
    "Data = np.loadtxt(dataset_file, delimiter=\",\",skiprows=1)\n",
    "\n",
    "# Parameters\n",
    "No = len(Data)\n",
    "Dim = len(Data[0,:])\n",
    "\n",
    "# Hidden state dimensions\n",
    "H_Dim1 = Dim\n",
    "H_Dim2 = Dim\n",
    "\n",
    "# Normalization (0 to 1)\n",
    "Min_Val = np.zeros(Dim)\n",
    "Max_Val = np.zeros(Dim)\n",
    "\n",
    "for i in range(Dim):\n",
    "    Min_Val[i] = np.min(Data[:,i])\n",
    "    Data[:,i] = Data[:,i] - np.min(Data[:,i])\n",
    "    Max_Val[i] = np.max(Data[:,i])\n",
    "    Data[:,i] = Data[:,i] / (np.max(Data[:,i]) + 1e-6)    \n",
    "\n",
    "#%% Missing introducing\n",
    "p_miss_vec = p_miss * np.ones((Dim,1)) \n",
    "   \n",
    "Missing = np.zeros((No,Dim))\n",
    "\n",
    "for i in range(Dim):\n",
    "    A = np.random.uniform(0., 1., size = [len(Data),])\n",
    "    B = A > p_miss_vec[i]\n",
    "    Missing[:,i] = 1.*B\n",
    "\n",
    "    \n",
    "#%% Train Test Division    \n",
    "   \n",
    "idx = np.random.permutation(No)\n",
    "\n",
    "Train_No = int(No * train_rate)\n",
    "Test_No = No - Train_No\n",
    "    \n",
    "# Train / Test Features\n",
    "trainX = Data[idx[:Train_No],:]\n",
    "testX = Data[idx[Train_No:],:]\n",
    "\n",
    "# Train / Test Missing Indicators\n",
    "trainM = Missing[idx[:Train_No],:]\n",
    "testM = Missing[idx[Train_No:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GainImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Base class for all imputers.\n",
    "    It adds automatically support for `add_indicator`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 batch_size=128, \n",
    "                 hint_rate=0.9, \n",
    "                 alpha=100, \n",
    "                 iterations=10000,\n",
    "                 missing_values=np.nan, \n",
    "                 random_state=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.hint_rate = hint_rate\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.missing_values = missing_values\n",
    "        self.random_state = random_state\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "            )\n",
    "        if random_state is not None:\n",
    "            torch.manual_seed(self.random_state)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.fit_transform(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if hasattr(X, 'dtypes'):\n",
    "            X = X.to_numpy()\n",
    "        #define mask matrix\n",
    "        X_mask = 1 - np.isnan(X)\n",
    "        #get dimensions\n",
    "        no, dim = X.shape\n",
    "        int_dim = int(dim)\n",
    "        #normalize the original data, and save parameters for renormalization\n",
    "        norm_data = X.copy()\n",
    "        min_val = np.zeros(dim)\n",
    "        max_val = np.zeros(dim)\n",
    "        for i in range(dim):\n",
    "            min_val[i] = np.nanmin(norm_data[i])\n",
    "            norm_data[:, i] -= np.nanmin(norm_data[:, i])\n",
    "            max_val[i] = np.nanmax(norm_data[i])\n",
    "            norm_data[:, i] /= (np.nanmax(norm_data[:, i]) + 1e-06)\n",
    "        norm_parameters = {'min_val': min_val, 'max_val': max_val}\n",
    "        norm_data_filled = np.nan_to_num(norm_data, 0)\n",
    "\n",
    "        Z_mb = self._uniform_sampler(0, 0.01, no, dim)\n",
    "        M_mb = X_mask\n",
    "        X_mb = norm_data_filled\n",
    "        New_X_mb = M_mb * X_mb + (1 - M_mb)*Z_mb\n",
    "\n",
    "        X_mb = torch.tensor(X_mb, dtype=torch.float32)\n",
    "        M_mb = torch.tensor(M_mb, dtype=torch.float32)\n",
    "        New_X_mb = torch.tensor(New_X_mb, dtype=torch.float32)\n",
    "        #test loss\n",
    "        G_sample = self.modelG.G_prob(New_X_mb, M_mb)\n",
    "        MSE_final = torch.mean(((1-M_mb)*X_mb-(1-M_mb)*G_sample)**2)/ torch.mean(1-M_mb)\n",
    "        print('Final Test RMSE: ' + str(np.sqrt(MSE_final.item())))\n",
    "        imputed_data = M_mb * X_mb + (1-M_mb) * G_sample\n",
    "        imputed_data = imputed_data.cpu().detach().numpy()\n",
    "        _, dim = imputed_data.shape\n",
    "        renorm_data = imputed_data.copy()\n",
    "        for i in range(dim):\n",
    "            renorm_data[:,i] = renorm_data[:,i] * (max_val[i] + 1e-6)   \n",
    "            renorm_data[:,i] = renorm_data[:,i] + min_val[i]\n",
    "        for i in range(dim):\n",
    "            temp = X[~np.isnan(X[:, i]), i]\n",
    "            # Only for the categorical variable\n",
    "            if len(np.unique(temp)) < 20:\n",
    "                renorm_data[:, i] = np.round(renorm_data[:, i])\n",
    "        return renorm_data\n",
    "        \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if hasattr(X, 'dtypes'):\n",
    "            X = X.to_numpy()\n",
    "        #define mask matrix\n",
    "        X_mask = 1 - np.isnan(X)\n",
    "        #get dimensions\n",
    "        no, dim = X.shape\n",
    "        int_dim = int(dim)\n",
    "        #normalize the original data, and save parameters for renormalization\n",
    "        norm_data = X.copy()\n",
    "        min_val = np.zeros(dim)\n",
    "        max_val = np.zeros(dim)\n",
    "        for i in range(dim):\n",
    "            min_val[i] = np.nanmin(norm_data[i])\n",
    "            norm_data[:, i] -= np.nanmin(norm_data[:, i])\n",
    "            max_val[i] = np.nanmax(norm_data[i])\n",
    "            norm_data[:, i] /= (np.nanmax(norm_data[:, i]) + 1e-06)\n",
    "        norm_parameters = {'min_val': min_val, 'max_val': max_val}\n",
    "        norm_data_filled = np.nan_to_num(norm_data, 0)\n",
    "        #Torch version of Gain\n",
    "        # Initalize discriminator weights, gives hints and data as inputs\n",
    "        D_W1 = torch.tensor(self._xavier_init([dim*2, int_dim]), dtype=torch.float32) \n",
    "        D_b1 = torch.zeros(size=[int_dim], dtype=torch.float32)\n",
    "        D_W2 = torch.tensor(self._xavier_init([int_dim, int_dim]), dtype=torch.float32)\n",
    "        D_b2 = torch.zeros(size=[int_dim], dtype=torch.float32)\n",
    "        D_W3 = torch.tensor(self._xavier_init([int_dim, dim]), dtype=torch.float32)\n",
    "        D_b3 = torch.zeros(size=[dim], dtype=torch.float32)\n",
    "        theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "        self.modelD = self.Discriminator(theta_D).to(self.device)\n",
    "        # Initalize generator weights, gives hints and data as inputs\n",
    "        G_W1 = torch.tensor(self._xavier_init([dim*2, int_dim]), dtype=torch.float32) \n",
    "        G_b1 = torch.zeros(size=[int_dim], dtype=torch.float32)\n",
    "        G_W2 = torch.tensor(self._xavier_init([int_dim, int_dim]), dtype=torch.float32)\n",
    "        G_b2 = torch.zeros(size=[int_dim], dtype=torch.float32)\n",
    "        G_W3 = torch.tensor(self._xavier_init([int_dim, dim]), dtype=torch.float32)\n",
    "        G_b3 = torch.zeros(size=[dim], dtype=torch.float32)\n",
    "        theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
    "        self.modelG = self.Generator(theta_G).to(self.device)\n",
    "        #Data + Mask as inputs (Random noise is in missing components)\n",
    "        def discriminator_loss(M, New_X, H):\n",
    "            G_sample = self.modelG.G_prob(New_X, M)\n",
    "            Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "            D_prob = self.modelD.D_prob(Hat_New_X, H)\n",
    "            D_loss = -torch.mean(M*torch.log(D_prob + 1e-8) \n",
    "                                 + (1-M)*torch.log(1. - D_prob + 1e-8))\n",
    "            return D_loss\n",
    "        def generator_loss(X, M, New_X, H):\n",
    "            G_sample = self.modelG.G_prob(New_X, M)\n",
    "            Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "            D_prob = self.modelD.D_prob(Hat_New_X, H)\n",
    "            G_loss = -torch.mean((1-M)*torch.log(D_prob + 1e-8))\n",
    "            MSE_train_loss = torch.mean(\n",
    "                (M*New_X - M*G_sample)**2\n",
    "                ) / torch.mean(M)\n",
    "            G_loss_final = G_loss + self.alpha*MSE_train_loss\n",
    "            MSE_test_loss = torch.mean(\n",
    "                ((1-M)*X-(1-M)*G_sample)**2\n",
    "                )/ torch.mean(1-M)\n",
    "            return G_loss_final, MSE_train_loss, MSE_test_loss\n",
    "        optimizer_D = torch.optim.Adam(params=self.modelD.parameters())\n",
    "        optimizer_G = torch.optim.Adam(params=self.modelG.parameters())\n",
    "        #Training Iterations\n",
    "        for it in range(self.iterations):\n",
    "            batch_idx = self._sample_batch_index(no, self.batch_size)\n",
    "            X_mb = norm_data_filled[batch_idx, :]\n",
    "            M_mb = X_mask[batch_idx, :]\n",
    "            #sample random vectors\n",
    "            Z_mb = self._uniform_sampler(0, 0.01, self.batch_size, dim)\n",
    "            #Sample hint vectors\n",
    "            H_mb_temp = self._binary_sampler(self.hint_rate, \n",
    "                                             self.batch_size, dim)\n",
    "            H_mb = M_mb * H_mb_temp\n",
    "            #combine vectors with observed vectors\n",
    "            New_X_mb = M_mb*X_mb + (1-M_mb)*Z_mb #Introduce Missin Data\n",
    "\n",
    "            X_mb = torch.tensor(X_mb, dtype=torch.float32)\n",
    "            M_mb = torch.tensor(M_mb, dtype=torch.float32)\n",
    "            H_mb = torch.tensor(H_mb, dtype=torch.float32)\n",
    "            New_X_mb = torch.tensor(New_X_mb, dtype=torch.float32)\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            D_loss_curr = discriminator_loss(M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "            D_loss_curr.requires_grad = True\n",
    "            D_loss_curr.backward()\n",
    "            optimizer_D.step()\n",
    "            optimizer_G.zero_grad()\n",
    "            G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = generator_loss(X=X_mb, M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "            G_loss_curr.requires_grad = True\n",
    "            G_loss_curr.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            if it % 100 == 0:\n",
    "                print('Iter: {}'.format(it))\n",
    "                print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr.item())))\n",
    "                print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr.item())))\n",
    "                print()\n",
    "            \n",
    "\n",
    "        Z_mb = self._uniform_sampler(0, 0.01, no, dim)\n",
    "        M_mb = X_mask\n",
    "        X_mb = norm_data_filled\n",
    "        New_X_mb = M_mb * X_mb + (1 - M_mb)*Z_mb\n",
    "\n",
    "        X_mb = torch.tensor(X_mb, dtype=torch.float32)\n",
    "        M_mb = torch.tensor(M_mb, dtype=torch.float32)\n",
    "        New_X_mb = torch.tensor(New_X_mb, dtype=torch.float32)\n",
    "\n",
    "        print(f'M_mb size: {M_mb.shape}')\n",
    "        print(f'New_X_mb size: {New_X_mb.shape}')\n",
    "        #test loss\n",
    "        G_sample = self.modelG.G_prob(New_X_mb, M_mb)\n",
    "        MSE_final = torch.mean(((1-M_mb)*X_mb-(1-M_mb)*G_sample)**2)/ torch.mean(1-M_mb)\n",
    "        print('Final Test RMSE: ' + str(np.sqrt(MSE_final.item())))\n",
    "        imputed_data = M_mb * X_mb + (1-M_mb) * G_sample\n",
    "        imputed_data = imputed_data.cpu().detach().numpy()\n",
    "        _, dim = imputed_data.shape\n",
    "        renorm_data = imputed_data.copy()\n",
    "        for i in range(dim):\n",
    "            renorm_data[:,i] = renorm_data[:,i] * (max_val[i] + 1e-6)   \n",
    "            renorm_data[:,i] = renorm_data[:,i] + min_val[i]\n",
    "        for i in range(dim):\n",
    "            temp = X[~np.isnan(X[:, i]), i]\n",
    "            # Only for the categorical variable\n",
    "            if len(np.unique(temp)) < 20:\n",
    "                renorm_data[:, i] = np.round(renorm_data[:, i])\n",
    "        return renorm_data\n",
    "    \n",
    "    def _binary_sampler(self, p, rows, cols):\n",
    "        '''Sample binary random variables.\n",
    "        Args:\n",
    "            - p: probability of 1\n",
    "            - rows: the number of rows\n",
    "            - cols: the number of columns\n",
    "        Returns:\n",
    "            - binary_random_matrix: generated binary random matrix.\n",
    "        '''\n",
    "        unif_random_matrix = np.random.uniform(0., 1., size = [rows, cols])\n",
    "        binary_random_matrix = 1*(unif_random_matrix > p)\n",
    "        return binary_random_matrix\n",
    "\n",
    "    def _uniform_sampler(self, low, high, rows, cols):\n",
    "        '''Sample uniform random variables.\n",
    "        Args:\n",
    "            - low: low limit\n",
    "            - high: high limit\n",
    "            - rows: the number of rows\n",
    "            - cols: the number of columns\n",
    "        Returns:\n",
    "            - uniform_random_matrix: generated uniform random matrix.\n",
    "        '''\n",
    "        return np.random.uniform(low, high, size = [rows, cols])       \n",
    "\n",
    "    def _sample_batch_index(self, total, batch_size):\n",
    "        '''Sample index of the mini-batch.\n",
    "        Args:\n",
    "            - total: total number of samples\n",
    "            - batch_size: batch size\n",
    "        Returns:\n",
    "            - batch_idx: batch index\n",
    "        '''\n",
    "        total_idx = np.random.permutation(total)\n",
    "        batch_idx = total_idx[:batch_size]\n",
    "        return batch_idx\n",
    "    \n",
    "    def _xavier_init(self, size):\n",
    "            '''Xavier initialization.\n",
    "            Args:\n",
    "                - size: vector size\n",
    "            Returns:\n",
    "                - initialized random vector.\n",
    "            '''\n",
    "            in_dim = size[0]\n",
    "            xavier_stddev = 1./np.sqrt(in_dim / 2.)\n",
    "            return np.random.normal(scale = xavier_stddev, size=size)\n",
    "    \n",
    "    class Generator(torch.nn.Module):\n",
    "        def __init__(self, params):\n",
    "            super().__init__()\n",
    "            self.theta_G = params\n",
    "            self.G_W1 = self.theta_G[0] \n",
    "            self.G_b1 = self.theta_G[3]\n",
    "            self.G_W2 = self.theta_G[1] \n",
    "            self.G_b2 = self.theta_G[4]\n",
    "            self.G_W3 = self.theta_G[2]\n",
    "            self.G_b3 = self.theta_G[5]\n",
    "\n",
    "        def G_prob(self, X: torch.float32, M: torch.float32):\n",
    "            inputs = torch.concat([X, M], dim=1)\n",
    "            G_h1 = torch.nn.functional.relu(\n",
    "                torch.matmul(inputs, self.G_W1) + self.G_b1\n",
    "                )\n",
    "            G_h2 = torch.nn.functional.relu(\n",
    "                torch.matmul(G_h1, self.G_W2) + self.G_b2\n",
    "                )\n",
    "            g_prob = torch.nn.functional.sigmoid(\n",
    "                torch.matmul(G_h2, self.G_W3) + self.G_b3\n",
    "                )\n",
    "            return g_prob\n",
    "        \n",
    "    class Discriminator(torch.nn.Module):\n",
    "        def __init__(self, params):\n",
    "            super().__init__()\n",
    "            self.theta_D = params\n",
    "            self.D_W1 = self.theta_D[0] \n",
    "            self.D_b1 = self.theta_D[3]\n",
    "            self.D_W2 = self.theta_D[1] \n",
    "            self.D_b2 = self.theta_D[4]\n",
    "            self.D_W3 = self.theta_D[2]\n",
    "            self.D_b3 = self.theta_D[5]\n",
    "        \n",
    "        def D_prob(self, X: torch.float32, H: torch.float32):\n",
    "            inputs = torch.concat([X, H], dim=1)\n",
    "            D_h1 = torch.nn.functional.relu(\n",
    "                torch.matmul(inputs, self.D_W1) + self.D_b1\n",
    "                )\n",
    "            D_h2 = torch.nn.functional.relu(\n",
    "                torch.matmul(D_h1, self.D_W2) + self.D_b2\n",
    "                )\n",
    "            d_prob = torch.nn.functional.sigmoid(\n",
    "                torch.matmul(D_h2, self.D_W3) + self.D_b3\n",
    "                )\n",
    "            return d_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m gain \u001b[38;5;241m=\u001b[39m GainImputer(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, hint_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mgain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m out \u001b[38;5;241m=\u001b[39m gain\u001b[38;5;241m.\u001b[39mtransform(trainX)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrmse_loss\u001b[39m(ori_data, imputed_data, data_m):\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mGainImputer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/tpot2_addimputers/env2/lib/python3.10/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[3], line 145\u001b[0m, in \u001b[0;36mGainImputer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    141\u001b[0m     MSE_test_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m    142\u001b[0m         ((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mM)\u001b[38;5;241m*\u001b[39mX\u001b[38;5;241m-\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mM)\u001b[38;5;241m*\u001b[39mG_sample)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    143\u001b[0m         )\u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mM)\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m G_loss_final, MSE_train_loss, MSE_test_loss\n\u001b[0;32m--> 145\u001b[0m optimizer_D \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodelD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m optimizer_G \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodelG\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m#Training Iterations\u001b[39;00m\n",
      "File \u001b[0;32m~/tpot2_addimputers/env2/lib/python3.10/site-packages/torch/optim/adam.py:73\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     62\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     63\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m~/tpot2_addimputers/env2/lib/python3.10/site-packages/torch/optim/optimizer.py:362\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    360\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    364\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "gain = GainImputer(batch_size=128, hint_rate=0.9, alpha=10, iterations=1000)\n",
    "\n",
    "gain.fit(trainX)\n",
    "\n",
    "out = gain.transform(trainX)\n",
    "\n",
    "def rmse_loss(ori_data, imputed_data, data_m):\n",
    "        '''Compute RMSE loss between ori_data and imputed_data\n",
    "        Args:\n",
    "            - ori_data: original data without missing values\n",
    "            - imputed_data: imputed data\n",
    "            - data_m: indicator matrix for missingness\n",
    "        Returns:\n",
    "            - rmse: Root Mean Squared Error\n",
    "        '''\n",
    "        #ori_data, norm_parameters = normalization(ori_data)\n",
    "        #imputed_data, _ = normalization(imputed_data, norm_parameters)\n",
    "        # Only for missing values\n",
    "        nominator = np.sum(((1-data_m) * ori_data - (1-data_m) * imputed_data)**2)\n",
    "        denominator = np.sum(1-data_m)\n",
    "        rmse = np.sqrt(nominator/float(denominator))\n",
    "        return rmse\n",
    "\n",
    "print(rmse_loss(ori_data=trainX, imputed_data=out, data_m=trainM))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
