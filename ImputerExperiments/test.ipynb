{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted, _check_feature_names_in\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn\n",
    "import sklearn.impute\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import sklearn.compose\n",
    "\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = '/Users/gabrielketron/tpot2_addimputers/tpot2/ImputerExperiments/data/Spam.csv'\n",
    "#%% System Parameters\n",
    "# 1. Mini batch size\n",
    "mb_size = 128\n",
    "# 2. Missing rate\n",
    "p_miss = 0.2\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "# 4. Loss Hyperparameters\n",
    "alpha = 10\n",
    "# 5. Train Rate\n",
    "train_rate = 0.8\n",
    "\n",
    "#%% Data\n",
    "\n",
    "# Data generation\n",
    "Data = np.loadtxt(dataset_file, delimiter=\",\",skiprows=1)\n",
    "X = pd.DataFrame(Data)\n",
    "no, dim = X.shape\n",
    "idx = np.random.permutation(no)\n",
    "Train_No = int(no * 0.7)\n",
    "Test_No = no - Train_No\n",
    "trainX = X.iloc[0:Train_No]\n",
    "testX = X.iloc[Train_No+1:]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GainImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Base class for all imputers.\n",
    "    It adds automatically support for `add_indicator`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 batch_size=128, \n",
    "                 hint_rate=0.9, \n",
    "                 alpha=100, \n",
    "                 iterations=10000,\n",
    "                 train_rate = 0.8,\n",
    "                 learning_rate = 0.001,\n",
    "                 p_miss = 0.2,\n",
    "                 random_state=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.hint_rate = hint_rate\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.train_rate = train_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.p_miss = p_miss\n",
    "        self.random_state = random_state\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "            )\n",
    "        torch.set_default_device(self.device)\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        torch.set_grad_enabled(True)\n",
    "        if random_state is not None:\n",
    "            torch.manual_seed(self.random_state)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.fit_transform(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        \n",
    "        self.modelG.load_state_dict(self._Gen_params)\n",
    "\n",
    "        if hasattr(X, 'dtypes'):\n",
    "            X = X.to_numpy()\n",
    "        #define mask matrix\n",
    "        X_mask = 1 - np.isnan(X)\n",
    "        #get dimensions\n",
    "        no, self.dim = X.shape\n",
    "        self.int_dim = int(self.dim)\n",
    "        #normalize the original data, and save parameters for renormalization\n",
    "        norm_data = X.copy()\n",
    "        min_val = np.zeros(self.dim)\n",
    "        max_val = np.zeros(self.dim)\n",
    "        for i in range(self.dim):\n",
    "            min_val[i] = np.nanmin(norm_data[i])\n",
    "            norm_data[:, i] -= np.nanmin(norm_data[:, i])\n",
    "            max_val[i] = np.nanmax(norm_data[i])\n",
    "            norm_data[:, i] /= (np.nanmax(norm_data[:, i]) + 1e-06)\n",
    "        norm_parameters = {'min_val': min_val, 'max_val': max_val}\n",
    "        norm_data_filled = np.nan_to_num(norm_data, 0)\n",
    "        p_miss_vec = self.p_miss * np.ones((self.dim,1)) \n",
    "        Missing = np.zeros((no,self.dim))\n",
    "        for i in range(self.dim):\n",
    "            A = np.random.uniform(0., 1., size = [len(norm_data_filled),])\n",
    "            B = A > p_miss_vec[i]\n",
    "            Missing[:,i] = 1.*B\n",
    "\n",
    "        Z_mb = self._sample_Z(no, self.dim)\n",
    "        M_mb = Missing\n",
    "        X_mb = norm_data_filled\n",
    "\n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb\n",
    "\n",
    "        X_mb = torch.tensor(X_mb, dtype=torch.float32)\n",
    "        New_X_mb = torch.tensor(New_X_mb, dtype=torch.float32)\n",
    "        M_mb = torch.tensor(M_mb, dtype=torch.float32)\n",
    "\n",
    "        G_sample = self.modelG(X_mb, New_X_mb, M_mb)\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        mse_final = mse_loss((1-M_mb)*X_mb, (1-M_mb)*G_sample)/(1-M_mb).sum()\n",
    "        print('Final Test RMSE: ' + str(np.sqrt(mse_final.item())))\n",
    "\n",
    "        imputed_data = M_mb * X_mb + (1-M_mb) * G_sample\n",
    "        imputed_data = imputed_data.cpu().detach().numpy()\n",
    "        _, dim = imputed_data.shape\n",
    "        renorm_data = imputed_data.copy()\n",
    "        for i in range(dim):\n",
    "            renorm_data[:,i] = renorm_data[:,i] * (max_val[i] + 1e-6)   \n",
    "            renorm_data[:,i] = renorm_data[:,i] + min_val[i]\n",
    "        for i in range(dim):\n",
    "            temp = X[~np.isnan(X[:, i]), i]\n",
    "            # Only for the categorical variable\n",
    "            if len(np.unique(temp)) < 20:\n",
    "                renorm_data[:, i] = np.round(renorm_data[:, i])\n",
    "        return \n",
    "        \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if hasattr(X, 'dtypes'):\n",
    "            X = X.to_numpy()\n",
    "        #define mask matrix\n",
    "        X_mask = 1 - np.isnan(X)\n",
    "        #get dimensions\n",
    "        no, self.dim = X.shape\n",
    "        self.int_dim = int(self.dim)\n",
    "        #normalize the original data, and save parameters for renormalization\n",
    "        norm_data = X.copy()\n",
    "        min_val = np.zeros(self.dim)\n",
    "        max_val = np.zeros(self.dim)\n",
    "        for i in range(self.dim):\n",
    "            min_val[i] = np.nanmin(norm_data[i])\n",
    "            norm_data[:, i] -= np.nanmin(norm_data[:, i])\n",
    "            max_val[i] = np.nanmax(norm_data[i])\n",
    "            norm_data[:, i] /= (np.nanmax(norm_data[:, i]) + 1e-06)\n",
    "        norm_parameters = {'min_val': min_val, 'max_val': max_val}\n",
    "        norm_data_filled = np.nan_to_num(norm_data, 0)\n",
    "        p_miss_vec = self.p_miss * np.ones((self.dim,1)) \n",
    "        Missing = np.zeros((no,self.dim))\n",
    "        for i in range(self.dim):\n",
    "            A = np.random.uniform(0., 1., size = [len(norm_data_filled),])\n",
    "            B = A > p_miss_vec[i]\n",
    "            Missing[:,i] = 1.*B\n",
    "        #internal test-train split\n",
    "        # Train / Test Missing Indicators\n",
    "        #model training\n",
    "        self.modelD = self.Discriminator(GainImputer=self)\n",
    "        self.modelG = self.Generator(GainImputer=self)\n",
    "\n",
    "        optimizer_D = torch.optim.Adam(self.modelD.parameters(), \n",
    "                                       lr = self.learning_rate)\n",
    "        optimizer_G = torch.optim.Adam(self.modelG.parameters(), \n",
    "                                       lr = self.learning_rate)\n",
    "        \n",
    "        bce_loss = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "        for it in range(self.iterations):\n",
    "            mb_idx = self._sample_index(no, self.batch_size)\n",
    "            X_mb = norm_data_filled[mb_idx,:]\n",
    "            Z_mb = self._sample_Z(self.batch_size, self.dim)\n",
    "\n",
    "            M_mb = Missing[mb_idx, :]\n",
    "            H_mb1 = self._sample_M(self.batch_size, self.dim, 1-self.hint_rate)\n",
    "            H_mb = M_mb*H_mb1 + 0.5*(1-H_mb1)\n",
    "\n",
    "            New_X_mb = M_mb * X_mb + (1-M_mb)*Z_mb #introduce missing data\n",
    "\n",
    "            X_mb = torch.tensor(X_mb, dtype=torch.float32)\n",
    "            New_X_mb = torch.tensor(New_X_mb, dtype=torch.float32)\n",
    "            Z_mb = torch.tensor(Z_mb, dtype=torch.float32)\n",
    "            M_mb = torch.tensor(M_mb, dtype=torch.float32)\n",
    "            H_mb = torch.tensor(H_mb, dtype=torch.float32)\n",
    "\n",
    "            #Train Discriminator\n",
    "            G_sample = self.modelG(X_mb, New_X_mb, M_mb)\n",
    "            D_prob = self.modelD(X_mb, M_mb, G_sample, H_mb)\n",
    "            D_loss = bce_loss(D_prob, M_mb)\n",
    "\n",
    "            D_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            #Train Generator\n",
    "            G_sample = self.modelG(X_mb, New_X_mb, M_mb)\n",
    "            D_prob = self.modelD(X_mb, M_mb, G_sample, H_mb)\n",
    "            D_prob.cpu().detach()\n",
    "            G_loss1 = ((1-M_mb)*(torch.sigmoid(D_prob)+1e-8).log()).mean()/(1-M_mb).sum()\n",
    "            G_mse_loss = mse_loss(M_mb*X_mb, M_mb*G_sample)/M_mb.sum()\n",
    "            G_loss = G_loss1 + self.alpha*G_mse_loss\n",
    "\n",
    "            G_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            G_mse_test = mse_loss((1-M_mb)*X_mb, (1-M_mb)*G_sample)/(1-M_mb).sum()\n",
    "\n",
    "            '''if it % 100 == 0:\n",
    "                print('Iter: {}'.format(it))\n",
    "                print('D_loss: {:.4}'.format(D_loss))\n",
    "                print('Train_loss: {:.4}'.format(G_mse_loss))\n",
    "                print('Test_loss: {:.4}'.format(G_mse_test))\n",
    "                print()'''\n",
    "        self._Gen_params = self.modelG.state_dict()\n",
    "\n",
    "        Z_mb = self._sample_Z(no, self.dim) \n",
    "        M_mb = Missing\n",
    "        X_mb = norm_data_filled\n",
    "   \n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb\n",
    "\n",
    "        X_mb = torch.tensor(X_mb, dtype=torch.float32)\n",
    "        New_X_mb = torch.tensor(New_X_mb, dtype=torch.float32)\n",
    "        M_mb = torch.tensor(M_mb, dtype=torch.float32)\n",
    "\n",
    "        G_sample = self.modelG(X_mb, New_X_mb, M_mb)\n",
    "        mse_final = mse_loss((1-M_mb)*X_mb, (1-M_mb)*G_sample)/(1-M_mb).sum()\n",
    "        #print('Final Train RMSE: ' + str(np.sqrt(mse_final.item())))\n",
    "\n",
    "        imputed_data = M_mb * X_mb + (1-M_mb) * G_sample\n",
    "        imputed_data = imputed_data.cpu().detach().numpy()\n",
    "        _, dim = imputed_data.shape\n",
    "        renorm_data = imputed_data.copy()\n",
    "        for i in range(dim):\n",
    "            renorm_data[:,i] = renorm_data[:,i] * (max_val[i] + 1e-6)   \n",
    "            renorm_data[:,i] = renorm_data[:,i] + min_val[i]\n",
    "        for i in range(dim):\n",
    "            temp = X[~np.isnan(X[:, i]), i]\n",
    "            # Only for the categorical variable\n",
    "            if len(np.unique(temp)) < 20:\n",
    "                renorm_data[:, i] = np.round(renorm_data[:, i])\n",
    "        return renorm_data\n",
    "    \n",
    "    def _sample_M(self, rows, cols, p):\n",
    "        '''Sample binary random variables.\n",
    "        Args:\n",
    "            - p: probability of 1\n",
    "            - rows: the number of rows\n",
    "            - cols: the number of columns\n",
    "        Returns:\n",
    "            - binary_random_matrix: generated binary random matrix.\n",
    "        '''\n",
    "        unif_random_matrix = np.random.uniform(0., 1., size = [rows, cols])\n",
    "        binary_random_matrix = unif_random_matrix > p\n",
    "        return 1.*binary_random_matrix\n",
    "\n",
    "    def _sample_Z(self, rows, cols):\n",
    "        '''Sample uniform random variables.\n",
    "        Args:\n",
    "            - rows: the number of rows\n",
    "            - cols: the number of columns\n",
    "        Returns:\n",
    "            - uniform_random_matrix: generated uniform random matrix.\n",
    "        '''\n",
    "        return np.random.uniform(0., 1., size = [rows, cols])       \n",
    "\n",
    "    def _sample_index(self, rows, batch_size):\n",
    "        '''Sample index of the mini-batch.\n",
    "        Args:\n",
    "            - total: total number of samples (rows)\n",
    "            - batch_size: batch size\n",
    "        Returns:\n",
    "            - batch_idx: batch index\n",
    "        '''\n",
    "        total_idx = np.random.permutation(rows)\n",
    "        batch_idx = total_idx[:batch_size]\n",
    "        return batch_idx\n",
    "    \n",
    "    class Generator(torch.nn.Module):\n",
    "        def __init__(self, GainImputer):\n",
    "            super(GainImputer.Generator, self).__init__()\n",
    "            self.G1 = torch.nn.Linear(GainImputer.dim*2,GainImputer.int_dim)\n",
    "            self.G2 = torch.nn.Linear(GainImputer.int_dim,GainImputer.int_dim)\n",
    "            self.G3 = torch.nn.Linear(GainImputer.int_dim,GainImputer.dim)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.init_weight()\n",
    "\n",
    "        def init_weight(self):\n",
    "            layers = [self.G1, self.G2, self.G3]\n",
    "            [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "\n",
    "        def forward(self, X: torch.float32, Z: torch.float32, M: torch.float32):\n",
    "            input = M * X + (1-M)*Z\n",
    "            input = torch.cat([input, M], dim=1)\n",
    "            out = self.relu(self.G1(input))\n",
    "            out = self.relu(self.G2(out))\n",
    "            out = self.sigmoid(self.G3(out))\n",
    "            return out\n",
    "        \n",
    "    class Discriminator(torch.nn.Module):\n",
    "        def __init__(self, GainImputer):\n",
    "            super(GainImputer.Discriminator, self).__init__()\n",
    "            self.D1 = torch.nn.Linear(GainImputer.dim*2,GainImputer.int_dim)\n",
    "            self.D2 = torch.nn.Linear(GainImputer.int_dim,GainImputer.int_dim)\n",
    "            self.D3 = torch.nn.Linear(GainImputer.int_dim,GainImputer.dim)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.init_weight()\n",
    "        \n",
    "        def init_weight(self):\n",
    "            layers = [self.D1, self.D2, self.D3]\n",
    "            [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "        \n",
    "        def forward(self, X, M, G, H):\n",
    "            input = M * X + (1-M)*G\n",
    "            input = torch.cat([input, H], dim=1)\n",
    "            out = self.relu(self.D1(input))\n",
    "            out = self.relu(self.D2(out))\n",
    "            out = self.D3(out)\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test RMSE: 0.0034147700228270507\n"
     ]
    }
   ],
   "source": [
    "gain = GainImputer(batch_size=128, hint_rate=0.9, alpha=10, iterations=10000)\n",
    "\n",
    "gain.fit(trainX)\n",
    "\n",
    "out = gain.transform(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
