{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielketron/tpot2_addimputers/env2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted, _check_feature_names_in\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn\n",
    "import sklearn.impute\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import sklearn.compose\n",
    "from scipy import optimize\n",
    "\n",
    "import torch\n",
    "import sklearn.preprocessing\n",
    "import openml\n",
    "import tpot2\n",
    "import sklearn.metrics\n",
    "import sklearn\n",
    "from sklearn.metrics import (roc_auc_score, roc_curve, precision_score, auc, recall_score, precision_recall_curve, \\\n",
    "                             roc_auc_score, accuracy_score, balanced_accuracy_score, f1_score, log_loss,\n",
    "                             f1_score, root_mean_squared_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing(X, add_missing = 0.05, missing_type = 'MAR'):\n",
    "    if isinstance(X,np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    missing_mask = X\n",
    "    missing_mask = missing_mask.mask(missing_mask.isna(), True)\n",
    "    missing_mask = missing_mask.mask(missing_mask.notna(), False)\n",
    "    X = X.mask(X.isna(), 0)\n",
    "    T = torch.tensor(X.to_numpy())\n",
    "\n",
    "    match missing_type:\n",
    "        case 'MAR':\n",
    "            out = MAR(T, [add_missing])\n",
    "        case 'MCAR':\n",
    "            out = MCAR(T, [add_missing])\n",
    "        case 'MNAR':\n",
    "            out = MNAR_mask_logistic(T, [add_missing])\n",
    "    \n",
    "    masked_set = pd.DataFrame(out['Mask'].numpy())\n",
    "    missing_combo = (missing_mask | masked_set.isna())\n",
    "    masked_set = masked_set.mask(missing_combo, True)\n",
    "    masked_set.columns = X.columns.values\n",
    "    #masked_set = masked_set.to_numpy()\n",
    "\n",
    "    missing_set = pd.DataFrame(out['Missing'].numpy())\n",
    "    missing_set.columns = X.columns.values\n",
    "    #missing_set = missing_set.to_numpy()\n",
    "\n",
    "    return missing_set, masked_set\n",
    "\n",
    "\"\"\"BEYOND THIS POINT WRITTEN BY Aude Sportisse, Marine Le Morvan and Boris Muzellec - https://rmisstastic.netlify.app/how-to/python/generate_html/how%20to%20generate%20missing%20values\"\"\"\n",
    "\n",
    "def MCAR(X, p_miss):\n",
    "    out = {'X': X.double()}\n",
    "    for p in p_miss: \n",
    "        mask = (torch.rand(X.shape) < p).double()\n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "        model_name = 'Missing'\n",
    "        mask_name = 'Mask'\n",
    "        out[model_name] = X_nas\n",
    "        out[mask_name] = mask\n",
    "    return out\n",
    "\n",
    "def MAR(X,p_miss,p_obs=0.5):\n",
    "    out = {'X': X.double()}\n",
    "    for p in p_miss:\n",
    "        n, d = X.shape\n",
    "        mask = torch.zeros(n, d).bool()\n",
    "        num_no_missing = max(int(p_obs * d), 1)\n",
    "        num_missing = d - num_no_missing\n",
    "        obs_samples = np.random.choice(d, num_no_missing, replace=False)\n",
    "        copy_samples = np.array([i for i in range(d) if i not in obs_samples])\n",
    "        len_obs = len(obs_samples)\n",
    "        len_na = len(copy_samples)\n",
    "        coeffs = torch.randn(len_obs, len_na).double()\n",
    "        Wx = X[:, obs_samples].mm(coeffs)\n",
    "        coeffs /= torch.std(Wx, 0, keepdim=True)\n",
    "        coeffs.double()\n",
    "        len_obs, len_na = coeffs.shape\n",
    "        intercepts = torch.zeros(len_na)\n",
    "        for j in range(len_na):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X[:, obs_samples].mv(coeffs[:, j]) + x).mean().item() - p\n",
    "            intercepts[j] = optimize.bisect(f, -50, 50)\n",
    "        ps = torch.sigmoid(X[:, obs_samples].mm(coeffs) + intercepts)\n",
    "        ber = torch.rand(n, len_na)\n",
    "        mask[:, copy_samples] = ber < ps\n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "        model_name = 'Missing'\n",
    "        mask_name = 'Mask'\n",
    "        out[model_name] = X_nas\n",
    "        out[mask_name] = mask\n",
    "    return out\n",
    "\n",
    "def MNAR_mask_logistic(X, p_miss, p_params =.5, exclude_inputs=True):\n",
    "    \"\"\"\n",
    "    Missing not at random mechanism with a logistic masking model. It implements two mechanisms:\n",
    "    (i) Missing probabilities are selected with a logistic model, taking all variables as inputs. Hence, values that are\n",
    "    inputs can also be missing.\n",
    "    (ii) Variables are split into a set of intputs for a logistic model, and a set whose missing probabilities are\n",
    "    determined by the logistic model. Then inputs are then masked MCAR (hence, missing values from the second set will\n",
    "    depend on masked values.\n",
    "    In either case, weights are random and the intercept is selected to attain the desired proportion of missing values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.DoubleTensor or np.ndarray, shape (n, d)\n",
    "        Data for which missing values will be simulated.\n",
    "        If a numpy array is provided, it will be converted to a pytorch tensor.\n",
    "    p : float\n",
    "        Proportion of missing values to generate for variables which will have missing values.\n",
    "    p_params : float\n",
    "        Proportion of variables that will be used for the logistic masking model (only if exclude_inputs).\n",
    "    exclude_inputs : boolean, default=True\n",
    "        True: mechanism (ii) is used, False: (i)\n",
    "    Returns\n",
    "    -------\n",
    "    mask : torch.BoolTensor or np.ndarray (depending on type of X)\n",
    "        Mask of generated missing values (True if the value is missing).\n",
    "    \"\"\"\n",
    "    out = {'X_init_MNAR': X.double()}\n",
    "    for p in p_miss: \n",
    "        n, d = X.shape\n",
    "        to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "        if not to_torch:\n",
    "            X = torch.from_numpy(X)\n",
    "        mask = torch.zeros(n, d).bool() if to_torch else np.zeros((n, d)).astype(bool)\n",
    "        d_params = max(int(p_params * d), 1) if exclude_inputs else d ## number of variables used as inputs (at least 1)\n",
    "        d_na = d - d_params if exclude_inputs else d ## number of variables masked with the logistic model\n",
    "        ### Sample variables that will be parameters for the logistic regression:\n",
    "        idxs_params = np.random.choice(d, d_params, replace=False) if exclude_inputs else np.arange(d)\n",
    "        idxs_nas = np.array([i for i in range(d) if i not in idxs_params]) if exclude_inputs else np.arange(d)\n",
    "        ### Other variables will have NA proportions selected by a logistic model\n",
    "        ### The parameters of this logistic model are random.\n",
    "        ### Pick coefficients so that W^Tx has unit variance (avoids shrinking)\n",
    "        len_obs = len(idxs_params)\n",
    "        len_na = len(idxs_nas)\n",
    "        coeffs = torch.randn(len_obs, len_na).double()\n",
    "        Wx = X[:, idxs_params].mm(coeffs)\n",
    "        coeffs /= torch.std(Wx, 0, keepdim=True)\n",
    "        coeffs.double()\n",
    "        ### Pick the intercepts to have a desired amount of missing values\n",
    "        len_obs, len_na = coeffs.shape\n",
    "        intercepts = torch.zeros(len_na)\n",
    "        for j in range(len_na):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X[:, idxs_params].mv(coeffs[:, j]) + x).mean().item() - p\n",
    "            intercepts[j] = optimize.bisect(f, -50, 50)\n",
    "        ps = torch.sigmoid(X[:, idxs_params].mm(coeffs) + intercepts)\n",
    "        ber = torch.rand(n, d_na)\n",
    "        mask[:, idxs_nas] = ber < ps\n",
    "        ## If the inputs of the logistic model are excluded from MNAR missingness,\n",
    "        ## mask some values used in the logistic model at random.\n",
    "        ## This makes the missingness of other variables potentially dependent on masked values\n",
    "        if exclude_inputs:\n",
    "            mask[:, idxs_params] = torch.rand(n, d_params) < p\n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "        model_name = 'Missing'\n",
    "        mask_name = 'Mask'\n",
    "        out[model_name] = X_nas\n",
    "        out[mask_name] = mask\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_pipeline_full = tpot2.search_spaces.pipelines.SequentialPipeline([\n",
    "        tpot2.config.get_search_space(\"imputers\"), \n",
    "        tpot2.config.get_search_space(\"regressors\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_full = {\n",
    "                    'scorers':['neg_root_mean_squared_error'],\n",
    "                    'scorers_weights':[1],\n",
    "                    'population_size' : 5,\n",
    "                    'survival_percentage':1, \n",
    "                    'initial_population_size' : 5,\n",
    "                    'generations' : 5, \n",
    "                    'n_jobs':5,\n",
    "                    'cv': sklearn.model_selection.KFold(n_splits=10, shuffle=True, random_state=0),\n",
    "                    'verbose':5, \n",
    "                    'max_time_seconds': 360000,\n",
    "                    'max_eval_time_seconds':60*10, \n",
    "                    'classification' : False,\n",
    "                    'search_space': regression_pipeline_full,\n",
    "                    'preprocessing':False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(base_save_folder, task_id, r_or_c):\n",
    "    \n",
    "    cached_data_path = f\"{base_save_folder}/{task_id}.pkl\"\n",
    "    print(cached_data_path)\n",
    "    if os.path.exists(cached_data_path):\n",
    "        d = pickle.load(open(cached_data_path, \"rb\"))\n",
    "        X_train, y_train, X_test, y_test = d['X_train'], d['y_train'], d['X_test'], d['y_test']\n",
    "    else:\n",
    "        #kwargs = {'force_refresh_cache': True}\n",
    "        task = openml.datasets.get_dataset(task_id)\n",
    "        X, y, _, _  = task.get_data(dataset_format=\"dataframe\")\n",
    "        print(X)\n",
    "        print(y)\n",
    "        if y is None: \n",
    "            y = X.iloc[:, -1:]\n",
    "            X = X.iloc[:, :-1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "        preprocessing_pipeline = sklearn.pipeline.make_pipeline(\n",
    "            tpot2.builtin_modules.ColumnSimpleImputer(\n",
    "                \"categorical\", strategy='most_frequent'), \n",
    "            tpot2.builtin_modules.ColumnSimpleImputer(\n",
    "                \"numeric\", strategy='mean'), \n",
    "                tpot2.builtin_modules.ColumnOneHotEncoder(\n",
    "                    \"categorical\", min_frequency=0.001, handle_unknown=\"ignore\")\n",
    "            )\n",
    "        X_train = preprocessing_pipeline.fit_transform(X_train)\n",
    "        X_test = preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "        X_train = sklearn.preprocessing.normalize(X_train)\n",
    "        X_test = sklearn.preprocessing.normalize(X_test)\n",
    "\n",
    "        if r_or_c =='c':\n",
    "            le = sklearn.preprocessing.LabelEncoder()\n",
    "            y_train = le.fit_transform(y_train)\n",
    "            y_test = le.transform(y_test)\n",
    "\n",
    "        d = {\"X_train\": X_train, \"y_train\": y_train, \"X_test\": X_test, \"y_test\": y_test}\n",
    "        if not os.path.exists(f\"{base_save_folder}\"):\n",
    "            os.makedirs(f\"{base_save_folder}\")\n",
    "        with open(cached_data_path, \"wb\") as f:\n",
    "            pickle.dump(d, f)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpipe = regression_pipeline_full.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/197.pkl\n",
      "      lread  lwrite   scall  sread  swrite   fork   exec     rchar     wchar  \\\n",
      "0       6.0     2.0  1036.0  103.0   114.0   1.00   1.00  172076.0  355965.0   \n",
      "1       1.0     0.0  2165.0  205.0   101.0   0.40   1.20   43107.0   44139.0   \n",
      "2      62.0    77.0  3806.0  258.0   166.0   1.40   1.40  492142.0  268706.0   \n",
      "3       5.0     0.0  4721.0  256.0   177.0   0.99   2.58  524787.0  174964.0   \n",
      "4      42.0    55.0  3949.0  249.0   244.0   2.60   4.60  197289.0  529200.0   \n",
      "...     ...     ...     ...    ...     ...    ...    ...       ...       ...   \n",
      "8187   74.0    49.0  2688.0  176.0   103.0  11.00  32.20   57714.0   38484.0   \n",
      "8188   29.0    40.0  1906.0  118.0    90.0   0.80   2.00    8175.0   27313.0   \n",
      "8189    3.0     0.0   926.0   90.0    67.0   0.60   1.00    5411.0   19322.0   \n",
      "8190    4.0     0.0   418.0   30.0    29.0   0.80   1.00    3959.0   10679.0   \n",
      "8191    5.0     0.0  1888.0  248.0   215.0   6.20   1.80  216420.0   39346.0   \n",
      "\n",
      "      pgout  ...  pgscan  atch   pgin  ppgin    pflt    vflt  runqsz  freemem  \\\n",
      "0      0.00  ...    0.00  0.00   2.00   4.00   73.60   89.00     2.0   6527.0   \n",
      "1      4.80  ...  181.40  0.20  85.40  88.20   19.40  161.80     3.0    130.0   \n",
      "2      4.80  ...   79.20  2.20   7.60  12.20   68.00  218.80     5.2    256.0   \n",
      "3     14.51  ...  189.86  1.99   4.17  24.85   95.63  248.91     1.0    233.0   \n",
      "4      4.20  ...    0.00  1.40   1.80   2.20  219.60  297.20     3.4    331.0   \n",
      "...     ...  ...     ...   ...    ...    ...     ...     ...     ...      ...   \n",
      "8187   0.80  ...    0.00  0.00   0.80   0.80  343.20  649.40     7.0    314.0   \n",
      "8188   0.00  ...    0.00  0.00   0.80   0.80   56.20   78.60     3.6    166.0   \n",
      "8189   0.00  ...    0.00  0.40   0.40   0.40   53.40  154.00     1.0   1177.0   \n",
      "8190   0.00  ...    0.00  0.00   0.20   0.20   61.00   73.20     2.4   6355.0   \n",
      "8191   0.00  ...    0.00  0.00   7.40  14.80  296.60  420.20     4.6   1628.0   \n",
      "\n",
      "       freeswap  usr  \n",
      "0     1851864.0   90  \n",
      "1     1131931.0   88  \n",
      "2     1314590.0   85  \n",
      "3      972606.0   81  \n",
      "4     1013805.0   79  \n",
      "...         ...  ...  \n",
      "8187  1096333.0   69  \n",
      "8188  1107088.0   88  \n",
      "8189  1020400.0   92  \n",
      "8190  1702592.0   96  \n",
      "8191  1757696.0   80  \n",
      "\n",
      "[8192 rows x 22 columns]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/05y1vdbd3vd52x71qyhklyjw0000gn/T/ipykernel_6124/3242815643.py:10: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  task = openml.datasets.get_dataset(task_id)\n",
      "/var/folders/b8/05y1vdbd3vd52x71qyhklyjw0000gn/T/ipykernel_6124/518150171.py:6: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  missing_mask = missing_mask.mask(missing_mask.notna(), False)\n",
      "/var/folders/b8/05y1vdbd3vd52x71qyhklyjw0000gn/T/ipykernel_6124/518150171.py:6: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  missing_mask = missing_mask.mask(missing_mask.notna(), False)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_task(base_save_folder='../data', task_id=197, r_or_c= 'r')\n",
    "for level in [0.3]:\n",
    "        for type_1 in ['MNAR']:\n",
    "                X_train = pd.DataFrame(X_train)\n",
    "                X_test = pd.DataFrame(X_test)\n",
    "                X_train_M, mask_train = add_missing(X_train, add_missing=level, missing_type=type_1)\n",
    "                X_test_M, mask_test = add_missing(X_test, add_missing=level, missing_type=type_1)\n",
    "                X_train_n = X_train_M.to_numpy()\n",
    "                X_test_n = X_test_M.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2215\n",
       "1     2204\n",
       "2     2177\n",
       "3     2249\n",
       "4     2188\n",
       "5     2253\n",
       "6     2171\n",
       "7     2272\n",
       "8     2195\n",
       "9     2156\n",
       "10    2200\n",
       "11    2210\n",
       "12    2194\n",
       "13    2141\n",
       "14    2154\n",
       "15    2174\n",
       "16    2196\n",
       "17    2246\n",
       "18    2262\n",
       "19    2209\n",
       "20    2231\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_M.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;simpleimputer&#x27;, SimpleImputer()),\n",
       "                (&#x27;xgbregressor&#x27;,\n",
       "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                              colsample_bylevel=None, colsample_bynode=None,\n",
       "                              colsample_bytree=None, device=None,\n",
       "                              early_stopping_rounds=None,\n",
       "                              enable_categorical=False, eval_metric=None,\n",
       "                              feature_types=None, gamma=0.000308538695,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None,\n",
       "                              learning_rate=0.0164213396083, max_bin=None,\n",
       "                              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                              max_delta_step=None, max_depth=17,\n",
       "                              max_leaves=None, min_child_weight=7, missing=nan,\n",
       "                              monotone_constraints=None, multi_strategy=None,\n",
       "                              n_estimators=100, n_jobs=1, nthread=1,\n",
       "                              num_parallel_tree=None, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;simpleimputer&#x27;, SimpleImputer()),\n",
       "                (&#x27;xgbregressor&#x27;,\n",
       "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                              colsample_bylevel=None, colsample_bynode=None,\n",
       "                              colsample_bytree=None, device=None,\n",
       "                              early_stopping_rounds=None,\n",
       "                              enable_categorical=False, eval_metric=None,\n",
       "                              feature_types=None, gamma=0.000308538695,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None,\n",
       "                              learning_rate=0.0164213396083, max_bin=None,\n",
       "                              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                              max_delta_step=None, max_depth=17,\n",
       "                              max_leaves=None, min_child_weight=7, missing=nan,\n",
       "                              monotone_constraints=None, multi_strategy=None,\n",
       "                              n_estimators=100, n_jobs=1, nthread=1,\n",
       "                              num_parallel_tree=None, ...))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">XGBRegressor</label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=0.000308538695, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.0164213396083,\n",
       "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=17, max_leaves=None,\n",
       "             min_child_weight=7, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=100, n_jobs=1, nthread=1,\n",
       "             num_parallel_tree=None, ...)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('simpleimputer', SimpleImputer()),\n",
       "                ('xgbregressor',\n",
       "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                              colsample_bylevel=None, colsample_bynode=None,\n",
       "                              colsample_bytree=None, device=None,\n",
       "                              early_stopping_rounds=None,\n",
       "                              enable_categorical=False, eval_metric=None,\n",
       "                              feature_types=None, gamma=0.000308538695,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None,\n",
       "                              learning_rate=0.0164213396083, max_bin=None,\n",
       "                              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                              max_delta_step=None, max_depth=17,\n",
       "                              max_leaves=None, min_child_weight=7, missing=nan,\n",
       "                              monotone_constraints=None, multi_strategy=None,\n",
       "                              n_estimators=100, n_jobs=1, nthread=1,\n",
       "                              num_parallel_tree=None, ...))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpipe.fit(X_train_M, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('simpleimputer', SimpleImputer()),\n",
      "                ('xgbregressor',\n",
      "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "                              colsample_bylevel=None, colsample_bynode=None,\n",
      "                              colsample_bytree=None, device=None,\n",
      "                              early_stopping_rounds=None,\n",
      "                              enable_categorical=False, eval_metric=None,\n",
      "                              feature_types=None, gamma=0.000308538695,\n",
      "                              grow_policy=None, importance_type=None,\n",
      "                              interaction_constraints=None,\n",
      "                              learning_rate=0.0164213396083, max_bin=None,\n",
      "                              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "                              max_delta_step=None, max_depth=17,\n",
      "                              max_leaves=None, min_child_weight=7, missing=nan,\n",
      "                              monotone_constraints=None, multi_strategy=None,\n",
      "                              n_estimators=100, n_jobs=1, nthread=1,\n",
      "                              num_parallel_tree=None, ...))])\n"
     ]
    }
   ],
   "source": [
    "print(testpipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/05y1vdbd3vd52x71qyhklyjw0000gn/T/ipykernel_5982/518150171.py:6: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  missing_mask = missing_mask.mask(missing_mask.notna(), False)\n"
     ]
    }
   ],
   "source": [
    "dataset_file = '/Users/gabrielketron/tpot2_addimputers/tpot2/ImputerExperiments/data/Spam.csv'\n",
    "#%% System Parameters\n",
    "# 1. Mini batch size\n",
    "mb_size = 128\n",
    "# 2. Missing rate\n",
    "p_miss = 0.2\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "# 4. Loss Hyperparameters\n",
    "alpha = 10\n",
    "# 5. Train Rate\n",
    "train_rate = 0.8\n",
    "\n",
    "#%% Data\n",
    "\n",
    "# Data generation\n",
    "Data = np.loadtxt(dataset_file, delimiter=\",\",skiprows=1)\n",
    "X = pd.DataFrame(Data)\n",
    "\n",
    "X_M, mask = add_missing(X)\n",
    "\n",
    "no, dim = X_M.shape\n",
    "idx = np.random.permutation(no)\n",
    "\n",
    "Train_No = int(no * 0.7)\n",
    "Test_No = no - Train_No\n",
    "    \n",
    "# Train / Test Features\n",
    "trainX = X_M.iloc[:Train_No]\n",
    "testX = X_M.iloc[Train_No:]\n",
    "\n",
    "# Train / Test Missing Indicators\n",
    "trainM = mask.iloc[:Train_No]\n",
    "testM = mask.iloc[Train_No:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GainImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Base class for all imputers.\n",
    "    It adds automatically support for `add_indicator`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 batch_size=128, \n",
    "                 hint_rate=0.9, \n",
    "                 alpha=100, \n",
    "                 iterations=10000,\n",
    "                 train_rate = 0.8,\n",
    "                 learning_rate = 0.001,\n",
    "                 p_miss = 0.2,\n",
    "                 random_state=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.hint_rate = hint_rate\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.train_rate = train_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.p_miss = p_miss\n",
    "        self.random_state = random_state\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "            )\n",
    "        torch.set_default_device(self.device)\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        torch.set_grad_enabled(True)\n",
    "        if random_state is not None:\n",
    "            torch.manual_seed(self.random_state)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.fit_transform(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        \n",
    "        self.modelG.load_state_dict(self._Gen_params)\n",
    "\n",
    "        if hasattr(X, 'dtypes'):\n",
    "            X = X.to_numpy()\n",
    "        #define mask matrix\n",
    "        X_mask = 1 - np.isnan(X)\n",
    "        #get dimensions\n",
    "        no, self.dim = X.shape\n",
    "        self.int_dim = int(self.dim)\n",
    "        #normalize the original data, and save parameters for renormalization\n",
    "        norm_data = X.copy()\n",
    "        min_val = np.zeros(self.dim)\n",
    "        max_val = np.zeros(self.dim)\n",
    "        for i in range(self.dim):\n",
    "            min_val[i] = np.nanmin(norm_data[i])\n",
    "            norm_data[:, i] -= np.nanmin(norm_data[:, i])\n",
    "            max_val[i] = np.nanmax(norm_data[i])\n",
    "            norm_data[:, i] /= (np.nanmax(norm_data[:, i]) + 1e-06)\n",
    "        norm_parameters = {'min_val': min_val, 'max_val': max_val}\n",
    "        norm_data_filled = np.nan_to_num(norm_data, 0)\n",
    "        p_miss_vec = self.p_miss * np.ones((self.dim,1)) \n",
    "        Missing = np.zeros((no,self.dim))\n",
    "        for i in range(self.dim):\n",
    "            A = np.random.uniform(0., 1., size = [len(norm_data_filled),])\n",
    "            B = A > p_miss_vec[i]\n",
    "            Missing[:,i] = 1.*B\n",
    "\n",
    "        Z_mb = self._sample_Z(no, self.dim)\n",
    "        M_mb = Missing\n",
    "        X_mb = norm_data_filled\n",
    "\n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb\n",
    "\n",
    "        X_mb = torch.tensor(X_mb, dtype=torch.float32)\n",
    "        New_X_mb = torch.tensor(New_X_mb, dtype=torch.float32)\n",
    "        M_mb = torch.tensor(M_mb, dtype=torch.float32)\n",
    "\n",
    "        G_sample = self.modelG(X_mb, New_X_mb, M_mb)\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        mse_final = mse_loss((1-M_mb)*X_mb, (1-M_mb)*G_sample)/(1-M_mb).sum()\n",
    "        print('Transform RMSE: ' + str(np.sqrt(mse_final.item())))\n",
    "\n",
    "        imputed_data = M_mb * X_mb + (1-M_mb) * G_sample\n",
    "        imputed_data = imputed_data.cpu().detach().numpy()\n",
    "        _, dim = imputed_data.shape\n",
    "        renorm_data = imputed_data.copy()\n",
    "        for i in range(dim):\n",
    "            renorm_data[:,i] = renorm_data[:,i] * (max_val[i] + 1e-6)   \n",
    "            renorm_data[:,i] = renorm_data[:,i] + min_val[i]\n",
    "        for i in range(dim):\n",
    "            temp = X[~np.isnan(X[:, i]), i]\n",
    "            # Only for the categorical variable\n",
    "            if len(np.unique(temp)) < 20:\n",
    "                renorm_data[:, i] = np.round(renorm_data[:, i])\n",
    "        return renorm_data\n",
    "        \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if hasattr(X, 'dtypes'):\n",
    "            X = X.to_numpy()\n",
    "        #define mask matrix\n",
    "        X_mask = 1 - np.isnan(X)\n",
    "        #get dimensions\n",
    "        no, self.dim = X.shape\n",
    "        self.int_dim = int(self.dim)\n",
    "        #normalize the original data, and save parameters for renormalization\n",
    "        norm_data = X.copy()\n",
    "        min_val = np.zeros(self.dim)\n",
    "        max_val = np.zeros(self.dim)\n",
    "        for i in range(self.dim):\n",
    "            min_val[i] = np.nanmin(norm_data[i])\n",
    "            norm_data[:, i] -= np.nanmin(norm_data[:, i])\n",
    "            max_val[i] = np.nanmax(norm_data[i])\n",
    "            norm_data[:, i] /= (np.nanmax(norm_data[:, i]) + 1e-06)\n",
    "        norm_parameters = {'min_val': min_val, 'max_val': max_val}\n",
    "        norm_data_filled = np.nan_to_num(norm_data, 0)\n",
    "        p_miss_vec = self.p_miss * np.ones((self.dim,1)) \n",
    "        Missing = np.zeros((no,self.dim))\n",
    "        for i in range(self.dim):\n",
    "            A = np.random.uniform(0., 1., size = [len(norm_data_filled),])\n",
    "            B = A > p_miss_vec[i]\n",
    "            Missing[:,i] = 1.*B\n",
    "        #internal test-train split\n",
    "        # Train / Test Missing Indicators\n",
    "        #model training\n",
    "        self.modelD = self.Discriminator(GainImputer=self)\n",
    "        self.modelG = self.Generator(GainImputer=self)\n",
    "\n",
    "        optimizer_D = torch.optim.Adam(self.modelD.parameters(), \n",
    "                                       lr = self.learning_rate)\n",
    "        optimizer_G = torch.optim.Adam(self.modelG.parameters(), \n",
    "                                       lr = self.learning_rate)\n",
    "        \n",
    "        bce_loss = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "        for it in range(self.iterations):\n",
    "            mb_idx = self._sample_index(no, self.batch_size)\n",
    "            X_mb = norm_data_filled[mb_idx,:]\n",
    "            Z_mb = self._sample_Z(self.batch_size, self.dim)\n",
    "\n",
    "            M_mb = Missing[mb_idx, :]\n",
    "            H_mb1 = self._sample_M(self.batch_size, self.dim, 1-self.hint_rate)\n",
    "            H_mb = M_mb*H_mb1 + 0.5*(1-H_mb1)\n",
    "\n",
    "            New_X_mb = M_mb * X_mb + (1-M_mb)*Z_mb #introduce missing data\n",
    "\n",
    "            X_mb = torch.tensor(X_mb, dtype=torch.float32)\n",
    "            New_X_mb = torch.tensor(New_X_mb, dtype=torch.float32)\n",
    "            Z_mb = torch.tensor(Z_mb, dtype=torch.float32)\n",
    "            M_mb = torch.tensor(M_mb, dtype=torch.float32)\n",
    "            H_mb = torch.tensor(H_mb, dtype=torch.float32)\n",
    "\n",
    "            #Train Discriminator\n",
    "            G_sample = self.modelG(X_mb, New_X_mb, M_mb)\n",
    "            D_prob = self.modelD(X_mb, M_mb, G_sample, H_mb)\n",
    "            D_loss = bce_loss(D_prob, M_mb)\n",
    "\n",
    "            D_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            #Train Generator\n",
    "            G_sample = self.modelG(X_mb, New_X_mb, M_mb)\n",
    "            D_prob = self.modelD(X_mb, M_mb, G_sample, H_mb)\n",
    "            D_prob.cpu().detach()\n",
    "            G_loss1 = ((1-M_mb)*(torch.sigmoid(D_prob)+1e-8).log()).mean()/(1-M_mb).sum()\n",
    "            G_mse_loss = mse_loss(M_mb*X_mb, M_mb*G_sample)/M_mb.sum()\n",
    "            G_loss = G_loss1 + self.alpha*G_mse_loss\n",
    "\n",
    "            G_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            G_mse_test = mse_loss((1-M_mb)*X_mb, (1-M_mb)*G_sample)/(1-M_mb).sum()\n",
    "\n",
    "            if it % 100 == 0:\n",
    "                print('Iter: {}'.format(it))\n",
    "                print('D_loss: {:.4}'.format(D_loss))\n",
    "                print('Train_loss: {:.4}'.format(G_mse_loss))\n",
    "                print()\n",
    "        self._Gen_params = self.modelG.state_dict()\n",
    "\n",
    "        Z_mb = self._sample_Z(no, self.dim) \n",
    "        M_mb = Missing\n",
    "        X_mb = norm_data_filled\n",
    "   \n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb\n",
    "\n",
    "        X_mb = torch.tensor(X_mb, dtype=torch.float32)\n",
    "        New_X_mb = torch.tensor(New_X_mb, dtype=torch.float32)\n",
    "        M_mb = torch.tensor(M_mb, dtype=torch.float32)\n",
    "\n",
    "        G_sample = self.modelG(X_mb, New_X_mb, M_mb)\n",
    "        mse_final = mse_loss((1-M_mb)*X_mb, (1-M_mb)*G_sample)/(1-M_mb).sum()\n",
    "        print('Final Train RMSE: ' + str(np.sqrt(mse_final.item())))\n",
    "\n",
    "        imputed_data = M_mb * X_mb + (1-M_mb) * G_sample\n",
    "        imputed_data = imputed_data.cpu().detach().numpy()\n",
    "        _, dim = imputed_data.shape\n",
    "        renorm_data = imputed_data.copy()\n",
    "        for i in range(dim):\n",
    "            renorm_data[:,i] = renorm_data[:,i] * (max_val[i] + 1e-6)   \n",
    "            renorm_data[:,i] = renorm_data[:,i] + min_val[i]\n",
    "        for i in range(dim):\n",
    "            temp = X[~np.isnan(X[:, i]), i]\n",
    "            # Only for the categorical variable\n",
    "            if len(np.unique(temp)) < 20:\n",
    "                renorm_data[:, i] = np.round(renorm_data[:, i])\n",
    "        return renorm_data\n",
    "    \n",
    "    def _sample_M(self, rows, cols, p):\n",
    "        '''Sample binary random variables.\n",
    "        Args:\n",
    "            - p: probability of 1\n",
    "            - rows: the number of rows\n",
    "            - cols: the number of columns\n",
    "        Returns:\n",
    "            - binary_random_matrix: generated binary random matrix.\n",
    "        '''\n",
    "        unif_random_matrix = np.random.uniform(0., 1., size = [rows, cols])\n",
    "        binary_random_matrix = unif_random_matrix > p\n",
    "        return 1.*binary_random_matrix\n",
    "\n",
    "    def _sample_Z(self, rows, cols):\n",
    "        '''Sample uniform random variables.\n",
    "        Args:\n",
    "            - rows: the number of rows\n",
    "            - cols: the number of columns\n",
    "        Returns:\n",
    "            - uniform_random_matrix: generated uniform random matrix.\n",
    "        '''\n",
    "        return np.random.uniform(0., 1., size = [rows, cols])       \n",
    "\n",
    "    def _sample_index(self, rows, batch_size):\n",
    "        '''Sample index of the mini-batch.\n",
    "        Args:\n",
    "            - total: total number of samples (rows)\n",
    "            - batch_size: batch size\n",
    "        Returns:\n",
    "            - batch_idx: batch index\n",
    "        '''\n",
    "        total_idx = np.random.permutation(rows)\n",
    "        batch_idx = total_idx[:batch_size]\n",
    "        return batch_idx\n",
    "    \n",
    "    class Generator(torch.nn.Module):\n",
    "        def __init__(self, GainImputer):\n",
    "            super(GainImputer.Generator, self).__init__()\n",
    "            self.G1 = torch.nn.Linear(GainImputer.dim*2,GainImputer.int_dim)\n",
    "            self.G2 = torch.nn.Linear(GainImputer.int_dim,GainImputer.int_dim)\n",
    "            self.G3 = torch.nn.Linear(GainImputer.int_dim,GainImputer.dim)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.init_weight()\n",
    "\n",
    "        def init_weight(self):\n",
    "            layers = [self.G1, self.G2, self.G3]\n",
    "            [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "\n",
    "        def forward(self, X: torch.float32, Z: torch.float32, M: torch.float32):\n",
    "            input = M * X + (1-M)*Z\n",
    "            input = torch.cat([input, M], dim=1)\n",
    "            out = self.relu(self.G1(input))\n",
    "            out = self.relu(self.G2(out))\n",
    "            out = self.sigmoid(self.G3(out))\n",
    "            return out\n",
    "        \n",
    "    class Discriminator(torch.nn.Module):\n",
    "        def __init__(self, GainImputer):\n",
    "            super(GainImputer.Discriminator, self).__init__()\n",
    "            self.D1 = torch.nn.Linear(GainImputer.dim*2,GainImputer.int_dim)\n",
    "            self.D2 = torch.nn.Linear(GainImputer.int_dim,GainImputer.int_dim)\n",
    "            self.D3 = torch.nn.Linear(GainImputer.int_dim,GainImputer.dim)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.init_weight()\n",
    "        \n",
    "        def init_weight(self):\n",
    "            layers = [self.D1, self.D2, self.D3]\n",
    "            [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "        \n",
    "        def forward(self, X, M, G, H):\n",
    "            input = M * X + (1-M)*G\n",
    "            input = torch.cat([input, H], dim=1)\n",
    "            out = self.relu(self.D1(input))\n",
    "            out = self.relu(self.D2(out))\n",
    "            out = self.D3(out)\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "D_loss: 0.6997\n",
      "Train_loss: 3.186e-05\n",
      "\n",
      "Iter: 100\n",
      "D_loss: 0.4352\n",
      "Train_loss: 9.931e-05\n",
      "\n",
      "Iter: 200\n",
      "D_loss: 0.3231\n",
      "Train_loss: 0.0001087\n",
      "\n",
      "Iter: 300\n",
      "D_loss: 0.2233\n",
      "Train_loss: 0.0001086\n",
      "\n",
      "Iter: 400\n",
      "D_loss: 0.1838\n",
      "Train_loss: 0.0001096\n",
      "\n",
      "Iter: 500\n",
      "D_loss: 0.145\n",
      "Train_loss: 0.0001103\n",
      "\n",
      "Iter: 600\n",
      "D_loss: 0.1103\n",
      "Train_loss: 0.0001093\n",
      "\n",
      "Iter: 700\n",
      "D_loss: 0.09513\n",
      "Train_loss: 0.0001099\n",
      "\n",
      "Iter: 800\n",
      "D_loss: 0.07875\n",
      "Train_loss: 0.00011\n",
      "\n",
      "Iter: 900\n",
      "D_loss: 0.07089\n",
      "Train_loss: 0.00011\n",
      "\n",
      "Final Train RMSE: 0.002079686549113976\n",
      "Transform RMSE: 0.00319132059301811\n"
     ]
    }
   ],
   "source": [
    "gain = GainImputer(batch_size=128, hint_rate=0.9, alpha=10, iterations=1000)\n",
    "\n",
    "imputed_train = gain.fit_transform(trainX)\n",
    "imputed_test = gain.transform(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 209.2995963436833\n",
      "Test RMSE: 89.76741133216728\n"
     ]
    }
   ],
   "source": [
    "def rmse_loss(ori_data, imputed_data, data_m):\n",
    "        '''Compute RMSE loss between ori_data and imputed_data\n",
    "        Args:\n",
    "            - ori_data: original data without missing values\n",
    "            - imputed_data: imputed data\n",
    "            - data_m: indicator matrix for missingness\n",
    "        Returns:\n",
    "            - rmse: Root Mean Squared Error\n",
    "        '''\n",
    "        #ori_data, norm_parameters = normalization(ori_data)\n",
    "        #imputed_data, _ = normalization(imputed_data, norm_parameters)\n",
    "        # Only for missing values\n",
    "        nominator = np.sum(((1-data_m) * ori_data - (1-data_m) * imputed_data)**2)\n",
    "        denominator = np.sum(1-data_m)\n",
    "        rmse = np.sqrt(nominator/float(denominator))\n",
    "        return rmse\n",
    "\n",
    "\n",
    "print(f'Train RMSE: {rmse_loss(ori_data=X[:Train_No].to_numpy(), imputed_data=imputed_train, data_m=trainM.to_numpy())}')\n",
    "print(f'Test RMSE: {rmse_loss(ori_data=X[Train_No:].to_numpy(), imputed_data=imputed_test, data_m=testM.to_numpy())}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
