{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielketron/tpot2_addimputers/env2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released 1 day ago.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted, _check_feature_names_in\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn\n",
    "import sklearn.impute\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import sklearn.compose\n",
    "from scipy import optimize\n",
    "\n",
    "import torch\n",
    "import sklearn.preprocessing\n",
    "import openml\n",
    "import tpot2\n",
    "import sklearn.metrics\n",
    "import sklearn\n",
    "from sklearn.metrics import (roc_auc_score, roc_curve, precision_score, auc, recall_score, precision_recall_curve, \\\n",
    "                             roc_auc_score, accuracy_score, balanced_accuracy_score, f1_score, log_loss,\n",
    "                             f1_score, root_mean_squared_error)\n",
    "import os\n",
    "import dill \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing(X, add_missing = 0.05, missing_type = 'MAR'):\n",
    "    if isinstance(X,np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    missing_mask = X\n",
    "    missing_mask = missing_mask.mask(missing_mask.isna(), True)\n",
    "    missing_mask = missing_mask.mask(missing_mask.notna(), False)\n",
    "    X = X.mask(X.isna(), 0)\n",
    "    T = torch.tensor(X.to_numpy())\n",
    "\n",
    "    match missing_type:\n",
    "        case 'MAR':\n",
    "            out = MAR(T, [add_missing])\n",
    "        case 'MCAR':\n",
    "            out = MCAR(T, [add_missing])\n",
    "        case 'MNAR':\n",
    "            out = MNAR_mask_logistic(T, [add_missing])\n",
    "    \n",
    "    masked_set = pd.DataFrame(out['Mask'].numpy())\n",
    "    missing_combo = (missing_mask | masked_set.isna())\n",
    "    masked_set = masked_set.mask(missing_combo, True)\n",
    "    masked_set.columns = X.columns.values\n",
    "    #masked_set = masked_set.to_numpy()\n",
    "\n",
    "    missing_set = pd.DataFrame(out['Missing'].numpy())\n",
    "    missing_set.columns = X.columns.values\n",
    "    #missing_set = missing_set.to_numpy()\n",
    "\n",
    "    return missing_set, masked_set\n",
    "\n",
    "\"\"\"BEYOND THIS POINT WRITTEN BY Aude Sportisse, Marine Le Morvan and Boris Muzellec - https://rmisstastic.netlify.app/how-to/python/generate_html/how%20to%20generate%20missing%20values\"\"\"\n",
    "\n",
    "def MCAR(X, p_miss):\n",
    "    out = {'X': X.double()}\n",
    "    for p in p_miss: \n",
    "        mask = (torch.rand(X.shape) < p).double()\n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "        model_name = 'Missing'\n",
    "        mask_name = 'Mask'\n",
    "        out[model_name] = X_nas\n",
    "        out[mask_name] = mask\n",
    "    return out\n",
    "\n",
    "def MAR(X,p_miss,p_obs=0.5):\n",
    "    out = {'X': X.double()}\n",
    "    for p in p_miss:\n",
    "        n, d = X.shape\n",
    "        mask = torch.zeros(n, d).bool()\n",
    "        num_no_missing = max(int(p_obs * d), 1)\n",
    "        num_missing = d - num_no_missing\n",
    "        obs_samples = np.random.choice(d, num_no_missing, replace=False)\n",
    "        copy_samples = np.array([i for i in range(d) if i not in obs_samples])\n",
    "        len_obs = len(obs_samples)\n",
    "        len_na = len(copy_samples)\n",
    "        coeffs = torch.randn(len_obs, len_na).double()\n",
    "        Wx = X[:, obs_samples].mm(coeffs)\n",
    "        coeffs /= torch.std(Wx, 0, keepdim=True)\n",
    "        coeffs.double()\n",
    "        len_obs, len_na = coeffs.shape\n",
    "        intercepts = torch.zeros(len_na)\n",
    "        for j in range(len_na):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X[:, obs_samples].mv(coeffs[:, j]) + x).mean().item() - p\n",
    "            intercepts[j] = optimize.bisect(f, -50, 50)\n",
    "        ps = torch.sigmoid(X[:, obs_samples].mm(coeffs) + intercepts)\n",
    "        ber = torch.rand(n, len_na)\n",
    "        mask[:, copy_samples] = ber < ps\n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "        model_name = 'Missing'\n",
    "        mask_name = 'Mask'\n",
    "        out[model_name] = X_nas\n",
    "        out[mask_name] = mask\n",
    "    return out\n",
    "\n",
    "def MNAR_mask_logistic(X, p_miss, p_params =.5, exclude_inputs=True):\n",
    "    \"\"\"\n",
    "    Missing not at random mechanism with a logistic masking model. It implements two mechanisms:\n",
    "    (i) Missing probabilities are selected with a logistic model, taking all variables as inputs. Hence, values that are\n",
    "    inputs can also be missing.\n",
    "    (ii) Variables are split into a set of intputs for a logistic model, and a set whose missing probabilities are\n",
    "    determined by the logistic model. Then inputs are then masked MCAR (hence, missing values from the second set will\n",
    "    depend on masked values.\n",
    "    In either case, weights are random and the intercept is selected to attain the desired proportion of missing values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.DoubleTensor or np.ndarray, shape (n, d)\n",
    "        Data for which missing values will be simulated.\n",
    "        If a numpy array is provided, it will be converted to a pytorch tensor.\n",
    "    p : float\n",
    "        Proportion of missing values to generate for variables which will have missing values.\n",
    "    p_params : float\n",
    "        Proportion of variables that will be used for the logistic masking model (only if exclude_inputs).\n",
    "    exclude_inputs : boolean, default=True\n",
    "        True: mechanism (ii) is used, False: (i)\n",
    "    Returns\n",
    "    -------\n",
    "    mask : torch.BoolTensor or np.ndarray (depending on type of X)\n",
    "        Mask of generated missing values (True if the value is missing).\n",
    "    \"\"\"\n",
    "    out = {'X_init_MNAR': X.double()}\n",
    "    for p in p_miss: \n",
    "        n, d = X.shape\n",
    "        to_torch = torch.is_tensor(X) ## output a pytorch tensor, or a numpy array\n",
    "        if not to_torch:\n",
    "            X = torch.from_numpy(X)\n",
    "        mask = torch.zeros(n, d).bool() if to_torch else np.zeros((n, d)).astype(bool)\n",
    "        d_params = max(int(p_params * d), 1) if exclude_inputs else d ## number of variables used as inputs (at least 1)\n",
    "        d_na = d - d_params if exclude_inputs else d ## number of variables masked with the logistic model\n",
    "        ### Sample variables that will be parameters for the logistic regression:\n",
    "        idxs_params = np.random.choice(d, d_params, replace=False) if exclude_inputs else np.arange(d)\n",
    "        idxs_nas = np.array([i for i in range(d) if i not in idxs_params]) if exclude_inputs else np.arange(d)\n",
    "        ### Other variables will have NA proportions selected by a logistic model\n",
    "        ### The parameters of this logistic model are random.\n",
    "        ### Pick coefficients so that W^Tx has unit variance (avoids shrinking)\n",
    "        len_obs = len(idxs_params)\n",
    "        len_na = len(idxs_nas)\n",
    "        coeffs = torch.randn(len_obs, len_na).double()\n",
    "        Wx = X[:, idxs_params].mm(coeffs)\n",
    "        coeffs /= torch.std(Wx, 0, keepdim=True)\n",
    "        coeffs.double()\n",
    "        ### Pick the intercepts to have a desired amount of missing values\n",
    "        len_obs, len_na = coeffs.shape\n",
    "        intercepts = torch.zeros(len_na)\n",
    "        for j in range(len_na):\n",
    "            def f(x):\n",
    "                return torch.sigmoid(X[:, idxs_params].mv(coeffs[:, j]) + x).mean().item() - p\n",
    "            intercepts[j] = optimize.bisect(f, -50, 50)\n",
    "        ps = torch.sigmoid(X[:, idxs_params].mm(coeffs) + intercepts)\n",
    "        ber = torch.rand(n, d_na)\n",
    "        mask[:, idxs_nas] = ber < ps\n",
    "        ## If the inputs of the logistic model are excluded from MNAR missingness,\n",
    "        ## mask some values used in the logistic model at random.\n",
    "        ## This makes the missingness of other variables potentially dependent on masked values\n",
    "        if exclude_inputs:\n",
    "            mask[:, idxs_params] = torch.rand(n, d_params) < p\n",
    "        X_nas = X.clone()\n",
    "        X_nas[mask.bool()] = np.nan\n",
    "        model_name = 'Missing'\n",
    "        mask_name = 'Mask'\n",
    "        out[model_name] = X_nas\n",
    "        out[mask_name] = mask\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_pipeline_full = tpot2.search_spaces.pipelines.SequentialPipeline([\n",
    "        tpot2.config.get_search_space(\"imputers\"), \n",
    "        tpot2.config.get_search_space(\"regressors\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_full = {\n",
    "                    'scorers':['neg_root_mean_squared_error'],\n",
    "                    'scorers_weights':[1],\n",
    "                    'population_size' : 5,\n",
    "                    'survival_percentage':1, \n",
    "                    'initial_population_size' : 5,\n",
    "                    'generations' : 5, \n",
    "                    'n_jobs':5,\n",
    "                    'cv': sklearn.model_selection.KFold(n_splits=10, shuffle=True, random_state=0),\n",
    "                    'verbose':5, \n",
    "                    'max_time_seconds': 360000,\n",
    "                    'max_eval_time_seconds':60*10, \n",
    "                    'classification' : False,\n",
    "                    'search_space': regression_pipeline_full,\n",
    "                    'preprocessing':False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(base_save_folder, task_id, r_or_c):\n",
    "    \n",
    "    cached_data_path = f\"{base_save_folder}/{task_id}.pkl\"\n",
    "    print(cached_data_path)\n",
    "    if os.path.exists(cached_data_path):\n",
    "        d = pickle.load(open(cached_data_path, \"rb\"))\n",
    "        X_train, y_train, X_test, y_test = d['X_train'], d['y_train'], d['X_test'], d['y_test']\n",
    "    else:\n",
    "        #kwargs = {'force_refresh_cache': True}\n",
    "        task = openml.datasets.get_dataset(task_id)\n",
    "        X, y, _, _  = task.get_data(dataset_format=\"dataframe\")\n",
    "        print(X)\n",
    "        print(y)\n",
    "        if y is None: \n",
    "            y = X.iloc[:, -1:]\n",
    "            X = X.iloc[:, :-1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "        preprocessing_pipeline = sklearn.pipeline.make_pipeline(\n",
    "            tpot2.builtin_modules.ColumnSimpleImputer(\n",
    "                \"categorical\", strategy='most_frequent'), \n",
    "            tpot2.builtin_modules.ColumnSimpleImputer(\n",
    "                \"numeric\", strategy='mean'), \n",
    "                tpot2.builtin_modules.ColumnOneHotEncoder(\n",
    "                    \"categorical\", min_frequency=0.001, handle_unknown=\"ignore\")\n",
    "            )\n",
    "        X_train = preprocessing_pipeline.fit_transform(X_train)\n",
    "        X_test = preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "        X_train = sklearn.preprocessing.normalize(X_train)\n",
    "        X_test = sklearn.preprocessing.normalize(X_test)\n",
    "\n",
    "        if r_or_c =='c':\n",
    "            le = sklearn.preprocessing.LabelEncoder()\n",
    "            y_train = le.fit_transform(y_train)\n",
    "            y_test = le.transform(y_test)\n",
    "\n",
    "        d = {\"X_train\": X_train, \"y_train\": y_train, \"X_test\": X_test, \"y_test\": y_test}\n",
    "        if not os.path.exists(f\"{base_save_folder}\"):\n",
    "            os.makedirs(f\"{base_save_folder}\")\n",
    "        with open(cached_data_path, \"wb\") as f:\n",
    "            pickle.dump(d, f)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testpipe = regression_pipeline_full.generate().export_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, y_train, X_test, y_test = load_task(base_save_folder='../data', task_id=197, r_or_c= 'r')\n",
    "for level in [0.3]:\n",
    "        for type_1 in ['MNAR']:\n",
    "                X_train = pd.DataFrame(X_train)\n",
    "                X_test = pd.DataFrame(X_test)\n",
    "                X_train_M, mask_train = add_missing(X_train, add_missing=level, missing_type=type_1)\n",
    "                X_test_M, mask_test = add_missing(X_test, add_missing=level, missing_type=type_1)\n",
    "                X_train_n = X_train_M.to_numpy()\n",
    "                X_test_n = X_test_M.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_M.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testpipe.fit(X_train_M, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(testpipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/05y1vdbd3vd52x71qyhklyjw0000gn/T/ipykernel_17760/518150171.py:6: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  missing_mask = missing_mask.mask(missing_mask.notna(), False)\n"
     ]
    }
   ],
   "source": [
    "dataset_file = '/Users/gabrielketron/tpot2_addimputers/tpot2/ImputerExperiments/data/Spam.csv'\n",
    "#%% System Parameters\n",
    "# 1. Mini batch size\n",
    "mb_size = 128\n",
    "# 2. Missing rate\n",
    "p_miss = 0.2\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "# 4. Loss Hyperparameters\n",
    "alpha = 10\n",
    "# 5. Train Rate\n",
    "train_rate = 0.8\n",
    "\n",
    "#%% Data\n",
    "\n",
    "# Data generation\n",
    "Data = np.loadtxt(dataset_file, delimiter=\",\",skiprows=1)\n",
    "X = pd.DataFrame(Data)\n",
    "\n",
    "X_M, mask = add_missing(X, missing_type='MCAR')\n",
    "\n",
    "no, dim = X_M.shape\n",
    "idx = np.random.permutation(no)\n",
    "\n",
    "Train_No = int(no * 0.5)\n",
    "Test_No = no - Train_No\n",
    "    \n",
    "# Train / Test Features\n",
    "trainX = X_M.iloc[:Train_No]\n",
    "testX = X_M.iloc[Train_No:]\n",
    "\n",
    "# Train / Test Missing Indicators\n",
    "trainM = mask.iloc[:Train_No]\n",
    "testM = mask.iloc[Train_No:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GainImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Base class for all imputers.\n",
    "    It adds automatically support for `add_indicator`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 batch_size=128, \n",
    "                 hint_rate=0.9, \n",
    "                 alpha=100, \n",
    "                 iterations=10000,\n",
    "                 train_rate = 0.8,\n",
    "                 learning_rate = 0.001,\n",
    "                 p_miss = 0.2,\n",
    "                 random_state=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.hint_rate = hint_rate\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.train_rate = train_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.p_miss = p_miss\n",
    "        self.random_state = random_state\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "            )\n",
    "        torch.set_default_device(self.device)\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        torch.set_grad_enabled(True)\n",
    "        if random_state is not None:\n",
    "            torch.manual_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.fit_transform(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        \n",
    "        self.modelG.load_state(self._Gen_params)\n",
    "\n",
    "        if hasattr(X, 'dtypes'):\n",
    "            X = X.to_numpy()\n",
    "        #define mask matrix\n",
    "        X_mask = 1 - np.isnan(X)\n",
    "        #get dimensions\n",
    "        no, self.dim = X.shape\n",
    "        self.int_dim = int(self.dim)\n",
    "        #normalize the original data, and save parameters for renormalization\n",
    "        norm_data = X.copy()\n",
    "        min_val = np.zeros(self.dim)\n",
    "        max_val = np.zeros(self.dim)\n",
    "        for i in range(self.dim):\n",
    "            min_val[i] = np.nanmin(norm_data[i])\n",
    "            norm_data[:, i] -= np.nanmin(norm_data[:, i])\n",
    "            max_val[i] = np.nanmax(norm_data[i])\n",
    "            norm_data[:, i] /= (np.nanmax(norm_data[:, i]) + 1e-06)\n",
    "        norm_parameters = {'min_val': min_val, 'max_val': max_val}\n",
    "        norm_data_filled = np.nan_to_num(norm_data, 0)\n",
    "        p_miss_vec = self.p_miss * np.ones((self.dim,1)) \n",
    "        Missing = np.zeros((no,self.dim))\n",
    "        for i in range(self.dim):\n",
    "            A = np.random.uniform(0., 1., size = [len(norm_data_filled),])\n",
    "            B = A > p_miss_vec[i]\n",
    "            Missing[:,i] = 1.*B\n",
    "\n",
    "        Z_mb = self._sample_Z(no, self.dim)\n",
    "        M_mb = Missing\n",
    "        X_mb = norm_data_filled\n",
    "\n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb\n",
    "\n",
    "        X_mb = torch.tensor(X_mb, dtype=torch.float32)\n",
    "        New_X_mb = torch.tensor(New_X_mb, dtype=torch.float32)\n",
    "        M_mb = torch.tensor(M_mb, dtype=torch.float32)\n",
    "\n",
    "        G_sample = self.modelG.forward(X_mb, New_X_mb, M_mb)\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        mse_final = mse_loss((1-M_mb)*X_mb, (1-M_mb)*G_sample)/(1-M_mb).sum()\n",
    "        print('Transform RMSE: ' + str(np.sqrt(mse_final.item())))\n",
    "\n",
    "        imputed_data = M_mb * X_mb + (1-M_mb) * G_sample\n",
    "        imputed_data = imputed_data.cpu().detach().numpy()\n",
    "        _, dim = imputed_data.shape\n",
    "        renorm_data = imputed_data.copy()\n",
    "        for i in range(dim):\n",
    "            renorm_data[:,i] = renorm_data[:,i] * (max_val[i] + 1e-6)   \n",
    "            renorm_data[:,i] = renorm_data[:,i] + min_val[i]\n",
    "        for i in range(dim):\n",
    "            temp = X[~np.isnan(X[:, i]), i]\n",
    "            # Only for the categorical variable\n",
    "            if len(np.unique(temp)) < 20:\n",
    "                renorm_data[:, i] = np.round(renorm_data[:, i])\n",
    "        return renorm_data\n",
    "        \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if hasattr(X, 'dtypes'):\n",
    "            X = X.to_numpy()\n",
    "        #define mask matrix\n",
    "        X_mask = 1 - np.isnan(X)\n",
    "        #get dimensions\n",
    "        no, self.dim = X.shape\n",
    "        self.int_dim = int(self.dim)\n",
    "        #normalize the original data, and save parameters for renormalization\n",
    "        norm_data = X.copy()\n",
    "        min_val = np.zeros(self.dim)\n",
    "        max_val = np.zeros(self.dim)\n",
    "        for i in range(self.dim):\n",
    "            min_val[i] = np.nanmin(norm_data[i])\n",
    "            norm_data[:, i] -= np.nanmin(norm_data[:, i])\n",
    "            max_val[i] = np.nanmax(norm_data[i])\n",
    "            norm_data[:, i] /= (np.nanmax(norm_data[:, i]) + 1e-06)\n",
    "        norm_parameters = {'min_val': min_val, 'max_val': max_val}\n",
    "        norm_data_filled = np.nan_to_num(norm_data, 0)\n",
    "        p_miss_vec = self.p_miss * np.ones((self.dim,1)) \n",
    "        Missing = np.zeros((no,self.dim))\n",
    "        for i in range(self.dim):\n",
    "            A = np.random.uniform(0., 1., size = [len(norm_data_filled),])\n",
    "            B = A > p_miss_vec[i]\n",
    "            Missing[:,i] = 1.*B\n",
    "        #internal test-train split\n",
    "        # Train / Test Missing Indicators\n",
    "        #model training\n",
    "        self.modelD = self.Discriminator(GainImputer=self)\n",
    "        self.modelG = self.Generator(GainImputer=self)\n",
    "\n",
    "        optimizer_D = torch.optim.Adam(self.modelD.parameters(), \n",
    "                                       lr = self.learning_rate)\n",
    "        optimizer_G = torch.optim.Adam(self.modelG.parameters(), \n",
    "                                       lr = self.learning_rate)\n",
    "        \n",
    "        bce_loss = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "        for it in range(self.iterations):\n",
    "            mb_idx = self._sample_index(no, self.batch_size)\n",
    "            X_mb = norm_data_filled[mb_idx,:]\n",
    "            Z_mb = self._sample_Z(self.batch_size, self.dim)\n",
    "\n",
    "            M_mb = Missing[mb_idx, :]\n",
    "            H_mb1 = self._sample_M(self.batch_size, self.dim, 1-self.hint_rate)\n",
    "            H_mb = M_mb*H_mb1 + 0.5*(1-H_mb1)\n",
    "\n",
    "            New_X_mb = M_mb * X_mb + (1-M_mb)*Z_mb #introduce missing data\n",
    "\n",
    "            X_mb = torch.tensor(X_mb, dtype=torch.float32)\n",
    "            New_X_mb = torch.tensor(New_X_mb, dtype=torch.float32)\n",
    "            Z_mb = torch.tensor(Z_mb, dtype=torch.float32)\n",
    "            M_mb = torch.tensor(M_mb, dtype=torch.float32)\n",
    "            H_mb = torch.tensor(H_mb, dtype=torch.float32)\n",
    "\n",
    "            #Train Discriminator\n",
    "            G_sample = self.modelG.forward(X_mb, New_X_mb, M_mb)\n",
    "            D_prob = self.modelD.forward(X_mb, M_mb, G_sample, H_mb)\n",
    "            D_loss = bce_loss(D_prob, M_mb)\n",
    "\n",
    "            D_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            #Train Generator\n",
    "            G_sample = self.modelG.forward(X_mb, New_X_mb, M_mb)\n",
    "            D_prob = self.modelD.forward(X_mb, M_mb, G_sample, H_mb)\n",
    "            D_prob.cpu().detach()\n",
    "            G_loss1 = ((1-M_mb)*(torch.sigmoid(D_prob)+1e-8).log()).mean()/(1-M_mb).sum()\n",
    "            G_mse_loss = mse_loss(M_mb*X_mb, M_mb*G_sample)/M_mb.sum()\n",
    "            G_loss = G_loss1 + self.alpha*G_mse_loss\n",
    "\n",
    "            G_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            G_mse_test = mse_loss((1-M_mb)*X_mb, (1-M_mb)*G_sample)/(1-M_mb).sum()\n",
    "\n",
    "            if it % 100 == 0:\n",
    "                print('Iter: {}'.format(it))\n",
    "                print('D_loss: {:.4}'.format(D_loss))\n",
    "                print('Train_loss: {:.4}'.format(G_mse_loss))\n",
    "                print()\n",
    "        self._Gen_params = self.modelG.parameters()\n",
    "\n",
    "        Z_mb = self._sample_Z(no, self.dim) \n",
    "        M_mb = Missing\n",
    "        X_mb = norm_data_filled\n",
    "   \n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb\n",
    "\n",
    "        X_mb = torch.tensor(X_mb, dtype=torch.float32)\n",
    "        New_X_mb = torch.tensor(New_X_mb, dtype=torch.float32)\n",
    "        M_mb = torch.tensor(M_mb, dtype=torch.float32)\n",
    "\n",
    "        G_sample = self.modelG.forward(X_mb, New_X_mb, M_mb)\n",
    "        mse_final = mse_loss((1-M_mb)*X_mb, (1-M_mb)*G_sample)/(1-M_mb).sum()\n",
    "        print('Final Train RMSE: ' + str(np.sqrt(mse_final.item())))\n",
    "\n",
    "        imputed_data = M_mb * X_mb + (1-M_mb) * G_sample\n",
    "        imputed_data = imputed_data.cpu().detach().numpy()\n",
    "        _, dim = imputed_data.shape\n",
    "        renorm_data = imputed_data.copy()\n",
    "        for i in range(dim):\n",
    "            renorm_data[:,i] = renorm_data[:,i] * (max_val[i] + 1e-6)   \n",
    "            renorm_data[:,i] = renorm_data[:,i] + min_val[i]\n",
    "        for i in range(dim):\n",
    "            temp = X[~np.isnan(X[:, i]), i]\n",
    "            # Only for the categorical variable\n",
    "            if len(np.unique(temp)) < 20:\n",
    "                renorm_data[:, i] = np.round(renorm_data[:, i])\n",
    "        return renorm_data\n",
    "    \n",
    "    def _sample_M(self, rows, cols, p):\n",
    "        '''Sample binary random variables.\n",
    "        Args:\n",
    "            - p: probability of 1\n",
    "            - rows: the number of rows\n",
    "            - cols: the number of columns\n",
    "        Returns:\n",
    "            - binary_random_matrix: generated binary random matrix.\n",
    "        '''\n",
    "        unif_random_matrix = np.random.uniform(0., 1., size = [rows, cols])\n",
    "        binary_random_matrix = unif_random_matrix > p\n",
    "        return 1.*binary_random_matrix\n",
    "\n",
    "    def _sample_Z(self, rows, cols):\n",
    "        '''Sample uniform random variables.\n",
    "        Args:\n",
    "            - rows: the number of rows\n",
    "            - cols: the number of columns\n",
    "        Returns:\n",
    "            - uniform_random_matrix: generated uniform random matrix.\n",
    "        '''\n",
    "        return np.random.uniform(0., 1., size = [rows, cols])       \n",
    "\n",
    "    def _sample_index(self, rows, batch_size):\n",
    "        '''Sample index of the mini-batch.\n",
    "        Args:\n",
    "            - total: total number of samples (rows)\n",
    "            - batch_size: batch size\n",
    "        Returns:\n",
    "            - batch_idx: batch index\n",
    "        '''\n",
    "        total_idx = np.random.permutation(rows)\n",
    "        batch_idx = total_idx[:batch_size]\n",
    "        return batch_idx\n",
    "    \n",
    "    class Generator():\n",
    "        def __init__(self, GainImputer):\n",
    "            super(GainImputer.Generator, self).__init__()\n",
    "            self.G_W1 = torch.nn.init.xavier_normal_(torch.empty((GainImputer.int_dim, GainImputer.dim*2), requires_grad=True, device=GainImputer.device))    # Data + Hint as inputs\n",
    "            self.G_b1 = torch.zeros((GainImputer.int_dim),requires_grad=True, device=GainImputer.device)\n",
    "\n",
    "            self.G_W2 = torch.nn.init.xavier_normal_(torch.empty((GainImputer.int_dim, GainImputer.int_dim),requires_grad=True, device=GainImputer.device))\n",
    "            self.G_b2 = torch.zeros((GainImputer.int_dim),requires_grad=True, device=GainImputer.device)\n",
    "\n",
    "            self.G_W3 = torch.nn.init.xavier_normal_(torch.empty((GainImputer.dim, GainImputer.int_dim),requires_grad=True, device=GainImputer.device))\n",
    "            self.G_b3 = torch.zeros((GainImputer.dim), requires_grad=True, device=GainImputer.device)   \n",
    "\n",
    "        def forward(self, X: torch.float32, Z: torch.float32, M: torch.float32):\n",
    "            input = M * X + (1-M)*Z\n",
    "            input = torch.cat([input, M], dim=1)\n",
    "            l1 = torch.nn.functional.linear(input=input, weight=self.G_W1, bias=self.G_b1)\n",
    "            out1 = torch.nn.functional.relu(l1)\n",
    "            l2 = torch.nn.functional.linear(input=out1, weight=self.G_W2, bias=self.G_b2)\n",
    "            out2 = torch.nn.functional.relu(l2)\n",
    "            l3 = torch.nn.functional.linear(input=out2, weight=self.G_W3, bias=self.G_b3)\n",
    "            out = torch.nn.functional.sigmoid(l3)\n",
    "            return out\n",
    "        \n",
    "        def parameters(self):\n",
    "            params = [self.G_W1, self.G_b1, self.G_W2, self.G_b2, self.G_W3, self.G_b3]\n",
    "            return params\n",
    "        \n",
    "        def load_state(self, params):\n",
    "            self.G_W1 = params[0]\n",
    "            self.G_b1 = params[1]\n",
    "            self.G_W2 = params[2]\n",
    "            self.G_b2 = params[3]\n",
    "            self.G_W3 = params[4]\n",
    "            self.G_b3 = params[5]\n",
    "        \n",
    "    class Discriminator():\n",
    "        def __init__(self, GainImputer):\n",
    "            super(GainImputer.Discriminator, self).__init__()\n",
    "            self.D_W1 = torch.nn.init.xavier_normal_(torch.empty((GainImputer.int_dim, GainImputer.dim*2), requires_grad=True, device=GainImputer.device))     # Data + Hint as inputs\n",
    "            self.D_b1 = torch.zeros((GainImputer.int_dim),requires_grad=True, device=GainImputer.device)\n",
    "            self.D_W2 = torch.nn.init.xavier_normal_(torch.empty((GainImputer.int_dim, GainImputer.int_dim),requires_grad=True, device=GainImputer.device))\n",
    "            self.D_b2 = torch.zeros((GainImputer.int_dim),requires_grad=True, device=GainImputer.device)\n",
    "            self.D_W3 = torch.nn.init.xavier_normal_(torch.empty((GainImputer.dim, GainImputer.int_dim),requires_grad=True, device=GainImputer.device))\n",
    "            self.D_b3 = torch.zeros((GainImputer.dim), requires_grad=True, device=GainImputer.device)       # Output is multi-variate\n",
    "\n",
    "        \n",
    "        def forward(self, X, M, G, H):\n",
    "            input = M * X + (1-M)*G\n",
    "            input = torch.cat([input, H], dim=1)\n",
    "            l1 = torch.nn.functional.linear(input=input, weight=self.D_W1, bias=self.D_b1)\n",
    "            out1 = torch.nn.functional.relu(l1)\n",
    "            l2 = torch.nn.functional.linear(input=out1, weight=self.D_W2, bias=self.D_b2)\n",
    "            out2 = torch.nn.functional.relu(l2)\n",
    "            l3 = torch.nn.functional.linear(input=out2, weight=self.D_W3, bias=self.D_b3)\n",
    "            return l3\n",
    "        \n",
    "        def parameters(self):\n",
    "            params = [self.D_W1, self.D_b1, self.D_W2, self.D_b2, self.D_W3, self.D_b3]\n",
    "            return params\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "D_loss: 0.7609\n",
      "Train_loss: 3.384e-05\n",
      "\n",
      "Iter: 100\n",
      "D_loss: 0.4402\n",
      "Train_loss: 9.594e-05\n",
      "\n",
      "Iter: 200\n",
      "D_loss: 0.3212\n",
      "Train_loss: 0.0001066\n",
      "\n",
      "Iter: 300\n",
      "D_loss: 0.249\n",
      "Train_loss: 0.0001063\n",
      "\n",
      "Iter: 400\n",
      "D_loss: 0.1934\n",
      "Train_loss: 0.0001094\n",
      "\n",
      "Iter: 500\n",
      "D_loss: 0.1527\n",
      "Train_loss: 0.0001088\n",
      "\n",
      "Iter: 600\n",
      "D_loss: 0.1275\n",
      "Train_loss: 0.000109\n",
      "\n",
      "Iter: 700\n",
      "D_loss: 0.1233\n",
      "Train_loss: 0.0001093\n",
      "\n",
      "Iter: 800\n",
      "D_loss: 0.1131\n",
      "Train_loss: 0.0001095\n",
      "\n",
      "Iter: 900\n",
      "D_loss: 0.09329\n",
      "Train_loss: 0.0001096\n",
      "\n",
      "Final Train RMSE: 0.0024673127690825313\n",
      "Transform RMSE: 0.0024771628411869996\n"
     ]
    }
   ],
   "source": [
    "gain = GainImputer(batch_size=128, hint_rate=0.9, alpha=10, iterations=1000)\n",
    "\n",
    "imputed_train = gain.fit_transform(trainX)\n",
    "imputed_test = gain.transform(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 315.06209184165056\n",
      "Test RMSE: 243.98528301604676\n"
     ]
    }
   ],
   "source": [
    "def rmse_loss(ori_data, imputed_data, data_m):\n",
    "        '''Compute RMSE loss between ori_data and imputed_data\n",
    "        Args:\n",
    "            - ori_data: original data without missing values\n",
    "            - imputed_data: imputed data\n",
    "            - data_m: indicator matrix for missingness\n",
    "        Returns:\n",
    "            - rmse: Root Mean Squared Error\n",
    "        '''\n",
    "        #ori_data, norm_parameters = normalization(ori_data)\n",
    "        #imputed_data, _ = normalization(imputed_data, norm_parameters)\n",
    "        # Only for missing values\n",
    "        nominator = np.sum(((1-data_m) * ori_data - (1-data_m) * imputed_data)**2)\n",
    "        denominator = np.sum(1-data_m)\n",
    "        rmse = np.sqrt(nominator/float(denominator))\n",
    "        return rmse\n",
    "\n",
    "\n",
    "print(f'Train RMSE: {rmse_loss(ori_data=X[:Train_No].to_numpy(), imputed_data=imputed_train, data_m=trainM.to_numpy())}')\n",
    "print(f'Test RMSE: {rmse_loss(ori_data=X[Train_No:].to_numpy(), imputed_data=imputed_test, data_m=testM.to_numpy())}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
