{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: Dhanajit Brahma\n",
    "\n",
    "Adapted from the original implementation in tensorflow from here: https://github.com/jsyoon0823/GAIN\n",
    "\n",
    "Generative Adversarial Imputation Networks (GAIN) Implementation on Letter and Spam Dataset\n",
    "\n",
    "Reference: J. Yoon, J. Jordon, M. van der Schaar, \"GAIN: Missing Data Imputation using Generative Adversarial Nets,\" ICML, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Packages\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = '/Users/gabrielketron/tpot2_addimputers/tpot2/ImputerExperiments/data/Spam.csv'  # 'Letter.csv' for Letter dataset an 'Spam.csv' for Spam dataset\n",
    "use_gpu = False  # set it to True to use GPU and False to use CPU\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% System Parameters\n",
    "# 1. Mini batch size\n",
    "mb_size = 128\n",
    "# 2. Missing rate\n",
    "p_miss = 0.2\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "# 4. Loss Hyperparameters\n",
    "alpha = 10\n",
    "# 5. Train Rate\n",
    "train_rate = 0.8\n",
    "\n",
    "#%% Data\n",
    "\n",
    "# Data generation\n",
    "Data = np.loadtxt(dataset_file, delimiter=\",\",skiprows=1)\n",
    "\n",
    "# Parameters\n",
    "No = len(Data)\n",
    "Dim = len(Data[0,:])\n",
    "\n",
    "# Hidden state dimensions\n",
    "H_Dim1 = Dim\n",
    "H_Dim2 = Dim\n",
    "\n",
    "# Normalization (0 to 1)\n",
    "Min_Val = np.zeros(Dim)\n",
    "Max_Val = np.zeros(Dim)\n",
    "\n",
    "for i in range(Dim):\n",
    "    Min_Val[i] = np.min(Data[:,i])\n",
    "    Data[:,i] = Data[:,i] - np.min(Data[:,i])\n",
    "    Max_Val[i] = np.max(Data[:,i])\n",
    "    Data[:,i] = Data[:,i] / (np.max(Data[:,i]) + 1e-6)    \n",
    "\n",
    "#%% Missing introducing\n",
    "p_miss_vec = p_miss * np.ones((Dim,1)) \n",
    "   \n",
    "Missing = np.zeros((No,Dim))\n",
    "\n",
    "for i in range(Dim):\n",
    "    A = np.random.uniform(0., 1., size = [len(Data),])\n",
    "    B = A > p_miss_vec[i]\n",
    "    Missing[:,i] = 1.*B\n",
    "\n",
    "    \n",
    "#%% Train Test Division    \n",
    "   \n",
    "idx = np.random.permutation(No)\n",
    "\n",
    "Train_No = int(No * train_rate)\n",
    "Test_No = No - Train_No\n",
    "    \n",
    "# Train / Test Features\n",
    "trainX = Data[idx[:Train_No],:]\n",
    "testX = Data[idx[Train_No:],:]\n",
    "\n",
    "# Train / Test Missing Indicators\n",
    "trainM = Missing[idx[:Train_No],:]\n",
    "testM = Missing[idx[Train_No:],:]\n",
    "\n",
    "#%% Necessary Functions\n",
    "\n",
    "# 1. Xavier Initialization Definition\n",
    "# def xavier_init(size):\n",
    "#     in_dim = size[0]\n",
    "#     xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "#     return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return np.random.normal(size = size, scale = xavier_stddev)\n",
    "    \n",
    "# Hint Vector Generation\n",
    "def sample_M(m, n, p):\n",
    "    A = np.random.uniform(0., 1., size = [m, n])\n",
    "    B = A > p\n",
    "    C = 1.*B\n",
    "    return C\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAIN Architecture   \n",
    "GAIN Consists of 3 Components\n",
    "- Generator\n",
    "- Discriminator\n",
    "- Hint Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 1. Discriminator\n",
    "if use_gpu is True:\n",
    "    D_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True, device=\"cuda\")     # Data + Hint as inputs\n",
    "    D_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    D_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "    D_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    D_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True, device=\"cuda\")\n",
    "    D_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True, device=\"cuda\")       # Output is multi-variate\n",
    "else:\n",
    "    D_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True)     # Data + Hint as inputs\n",
    "    D_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True)\n",
    "\n",
    "    D_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True)\n",
    "    D_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True)\n",
    "\n",
    "    D_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True)\n",
    "    D_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True)       # Output is multi-variate\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "#%% 2. Generator\n",
    "if use_gpu is True:\n",
    "    G_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True, device=\"cuda\")     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "    G_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    G_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "    G_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    G_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True, device=\"cuda\")\n",
    "    G_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True, device=\"cuda\")\n",
    "else:\n",
    "    G_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True)     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "    G_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True)\n",
    "\n",
    "    G_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True)\n",
    "    G_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True)\n",
    "\n",
    "    G_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True)\n",
    "    G_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True)\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAIN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 1. Generator\n",
    "def generator(new_x,m):\n",
    "    inputs = torch.cat(dim = 1, tensors = [new_x,m])  # Mask + Data Concatenate\n",
    "    G_h1 = F.relu(torch.matmul(inputs, G_W1) + G_b1)\n",
    "    G_h2 = F.relu(torch.matmul(G_h1, G_W2) + G_b2)   \n",
    "    G_prob = torch.sigmoid(torch.matmul(G_h2, G_W3) + G_b3) # [0,1] normalized Output\n",
    "    \n",
    "    return G_prob\n",
    "\n",
    "#%% 2. Discriminator\n",
    "def discriminator(new_x, h):\n",
    "    inputs = torch.cat(dim = 1, tensors = [new_x,h])  # Hint + Data Concatenate\n",
    "    D_h1 = F.relu(torch.matmul(inputs, D_W1) + D_b1)  \n",
    "    D_h2 = F.relu(torch.matmul(D_h1, D_W2) + D_b2)\n",
    "    D_logit = torch.matmul(D_h2, D_W3) + D_b3\n",
    "    D_prob = torch.sigmoid(D_logit)  # [0,1] Probability Output\n",
    "    \n",
    "    return D_prob\n",
    "\n",
    "#%% 3. Other functions\n",
    "# Random sample generator for Z\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(0., 0.01, size = [m, n])        \n",
    "\n",
    "# Mini-batch generation\n",
    "def sample_idx(m, n):\n",
    "    A = np.random.permutation(m)\n",
    "    idx = A[:n]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAIN Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(M, New_X, H):\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    D_loss = -torch.mean(M * torch.log(D_prob + 1e-8) + (1-M) * torch.log(1. - D_prob + 1e-8))\n",
    "    return D_loss\n",
    "\n",
    "def generator_loss(X, M, New_X, H):\n",
    "    #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    G_loss1 = -torch.mean((1-M) * torch.log(D_prob + 1e-8))\n",
    "    MSE_train_loss = torch.mean((M * New_X - M * G_sample)**2) / torch.mean(M)\n",
    "\n",
    "    G_loss = G_loss1 + alpha * MSE_train_loss \n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return G_loss, MSE_train_loss, MSE_test_loss\n",
    "    \n",
    "def test_loss(X, M, New_X):\n",
    "    #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return MSE_test_loss, G_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_D = torch.optim.Adam(params=theta_D)\n",
    "optimizer_G = torch.optim.Adam(params=theta_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 88/5000 [00:00<00:11, 441.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Train_loss: 0.5158\n",
      "Test_loss: 0.5188\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 133/5000 [00:00<00:12, 384.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100\n",
      "Train_loss: 0.05808\n",
      "Test_loss: 0.0545\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 258/5000 [00:00<00:13, 360.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "Train_loss: 0.0538\n",
      "Test_loss: 0.05068\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 376/5000 [00:00<00:12, 375.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 300\n",
      "Train_loss: 0.06115\n",
      "Test_loss: 0.05914\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 457/5000 [00:01<00:12, 356.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 400\n",
      "Train_loss: 0.06275\n",
      "Test_loss: 0.06081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 541/5000 [00:01<00:11, 384.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 500\n",
      "Train_loss: 0.05074\n",
      "Test_loss: 0.04955\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 674/5000 [00:01<00:10, 418.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 600\n",
      "Train_loss: 0.05999\n",
      "Test_loss: 0.05863\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 763/5000 [00:01<00:09, 431.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 700\n",
      "Train_loss: 0.0485\n",
      "Test_loss: 0.06783\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 855/5000 [00:02<00:09, 441.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 800\n",
      "Train_loss: 0.05263\n",
      "Test_loss: 0.04699\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 949/5000 [00:02<00:09, 439.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 900\n",
      "Train_loss: 0.05028\n",
      "Test_loss: 0.06122\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1094/5000 [00:02<00:08, 461.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1000\n",
      "Train_loss: 0.05144\n",
      "Test_loss: 0.06054\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1190/5000 [00:02<00:08, 465.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1100\n",
      "Train_loss: 0.05412\n",
      "Test_loss: 0.05081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1285/5000 [00:03<00:08, 454.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1200\n",
      "Train_loss: 0.04881\n",
      "Test_loss: 0.0545\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1380/5000 [00:03<00:07, 463.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1300\n",
      "Train_loss: 0.04552\n",
      "Test_loss: 0.05873\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1472/5000 [00:03<00:08, 424.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1400\n",
      "Train_loss: 0.04274\n",
      "Test_loss: 0.06087\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 1566/5000 [00:03<00:07, 443.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1500\n",
      "Train_loss: 0.04222\n",
      "Test_loss: 0.04433\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1660/5000 [00:03<00:07, 456.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1600\n",
      "Train_loss: 0.04274\n",
      "Test_loss: 0.05668\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 1755/5000 [00:04<00:06, 463.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1700\n",
      "Train_loss: 0.04273\n",
      "Test_loss: 0.05793\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 1853/5000 [00:04<00:06, 462.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1800\n",
      "Train_loss: 0.03984\n",
      "Test_loss: 0.05007\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 1948/5000 [00:04<00:06, 445.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1900\n",
      "Train_loss: 0.03442\n",
      "Test_loss: 0.05378\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 2043/5000 [00:04<00:06, 441.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2000\n",
      "Train_loss: 0.03352\n",
      "Test_loss: 0.05143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 2189/5000 [00:05<00:06, 467.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2100\n",
      "Train_loss: 0.03428\n",
      "Test_loss: 0.04975\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2284/5000 [00:05<00:05, 463.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2200\n",
      "Train_loss: 0.03292\n",
      "Test_loss: 0.05255\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 2378/5000 [00:05<00:05, 454.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2300\n",
      "Train_loss: 0.02758\n",
      "Test_loss: 0.05088\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 2471/5000 [00:05<00:05, 431.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2400\n",
      "Train_loss: 0.03235\n",
      "Test_loss: 0.05737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 2565/5000 [00:05<00:05, 448.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2500\n",
      "Train_loss: 0.03353\n",
      "Test_loss: 0.04674\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 2656/5000 [00:06<00:05, 444.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2600\n",
      "Train_loss: 0.0309\n",
      "Test_loss: 0.05964\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 2751/5000 [00:06<00:05, 437.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2700\n",
      "Train_loss: 0.02715\n",
      "Test_loss: 0.05795\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 2881/5000 [00:06<00:05, 416.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2800\n",
      "Train_loss: 0.03021\n",
      "Test_loss: 0.05065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 2967/5000 [00:06<00:04, 421.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2900\n",
      "Train_loss: 0.0292\n",
      "Test_loss: 0.06192\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 3060/5000 [00:07<00:04, 443.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3000\n",
      "Train_loss: 0.02648\n",
      "Test_loss: 0.05452\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 3155/5000 [00:07<00:04, 454.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3100\n",
      "Train_loss: 0.02519\n",
      "Test_loss: 0.05174\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 3247/5000 [00:07<00:03, 452.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3200\n",
      "Train_loss: 0.02764\n",
      "Test_loss: 0.04973\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3394/5000 [00:07<00:03, 472.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3300\n",
      "Train_loss: 0.02434\n",
      "Test_loss: 0.05232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 3442/5000 [00:07<00:03, 433.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3400\n",
      "Train_loss: 0.03\n",
      "Test_loss: 0.06014\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 3585/5000 [00:08<00:03, 458.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3500\n",
      "Train_loss: 0.0269\n",
      "Test_loss: 0.04678\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 3681/5000 [00:08<00:02, 462.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3600\n",
      "Train_loss: 0.02455\n",
      "Test_loss: 0.05256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3773/5000 [00:08<00:02, 437.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3700\n",
      "Train_loss: 0.02507\n",
      "Test_loss: 0.05267\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 3869/5000 [00:08<00:02, 459.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3800\n",
      "Train_loss: 0.02464\n",
      "Test_loss: 0.06597\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 3964/5000 [00:09<00:02, 463.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3900\n",
      "Train_loss: 0.02419\n",
      "Test_loss: 0.05976\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 4059/5000 [00:09<00:02, 442.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4000\n",
      "Train_loss: 0.02421\n",
      "Test_loss: 0.05306\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 4149/5000 [00:09<00:01, 442.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4100\n",
      "Train_loss: 0.0226\n",
      "Test_loss: 0.04916\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 4289/5000 [00:09<00:01, 454.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4200\n",
      "Train_loss: 0.02527\n",
      "Test_loss: 0.04813\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 4336/5000 [00:09<00:01, 457.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4300\n",
      "Train_loss: 0.02271\n",
      "Test_loss: 0.06062\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 4479/5000 [00:10<00:01, 446.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4400\n",
      "Train_loss: 0.02222\n",
      "Test_loss: 0.05249\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 4575/5000 [00:10<00:00, 460.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4500\n",
      "Train_loss: 0.02225\n",
      "Test_loss: 0.0592\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 4673/5000 [00:10<00:00, 472.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4600\n",
      "Train_loss: 0.02693\n",
      "Test_loss: 0.05673\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 4768/5000 [00:10<00:00, 450.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4700\n",
      "Train_loss: 0.02231\n",
      "Test_loss: 0.06395\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 4862/5000 [00:11<00:00, 436.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4800\n",
      "Train_loss: 0.0235\n",
      "Test_loss: 0.06084\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 4957/5000 [00:11<00:00, 437.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4900\n",
      "Train_loss: 0.0229\n",
      "Test_loss: 0.05741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:11<00:00, 437.79it/s]\n"
     ]
    }
   ],
   "source": [
    "#%% Start Iterations\n",
    "for it in tqdm(range(5000)):    \n",
    "    \n",
    "    #%% Inputs\n",
    "    mb_idx = sample_idx(Train_No, mb_size)\n",
    "    X_mb = trainX[mb_idx,:]  \n",
    "    \n",
    "    Z_mb = sample_Z(mb_size, Dim) \n",
    "    M_mb = trainM[mb_idx,:]  \n",
    "    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n",
    "    H_mb = M_mb * H_mb1\n",
    "    \n",
    "    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "    \n",
    "    if use_gpu is True:\n",
    "        X_mb = torch.tensor(X_mb, device=\"cuda\")\n",
    "        M_mb = torch.tensor(M_mb, device=\"cuda\")\n",
    "        H_mb = torch.tensor(H_mb, device=\"cuda\")\n",
    "        New_X_mb = torch.tensor(New_X_mb, device=\"cuda\")\n",
    "    else:\n",
    "        X_mb = torch.tensor(X_mb)\n",
    "        M_mb = torch.tensor(M_mb)\n",
    "        H_mb = torch.tensor(H_mb)\n",
    "        New_X_mb = torch.tensor(New_X_mb)\n",
    "    \n",
    "    optimizer_D.zero_grad()\n",
    "    D_loss_curr = discriminator_loss(M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "    D_loss_curr.backward()\n",
    "    optimizer_D.step()\n",
    "    \n",
    "    optimizer_G.zero_grad()\n",
    "    G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = generator_loss(X=X_mb, M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "    G_loss_curr.backward()\n",
    "    optimizer_G.step()    \n",
    "        \n",
    "    #%% Intermediate Losses\n",
    "    if it % 100 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr.item())))\n",
    "        print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr.item())))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test RMSE: 0.05326744178163777\n"
     ]
    }
   ],
   "source": [
    "Z_mb = sample_Z(Test_No, Dim) \n",
    "M_mb = testM\n",
    "X_mb = testX\n",
    "        \n",
    "New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "\n",
    "if use_gpu is True:\n",
    "    X_mb = torch.tensor(X_mb, device='cuda')\n",
    "    M_mb = torch.tensor(M_mb, device='cuda')\n",
    "    New_X_mb = torch.tensor(New_X_mb, device='cuda')\n",
    "else:\n",
    "    X_mb = torch.tensor(X_mb)\n",
    "    M_mb = torch.tensor(M_mb)\n",
    "    New_X_mb = torch.tensor(New_X_mb)\n",
    "    \n",
    "MSE_final, Sample = test_loss(X=X_mb, M=M_mb, New_X=New_X_mb)\n",
    "        \n",
    "print('Final Test RMSE: ' + str(np.sqrt(MSE_final.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed test data:\n",
      "[[0.00000000e+00 0.00000000e+00 5.90227679e-02 ... 6.68857704e-03\n",
      "  3.80456548e-03 2.28080270e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.90649115e-04\n",
      "  3.79778740e-03 4.29292929e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.34543804e-03\n",
      "  1.52475242e-04 8.42976060e-03]\n",
      " ...\n",
      " [0.00000000e+00 3.88542850e-05 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 2.52525253e-04]\n",
      " [7.25846808e-03 0.00000000e+00 0.00000000e+00 ... 6.89968225e-05\n",
      "  1.00120144e-04 8.20707071e-04]\n",
      " [0.00000000e+00 1.22406508e-02 0.00000000e+00 ... 4.80254198e-04\n",
      "  3.00360432e-04 5.90242456e-03]]\n"
     ]
    }
   ],
   "source": [
    "imputed_data = M_mb * X_mb + (1-M_mb) * Sample\n",
    "print(\"Imputed test data:\")\n",
    "# np.set_printoptions(formatter={'float': lambda x: \"{0:0.8f}\".format(x)})\n",
    "\n",
    "if use_gpu is True:\n",
    "    print(imputed_data.cpu().detach().numpy())\n",
    "else:\n",
    "    print(imputed_data.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielketron/tpot2_addimputers/env2/lib/python3.10/site-packages/torch/nn/_reduction.py:13: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 57 into shape (28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 194\u001b[0m\n\u001b[1;32m    188\u001b[0m samples4 \u001b[38;5;241m=\u001b[39m M_mb \u001b[38;5;241m*\u001b[39m X_mb \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mM_mb) \u001b[38;5;241m*\u001b[39m samples4     \n\u001b[1;32m    191\u001b[0m samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([samples5\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mdata, samples2\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mdata, samples3\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mdata,\n\u001b[1;32m    192\u001b[0m                      samples4\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mdata, samples1\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mdata])          \n\u001b[0;32m--> 194\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiple_Impute_out1/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m3\u001b[39m)), bbox_inches\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    196\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m     22\u001b[0m     ax\u001b[38;5;241m.\u001b[39mset_yticklabels([])\n\u001b[1;32m     23\u001b[0m     ax\u001b[38;5;241m.\u001b[39mset_aspect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mequal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGreys_r\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fig\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 57 into shape (28,28)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAA5klEQVR4nO3QQREAIAzAMMC/5+GCPGgU9LpnZlaeOzrgV41HGo80Hmk80nik8UjjkcYjjUcajzQeaTzSeKTxSOORxiONRxqPNB5pPNJ4pPFI45HGI41HGo80Hmk80nik8UjjkcYjjUcajzQeaTzSeKTxSOORxiONRxqPNB5pPNJ4pPFI45HGI41HGo80Hmk80nik8UjjkcYjjUcajzQeaTzSeKTxSOORxiONRxqPNB5pPNJ4pPFI45HGI41HGo80Hmk80nik8UjjkcYjjUcajzQeaTzSeKTxSOORxiONRxqPNB5pPHIBBnkEuOGnLnAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Mask Vector and Hint Vector Generation\n",
    "def sample_M(m, n, p):\n",
    "    A = np.random.uniform(0., 1., size = [m, n])\n",
    "    B = A > p\n",
    "    C = 1.*B\n",
    "    return C\n",
    "    \n",
    "# 2. Plot (4 x 4 subfigures)\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize = (5,5))\n",
    "    gs = gridspec.GridSpec(5,5)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28,28), cmap='Greys_r')\n",
    "        \n",
    "    return fig\n",
    "\n",
    "#%% 3. Others\n",
    "# Random sample generator for Z\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(0., 1., size = [m, n])\n",
    "\n",
    "\n",
    "def sample_idx(m, n):\n",
    "    A = np.random.permutation(m)\n",
    "    idx = A[:n]\n",
    "    return idx\n",
    "\n",
    "\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetD, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(Dim*2, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 128)\n",
    "        self.fc3 = torch.nn.Linear(128, Dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "    \n",
    "    \n",
    "    def init_weight(self):\n",
    "        layers = [self.fc1, self.fc2, self.fc3]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "        \n",
    "        \n",
    "    def forward(self, x, m, g, h):\n",
    "        \"\"\"Eq(3)\"\"\"\n",
    "        inp = m * x + (1-m) * g \n",
    "        inp = torch.cat((inp, h), dim=1)\n",
    "        out = self.relu(self.fc1(inp))\n",
    "        out = self.relu(self.fc2(out))\n",
    "#         out = self.sigmoid(self.fc3(out)) # [0,1] Probability Output\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out    \n",
    "\n",
    "\"\"\"\n",
    "Eq(2)\n",
    "\"\"\"\n",
    "class NetG(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetG, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(Dim*2, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 128)\n",
    "        self.fc3 = torch.nn.Linear(128, Dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "    \n",
    "    def init_weight(self):\n",
    "        layers = [self.fc1, self.fc2, self.fc3]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "        \n",
    "        \n",
    "    def forward(self, x, z, m):\n",
    "        inp = m * x + (1-m) * z\n",
    "        inp = torch.cat((inp, m), dim=1)\n",
    "        out = self.relu(self.fc1(inp))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.sigmoid(self.fc3(out)) # [0,1] Probability Output\n",
    "#         out = self.fc3(out)\n",
    "        \n",
    "        return out \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "netD = NetD()\n",
    "netG = NetG()\n",
    "\n",
    "\n",
    "optimD = torch.optim.Adam(netD.parameters(), lr=0.001)\n",
    "optimG = torch.optim.Adam(netG.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Output Initialization\n",
    "if not os.path.exists('Multiple_Impute_out1/'):\n",
    "    os.makedirs('Multiple_Impute_out1/')\n",
    "\n",
    "\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss(reduction=\"elementwise_mean\")\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"elementwise_mean\")\n",
    "\n",
    "i = 1\n",
    "#%% Start Iterations\n",
    "for it in range(10000): \n",
    "    #%% Inputs\n",
    "    mb_idx = sample_idx(Train_No, mb_size)\n",
    "    X_mb = trainX[mb_idx,:]  \n",
    "    Z_mb = sample_Z(mb_size, Dim) \n",
    "    \n",
    "    M_mb = trainM[mb_idx,:]  \n",
    "    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n",
    "    H_mb = M_mb * H_mb1 + 0.5*(1-H_mb1)\n",
    "    \n",
    "    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "    \n",
    "    X_mb = torch.tensor(X_mb).float()\n",
    "    New_X_mb = torch.tensor(New_X_mb).float()\n",
    "    Z_mb = torch.tensor(Z_mb).float()\n",
    "    M_mb = torch.tensor(M_mb).float()\n",
    "    H_mb = torch.tensor(H_mb).float()\n",
    "    \n",
    "    # Train D\n",
    "    G_sample = netG(X_mb, New_X_mb, M_mb)\n",
    "    D_prob = netD(X_mb, M_mb, G_sample, H_mb)\n",
    "    D_loss = bce_loss(D_prob, M_mb)\n",
    "    \n",
    "    D_loss.backward()\n",
    "    optimD.step()\n",
    "    optimD.zero_grad()\n",
    "    \n",
    "    # Train G\n",
    "    G_sample = netG(X_mb, New_X_mb, M_mb)\n",
    "    D_prob = netD(X_mb, M_mb, G_sample, H_mb)\n",
    "    D_prob.detach_()\n",
    "    G_loss1 = ((1 - M_mb) * (torch.sigmoid(D_prob)+1e-8).log()).mean()/(1-M_mb).sum()\n",
    "    G_mse_loss = mse_loss(M_mb*X_mb, M_mb*G_sample) / M_mb.sum()\n",
    "    G_loss = G_loss1 + alpha*G_mse_loss\n",
    "    \n",
    "    G_loss.backward()\n",
    "    optimG.step()\n",
    "    optimG.zero_grad()\n",
    "    \n",
    "    G_mse_test = mse_loss((1-M_mb)*X_mb, (1-M_mb)*G_sample) / (1-M_mb).sum()\n",
    "    \n",
    "  #%% Output figure\n",
    "    if it % 100 == 0:\n",
    "      \n",
    "        mb_idx = sample_idx(Test_No, 5)\n",
    "        X_mb = testX[mb_idx,:]\n",
    "        M_mb = testM[mb_idx,:]  \n",
    "        Z_mb = sample_Z(5, Dim) \n",
    "        \n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb\n",
    "        \n",
    "        X_mb = torch.tensor(X_mb).float()\n",
    "        New_X_mb = torch.tensor(New_X_mb).float()\n",
    "        Z_mb = torch.tensor(Z_mb).float()\n",
    "        M_mb = torch.tensor(M_mb).float()\n",
    "        \n",
    "        samples1 = X_mb                \n",
    "        samples5 = M_mb * X_mb + (1-M_mb) * Z_mb\n",
    "        \n",
    "        samples2 = netG(X_mb, New_X_mb, M_mb)\n",
    "        samples2 = M_mb * X_mb + (1-M_mb) * samples2        \n",
    "        \n",
    "        Z_mb = torch.Tensor(sample_Z(5, Dim)).float()\n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  \n",
    "        \n",
    "        samples3 =netG(X_mb, New_X_mb, M_mb)\n",
    "        samples3 = M_mb * X_mb + (1-M_mb) * samples3     \n",
    "        \n",
    "        Z_mb = torch.tensor(sample_Z(5, Dim)).float()\n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb       \n",
    "        samples4 = netG(X_mb, New_X_mb, M_mb)\n",
    "        samples4 = M_mb * X_mb + (1-M_mb) * samples4     \n",
    "        \n",
    "        \n",
    "        samples = np.vstack([samples5.detach().data, samples2.detach().data, samples3.detach().data,\n",
    "                             samples4.detach().data, samples1.detach().data])          \n",
    "        \n",
    "        fig = plot(samples)\n",
    "        plt.savefig('Multiple_Impute_out1/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)\n",
    "        \n",
    "        \n",
    "    #%% Intermediate Losses\n",
    "    if it % 100 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('D_loss: {:.4}'.format(D_loss))\n",
    "        print('Train_loss: {:.4}'.format(G_mse_loss))\n",
    "        print('Test_loss: {:.4}'.format(G_mse_test))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
