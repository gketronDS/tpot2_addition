Run: 53
/cm/local/apps/slurm/var/spool/job1031877/slurm_script: line 27: 
pip install -e tpot2
pip install -r tpot2/ImputerExperiments/requirements_.txt
: No such file or directory
RunStart
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
starting loops
../data/c/847/847.pkl
working on 
../data/c/847/class_full_MAR_0.01_3
1.0809435844421387
running experiment 1/3 - Does large hyperparameter space improve reconstruction accuracy over simple
[I 2024-10-29 18:05:57,637] A new study created in memory with name: no-name-60c65b5a-5161-49b6-afa3-350feb06aa71
running
running
running
running
running
running
running
running
running
running
running
running
running
running
running
running
[I 2024-10-29 18:05:57,995] Trial 10 finished with value: 0.20724757458348617 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'median'}. Best is trial 10 with value: 0.20724757458348617.
[I 2024-10-29 18:05:58,125] Trial 12 finished with value: 0.4137313821610613 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'constant'}. Best is trial 10 with value: 0.20724757458348617.
running
running
[I 2024-10-29 18:05:58,396] Trial 17 finished with value: 0.4137313821610613 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'constant'}. Best is trial 10 with value: 0.20724757458348617.
running
[I 2024-10-29 18:06:00,398] Trial 8 finished with value: 0.17675266486704627 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 2362, 'weights': 'uniform'}. Best is trial 8 with value: 0.17675266486704627.
running
[I 2024-10-29 18:06:00,762] Trial 2 finished with value: 0.15904283730980828 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 1189, 'weights': 'distance'}. Best is trial 2 with value: 0.15904283730980828.
running
[I 2024-10-29 18:06:00,962] Trial 0 finished with value: 0.1672825888944392 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 1949, 'weights': 'distance'}. Best is trial 2 with value: 0.15904283730980828.
running
[I 2024-10-29 18:06:01,106] Trial 7 finished with value: 0.1733216168968083 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 2085, 'weights': 'uniform'}. Best is trial 2 with value: 0.15904283730980828.
running
[I 2024-10-29 18:06:01,247] Trial 22 finished with value: 0.20375524922081284 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'mean'}. Best is trial 2 with value: 0.15904283730980828.
[I 2024-10-29 18:06:01,357] Trial 6 finished with value: 0.19065443414328115 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 5250, 'weights': 'distance'}. Best is trial 2 with value: 0.15904283730980828.
[I 2024-10-29 18:06:01,473] Trial 13 finished with value: 0.1726825265767858 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 2034, 'weights': 'uniform'}. Best is trial 2 with value: 0.15904283730980828.
running
running
running
[I 2024-10-29 18:06:01,666] Trial 14 finished with value: 0.19389318129402996 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 3546, 'weights': 'uniform'}. Best is trial 2 with value: 0.15904283730980828.
running
[I 2024-10-29 18:06:03,465] Trial 21 finished with value: 0.17224157763542247 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 1998, 'weights': 'uniform'}. Best is trial 2 with value: 0.15904283730980828.
running
[I 2024-10-29 18:06:08,187] Trial 18 finished with value: 0.2650122502934159 and parameters: {'model_name': 'VAE', 'batch_size': 12, 'iterations': 5, 'learning_rate': 0.0030899942021526097, 'p_miss': 0.26004482946206725}. Best is trial 2 with value: 0.15904283730980828.
running
[I 2024-10-29 18:06:10,686] Trial 9 finished with value: 0.23703344933954212 and parameters: {'model_name': 'VAE', 'batch_size': 22, 'iterations': 3, 'learning_rate': 0.010899104689199055, 'p_miss': 0.21116213048881655}. Best is trial 2 with value: 0.15904283730980828.
running
[I 2024-10-29 18:06:11,800] Trial 23 finished with value: 0.2105946562269702 and parameters: {'model_name': 'VAE', 'batch_size': 138, 'iterations': 4, 'learning_rate': 0.04027610158947374, 'p_miss': 0.06398787537069879}. Best is trial 2 with value: 0.15904283730980828.
running
[I 2024-10-29 18:06:14,205] Trial 20 finished with value: inf and parameters: {'model_name': 'GAIN', 'batch_size': 2, 'hint_rate': 0.5049548646133596, 'alpha': 71, 'iterations': 101, 'learning_rate': 0.0022270721308732303, 'p_miss': 0.07220434766827098}. Best is trial 2 with value: 0.15904283730980828.
running
[I 2024-10-29 18:06:18,288] Trial 25 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 14, 'imputation_order': 'descending'}. Best is trial 25 with value: 0.12221474330723006.
running
[I 2024-10-29 18:06:30,218] Trial 26 finished with value: 0.20970169935815094 and parameters: {'model_name': 'VAE', 'batch_size': 101, 'iterations': 26, 'learning_rate': 0.004334489903184641, 'p_miss': 0.05907069106115338}. Best is trial 25 with value: 0.12221474330723006.
running
[I 2024-10-29 18:06:37,570] Trial 32 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 14, 'imputation_order': 'descending'}. Best is trial 25 with value: 0.12221474330723006.
running
[I 2024-10-29 18:06:48,631] Trial 4 finished with value: inf and parameters: {'model_name': 'GAIN', 'batch_size': 2, 'hint_rate': 0.5365836925343298, 'alpha': 35, 'iterations': 543, 'learning_rate': 0.0014841930492505116, 'p_miss': 0.12421860948385552}. Best is trial 25 with value: 0.12221474330723006.
running
[I 2024-10-29 18:06:51,297] Trial 33 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 14, 'imputation_order': 'descending'}. Best is trial 25 with value: 0.12221474330723006.
running
[I 2024-10-29 18:06:55,958] Trial 15 finished with value: 0.3522210406424585 and parameters: {'model_name': 'GAIN', 'batch_size': 35, 'hint_rate': 0.30720405045979615, 'alpha': 26, 'iterations': 111, 'learning_rate': 0.011620320509030245, 'p_miss': 0.08486205909912524}. Best is trial 25 with value: 0.12221474330723006.
running
[I 2024-10-29 18:07:00,933] Trial 34 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 14, 'imputation_order': 'descending'}. Best is trial 25 with value: 0.12221474330723006.
running
[I 2024-10-29 18:07:12,581] Trial 31 finished with value: 0.13737364718387934 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'median', 'n_nearest_features': 11, 'imputation_order': 'descending'}. Best is trial 25 with value: 0.12221474330723006.
running
[I 2024-10-29 18:07:12,774] Trial 35 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 14, 'imputation_order': 'descending'}. Best is trial 25 with value: 0.12221474330723006.
running
[I 2024-10-29 18:07:14,739] Trial 36 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 14, 'imputation_order': 'descending'}. Best is trial 25 with value: 0.12221474330723006.
running
[I 2024-10-29 18:07:18,891] Trial 37 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 14, 'imputation_order': 'descending'}. Best is trial 25 with value: 0.12221474330723006.
running
[I 2024-10-29 18:07:24,611] Trial 38 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 14, 'imputation_order': 'descending'}. Best is trial 25 with value: 0.12221474330723006.
running
[I 2024-10-29 18:07:26,140] Trial 5 finished with value: 0.12214336018244314 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'mean', 'n_nearest_features': 12, 'imputation_order': 'arabic', 'sample_posterior': False}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:07:35,754] Trial 39 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 14, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:07:37,647] Trial 1 finished with value: 0.17078543341653646 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'mean', 'n_nearest_features': 13, 'imputation_order': 'arabic', 'sample_posterior': True}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:07:39,169] Trial 40 finished with value: 0.14108523880452936 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 3, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:07:42,760] Trial 41 finished with value: 0.19592878869200336 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'most_frequent', 'n_nearest_features': 2, 'imputation_order': 'arabic', 'sample_posterior': True}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:08:13,396] Trial 27 finished with value: 0.2304973822127622 and parameters: {'model_name': 'VAE', 'batch_size': 1, 'iterations': 185, 'learning_rate': 0.002162817298960428, 'p_miss': 0.0774111214303702}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:08:34,390] Trial 44 finished with value: 0.13754166664531262 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'most_frequent', 'n_nearest_features': 3, 'imputation_order': 'arabic', 'sample_posterior': False}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:08:46,579] Trial 11 finished with value: inf and parameters: {'model_name': 'GAIN', 'batch_size': 1, 'hint_rate': 0.336059265916887, 'alpha': 61, 'iterations': 1833, 'learning_rate': 0.07534551295787667, 'p_miss': 0.046758651947844926}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:08:58,187] Trial 45 finished with value: 0.12559820285055462 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'constant', 'n_nearest_features': 9, 'imputation_order': 'arabic', 'sample_posterior': False}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:09:06,509] Trial 46 finished with value: 0.12255768338500819 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'constant', 'n_nearest_features': 9, 'imputation_order': 'ascending', 'sample_posterior': False}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:09:28,746] Trial 47 finished with value: 0.12272738657999879 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'ascending', 'sample_posterior': False}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:18:22,966] Trial 16 finished with value: 0.21087243981485965 and parameters: {'model_name': 'VAE', 'batch_size': 14, 'iterations': 447, 'learning_rate': 0.0885141480016823, 'p_miss': 0.2470771313811276}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:23:43,783] Trial 30 finished with value: inf and parameters: {'model_name': 'GAIN', 'batch_size': 1, 'hint_rate': 0.3361065278939245, 'alpha': 45, 'iterations': 5693, 'learning_rate': 0.0001091657740605519, 'p_miss': 0.01014614188291843}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:28:36,851] Trial 43 finished with value: 0.16698472369324321 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 1, 'imputation_order': 'random'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:29:23,340] Trial 57 finished with value: 0.12278044846421246 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 12, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:30:07,886] Trial 58 finished with value: 0.12296156794686794 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:30:51,782] Trial 59 finished with value: 0.12278044846421246 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 12, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:42:43,330] Trial 42 finished with value: 0.1388464650519358 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 4, 'imputation_order': 'random'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:43:04,126] Trial 61 finished with value: 0.14053671530066159 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'mean', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:43:39,370] Trial 62 finished with value: 0.12221479766777063 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'median', 'n_nearest_features': 13, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:44:12,726] Trial 63 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 13, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:44:13,750] Trial 64 finished with value: 0.33431849518218243 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'most_frequent'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:44:54,498] Trial 65 finished with value: 0.12278044846421246 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 12, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:45:24,529] Trial 66 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 14, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:45:57,583] Trial 67 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 13, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:46:26,486] Trial 68 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 14, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:46:29,458] Trial 69 finished with value: 0.13847275200512477 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 94, 'weights': 'distance'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:46:34,278] Trial 70 finished with value: 0.3624584686824008 and parameters: {'model_name': 'GAIN', 'batch_size': 941, 'hint_rate': 0.9362845702650062, 'alpha': 100, 'iterations': 1, 'learning_rate': 0.0001133500407142858, 'p_miss': 0.2936722375014451}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:47:09,820] Trial 71 finished with value: 0.12221485497576885 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 13, 'imputation_order': 'arabic'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:47:10,379] Trial 72 finished with value: 0.20724757458348617 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'median'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:47:34,190] Trial 73 finished with value: 0.13757913612190542 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'mean', 'n_nearest_features': 11, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:47:39,107] Trial 74 finished with value: 0.18819435092174758 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 3819, 'weights': 'distance'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:50:54,102] Trial 19 finished with value: 0.1373100996553091 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'mean', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:51:24,162] Trial 76 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 14, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:51:56,110] Trial 77 finished with value: 0.12221474330723006 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 13, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:52:27,643] Trial 78 finished with value: 0.12221485497576882 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 14, 'imputation_order': 'arabic'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:53:03,698] Trial 79 finished with value: 0.12221479766777063 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'median', 'n_nearest_features': 13, 'imputation_order': 'descending'}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 18:54:14,057] Trial 80 finished with value: 0.35525590905760557 and parameters: {'model_name': 'GAIN', 'batch_size': 920, 'hint_rate': 0.033271115661467954, 'alpha': 1, 'iterations': 32, 'learning_rate': 0.0004165917661277837, 'p_miss': 0.17363934934435524}. Best is trial 5 with value: 0.12214336018244314.
running
[I 2024-10-29 19:03:25,349] Trial 48 finished with value: 0.12060475774800443 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'ascending'}. Best is trial 48 with value: 0.12060475774800443.
running
[I 2024-10-29 19:03:43,238] Trial 49 finished with value: 0.12003824461983081 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'ascending'}. Best is trial 49 with value: 0.12003824461983081.
running
[I 2024-10-29 19:06:28,673] Trial 53 finished with value: 0.12093517511360326 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'mean', 'n_nearest_features': 11, 'imputation_order': 'random'}. Best is trial 49 with value: 0.12003824461983081.
running
[I 2024-10-29 19:06:38,529] Trial 50 finished with value: 0.11876445039514572 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 19:06:48,466] Trial 51 finished with value: 0.11910409262123953 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 19:07:38,099] Trial 52 finished with value: 0.12080914967196162 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'mean', 'n_nearest_features': 12, 'imputation_order': 'random'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 19:07:56,428] Trial 54 finished with value: 0.12076276253682641 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'mean', 'n_nearest_features': 12, 'imputation_order': 'random'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 19:11:56,225] Trial 24 finished with value: 0.20954039105366945 and parameters: {'model_name': 'VAE', 'batch_size': 22, 'iterations': 2409, 'learning_rate': 0.028307304291324418, 'p_miss': 0.040256550701746316}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 19:12:47,653] Trial 89 finished with value: 0.2758300230313028 and parameters: {'model_name': 'VAE', 'batch_size': 5, 'iterations': 16, 'learning_rate': 0.0005113085529583967, 'p_miss': 0.14415672460244766}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 19:16:26,156] Trial 55 finished with value: 0.12119895947372679 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'mean', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 19:21:55,921] Trial 56 finished with value: 0.12010591220832574 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'mean', 'n_nearest_features': 12, 'imputation_order': 'random'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 19:21:56,956] Trial 92 finished with value: 0.33431849518218243 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'most_frequent'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 19:29:52,778] Trial 60 finished with value: 0.11894327533725842 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'mean', 'n_nearest_features': 13, 'imputation_order': 'roman'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 19:30:46,217] Trial 3 finished with value: 0.21386930072502136 and parameters: {'model_name': 'VAE', 'batch_size': 9, 'iterations': 2621, 'learning_rate': 0.006325886553244027, 'p_miss': 0.06434122030139858}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 19:43:43,480] Trial 75 finished with value: 0.11999308004791845 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'mean', 'n_nearest_features': 13, 'imputation_order': 'arabic'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 19:55:26,921] Trial 87 finished with value: 0.11982713532378242 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'ascending'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 20:03:50,939] Trial 90 finished with value: 0.11935570518422786 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 20:06:43,219] Trial 91 finished with value: 0.1188630192390387 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 20:12:31,849] Trial 93 finished with value: 0.11888707992116314 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'random'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 20:18:21,520] Trial 94 finished with value: 0.1196134435247076 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'ascending'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 20:18:25,206] Trial 101 finished with value: 0.20375524922081284 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 5107, 'weights': 'uniform'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 20:18:55,859] Trial 95 finished with value: 0.12042137628325615 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'ascending'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 20:19:00,943] Trial 103 finished with value: 0.3692897591430044 and parameters: {'model_name': 'GAIN', 'batch_size': 257, 'hint_rate': 0.9562456796942215, 'alpha': 97, 'iterations': 1, 'learning_rate': 0.0004886524447326022, 'p_miss': 0.18880583747121582}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 20:32:20,673] Trial 96 finished with value: 0.12005150209091187 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'ascending'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 20:43:33,516] Trial 97 finished with value: 0.12074679285404051 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'ascending'}. Best is trial 50 with value: 0.11876445039514572.
running
[I 2024-10-29 20:53:01,091] Trial 98 finished with value: 0.11860906577138905 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 20:55:09,867] Trial 99 finished with value: 0.11997332026850346 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 21:01:46,018] Trial 100 finished with value: 0.12009767667701396 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 21:04:39,938] Trial 102 finished with value: 0.12076997235780493 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 9, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 21:05:25,119] Trial 104 finished with value: 0.12103302353326544 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 9, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 21:15:23,438] Trial 28 finished with value: 0.35702740469742855 and parameters: {'model_name': 'GAIN', 'batch_size': 885, 'hint_rate': 0.7530178281712734, 'alpha': 30, 'iterations': 5909, 'learning_rate': 0.00013956797337974012, 'p_miss': 0.01964169176235575}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 21:18:25,804] Trial 105 finished with value: 0.11947840867174331 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 9, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 21:19:01,960] Trial 113 finished with value: 0.2696174393816027 and parameters: {'model_name': 'VAE', 'batch_size': 57, 'iterations': 10, 'learning_rate': 0.000976416714215904, 'p_miss': 0.1167137449169384}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 21:33:53,017] Trial 106 finished with value: 0.12007848416230087 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 21:35:25,781] Trial 112 finished with value: 0.20789662853277627 and parameters: {'model_name': 'VAE', 'batch_size': 56, 'iterations': 463, 'learning_rate': 0.001011007576556454, 'p_miss': 0.12053162096191725}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 21:39:08,118] Trial 107 finished with value: 0.12128072381089564 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 9, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 21:40:54,295] Trial 108 finished with value: 0.120832772041534 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 9, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 21:47:45,647] Trial 109 finished with value: 0.12069972812988032 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 9, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 21:55:34,247] Trial 110 finished with value: 0.11953883347382632 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 21:56:37,115] Trial 111 finished with value: 0.119941863514986 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 22:09:53,599] Trial 114 finished with value: 0.11883055859607541 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 22:16:24,960] Trial 115 finished with value: 0.12117151307237539 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 22:27:09,865] Trial 116 finished with value: 0.1186346083370691 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 22:30:33,393] Trial 117 finished with value: 0.1199349183451663 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 22:30:33,901] Trial 125 finished with value: 0.20375524922081284 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'mean'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 22:31:36,584] Trial 118 finished with value: 0.11935622138105742 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 22:39:04,114] Trial 119 finished with value: 0.11891795601760191 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 22:46:23,701] Trial 120 finished with value: 0.11901651255954253 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 22:47:37,042] Trial 121 finished with value: 0.11937042221343334 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 22:52:39,450] Trial 122 finished with value: 0.12214617717084571 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:00:15,833] Trial 29 finished with value: 0.34979889180752144 and parameters: {'model_name': 'GAIN', 'batch_size': 991, 'hint_rate': 0.02889982377909356, 'alpha': 49, 'iterations': 8811, 'learning_rate': 0.00013099158789498444, 'p_miss': 0.022361651707488522}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:05:45,707] Trial 123 finished with value: 0.11997585830795773 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:16:33,062] Trial 82 finished with value: 0.22030375287341145 and parameters: {'model_name': 'VAE', 'batch_size': 5, 'iterations': 5422, 'learning_rate': 0.0005628890734834436, 'p_miss': 0.1450222953124729}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:16:41,730] Trial 124 finished with value: 0.11961240499076924 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:16:44,561] Trial 135 finished with value: 0.13976388944494186 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 118, 'weights': 'distance'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:19:54,419] Trial 126 finished with value: 0.11976947852719197 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:20:19,547] Trial 127 finished with value: 0.11867073786255729 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:27:40,551] Trial 88 finished with value: 0.2071922371780893 and parameters: {'model_name': 'VAE', 'batch_size': 261, 'iterations': 6201, 'learning_rate': 0.0007341541228912738, 'p_miss': 0.13643771305840816}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:27:52,491] Trial 128 finished with value: 0.11899192873567692 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:28:16,260] Trial 140 finished with value: 0.12906320174689503 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'most_frequent', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:35:04,320] Trial 129 finished with value: 0.11861424743557436 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 11, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:36:16,343] Trial 130 finished with value: 0.12042622183111476 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 11, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:40:57,125] Trial 131 finished with value: 0.11985236003460398 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 11, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:43:13,668] Trial 144 finished with value: 0.16968934663333685 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'median', 'n_nearest_features': 12, 'imputation_order': 'random', 'sample_posterior': True}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:43:51,320] Trial 83 finished with value: 0.21739728914128603 and parameters: {'model_name': 'VAE', 'batch_size': 5, 'iterations': 6928, 'learning_rate': 0.0005447863437368035, 'p_miss': 0.1377996677392262}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:49:44,121] Trial 132 finished with value: 0.11928034856383303 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 11, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:50:49,606] Trial 84 finished with value: 0.21631710648754504 and parameters: {'model_name': 'VAE', 'batch_size': 6, 'iterations': 6551, 'learning_rate': 0.000511815478492755, 'p_miss': 0.14434658246187995}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-29 23:54:22,098] Trial 133 finished with value: 0.119908714712051 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 11, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:04:47,677] Trial 134 finished with value: 0.12004985107950014 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 11, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:05:02,590] Trial 136 finished with value: 0.11992526736323675 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:08:00,988] Trial 137 finished with value: 0.11912934667388705 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:09:53,094] Trial 138 finished with value: 0.11910275541197002 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 12, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:16:44,136] Trial 139 finished with value: 0.11939925853141901 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 12, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:17:42,157] Trial 141 finished with value: 0.11970343720451555 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:24:14,726] Trial 142 finished with value: 0.11972943811434096 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:25:17,846] Trial 143 finished with value: 0.119691034135576 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:27:20,878] Trial 85 finished with value: 0.2133224218363612 and parameters: {'model_name': 'VAE', 'batch_size': 5, 'iterations': 7578, 'learning_rate': 0.0004973429260217419, 'p_miss': 0.1516416699004191}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:30:29,401] Trial 145 finished with value: 0.1201486701993767 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:30:47,366] Trial 146 finished with value: 0.11917630512985884 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:36:35,449] Trial 147 finished with value: 0.11871021608176593 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:37:28,166] Trial 148 finished with value: 0.11988894959079252 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:41:09,926] Trial 149 finished with value: 0.11955349364546763 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:45:25,359] Trial 86 finished with value: 0.21467749863956512 and parameters: {'model_name': 'VAE', 'batch_size': 5, 'iterations': 9336, 'learning_rate': 0.0005347641380985884, 'p_miss': 0.14641659234544674}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:52:03,263] Trial 150 finished with value: 0.11909465279760664 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:52:22,492] Trial 151 finished with value: 0.11948922847917151 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:55:27,275] Trial 152 finished with value: 0.1193677773346189 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 12, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:55:48,012] Trial 161 finished with value: 0.40142575495637844 and parameters: {'model_name': 'GAIN', 'batch_size': 393, 'hint_rate': 0.7371660897144359, 'alpha': 1, 'iterations': 1020, 'learning_rate': 0.00023197497106609022, 'p_miss': 0.21906325073502655}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:57:08,191] Trial 153 finished with value: 0.11894442983385596 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 12, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:57:08,791] Trial 169 finished with value: 0.20724757458348617 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'median'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 00:57:31,686] Trial 170 finished with value: 0.13584182984528995 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'most_frequent', 'n_nearest_features': 13, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:03:07,063] Trial 163 finished with value: 0.38162695335681207 and parameters: {'model_name': 'GAIN', 'batch_size': 354, 'hint_rate': 0.7485458070395822, 'alpha': 81, 'iterations': 1129, 'learning_rate': 0.0002721487029720383, 'p_miss': 0.2271668042223412}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:03:41,951] Trial 164 finished with value: 0.3675421147122389 and parameters: {'model_name': 'GAIN', 'batch_size': 399, 'hint_rate': 0.7024011615774308, 'alpha': 9, 'iterations': 908, 'learning_rate': 0.008102410109357291, 'p_miss': 0.099189473039073}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:04:15,956] Trial 154 finished with value: 0.1202519490658233 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 12, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:05:31,541] Trial 155 finished with value: 0.11989781668575158 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 12, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:06:02,373] Trial 162 finished with value: 0.38156186868252706 and parameters: {'model_name': 'GAIN', 'batch_size': 424, 'hint_rate': 0.7245416729847061, 'alpha': 2, 'iterations': 1485, 'learning_rate': 0.0002107647995798474, 'p_miss': 0.21917562535568932}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:07:40,033] Trial 175 finished with value: 0.16746732334942585 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'most_frequent', 'n_nearest_features': 10, 'imputation_order': 'random', 'sample_posterior': True}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:10:18,943] Trial 167 finished with value: 0.3776121150065751 and parameters: {'model_name': 'GAIN', 'batch_size': 306, 'hint_rate': 0.7055443458450124, 'alpha': 5, 'iterations': 879, 'learning_rate': 0.008381777060399019, 'p_miss': 0.22160798101984086}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:10:24,865] Trial 157 finished with value: 0.12001238754362967 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:11:46,818] Trial 165 finished with value: 0.4207429676828536 and parameters: {'model_name': 'GAIN', 'batch_size': 477, 'hint_rate': 0.7128307374132363, 'alpha': 4, 'iterations': 1115, 'learning_rate': 0.00025920528864040865, 'p_miss': 0.21914525191785822}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:11:49,988] Trial 166 finished with value: 0.3692703678863726 and parameters: {'model_name': 'GAIN', 'batch_size': 422, 'hint_rate': 0.7296767028492719, 'alpha': 1, 'iterations': 1131, 'learning_rate': 0.0002489320165014218, 'p_miss': 0.09994466056577117}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:11:53,101] Trial 81 finished with value: 0.20724967088905794 and parameters: {'model_name': 'VAE', 'batch_size': 318, 'iterations': 9735, 'learning_rate': 0.00042970706486388394, 'p_miss': 0.1433007344267841}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:12:12,493] Trial 158 finished with value: 0.12057959199108556 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:13:14,634] Trial 156 finished with value: 0.11941459755200778 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 12, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:13:17,201] Trial 184 finished with value: 0.1850515353360096 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 3578, 'weights': 'distance'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:15:01,895] Trial 159 finished with value: 0.12082628383884582 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:15:15,901] Trial 160 finished with value: 0.12033601878301199 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:34:03,979] Trial 180 finished with value: 0.13365343007608463 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 5, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:36:39,747] Trial 168 finished with value: 0.12010177015602959 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:37:01,700] Trial 181 finished with value: 0.13445335199548514 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:37:18,597] Trial 183 finished with value: 0.13018253799659474 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:38:01,694] Trial 171 finished with value: 0.1215553950989634 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 10, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:38:20,880] Trial 185 finished with value: 0.13068792422108422 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:40:04,739] Trial 186 finished with value: 0.131391638106566 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:42:32,195] Trial 172 finished with value: 0.12034490019491084 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 10, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:42:54,128] Trial 173 finished with value: 0.12127670020323897 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 10, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:43:11,081] Trial 174 finished with value: 0.12053958155794217 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 10, 'imputation_order': 'roman'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:44:35,639] Trial 176 finished with value: 0.12041028184730555 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 10, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:44:53,872] Trial 177 finished with value: 0.1198055939564681 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
running
[I 2024-10-30 01:47:06,509] Trial 179 finished with value: 0.1206263003148583 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 01:47:10,619] Trial 178 finished with value: 0.1214786301360763 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 10, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 01:48:14,395] Trial 182 finished with value: 0.12061111154586221 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'random'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 01:53:23,001] Trial 187 finished with value: 0.12162898799211082 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 02:09:47,497] Trial 188 finished with value: 0.1189727999964069 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 02:12:09,812] Trial 189 finished with value: 0.1197490068195197 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 02:12:25,639] Trial 190 finished with value: 0.11954549166263262 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 02:12:40,872] Trial 191 finished with value: 0.11928213899612727 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 02:13:26,730] Trial 192 finished with value: 0.11919931296197153 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 02:13:32,350] Trial 193 finished with value: 0.11984526244712712 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 02:15:12,227] Trial 194 finished with value: 0.11959792153949114 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 02:17:25,907] Trial 195 finished with value: 0.12022335984008385 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 02:17:37,000] Trial 196 finished with value: 0.11961748639517025 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 02:17:56,022] Trial 197 finished with value: 0.11924416279075249 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 02:18:58,790] Trial 198 finished with value: 0.11963539129423706 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
[I 2024-10-30 02:19:18,098] Trial 199 finished with value: 0.11946887829595583 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 11, 'imputation_order': 'ascending'}. Best is trial 98 with value: 0.11860906577138905.
fit
auto fit
auto transform
0     0
1     0
2     0
3     0
4     0
5     0
6     0
7     0
8     0
9     0
10    0
11    0
12    0
13    0
dtype: int64
0     0
1     0
2     0
3     0
4     0
5     0
6     0
7     0
8     0
9     0
10    0
11    0
12    0
13    0
dtype: int64
0.11860906577138905
{'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 10, 'imputation_order': 'ascending'}
running experiment 2/3 - Does reconstruction give good automl predictions
Start est fit
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
  0%|          | 0/25 [00:00<?, ?it/s]Generation:   0%|          | 0/25 [00:00<?, ?it/s]Generation:  1
Best f1_score score: 0.8569775660234754
Generation:   4%|▍         | 1/25 [00:15<06:00, 15.02s/it]Generation:  2
Best f1_score score: 0.8618453869343421
Generation:   8%|▊         | 2/25 [00:39<07:59, 20.84s/it]Generation:  3
Best f1_score score: 0.86386647869511
Generation:  12%|█▏        | 3/25 [00:59<07:20, 20.04s/it]Generation:  4
Best f1_score score: 0.86386647869511
Generation:  16%|█▌        | 4/25 [04:37<34:29, 98.53s/it]Generation:  5
Best f1_score score: 0.86386647869511
Generation:  20%|██        | 5/25 [04:57<23:18, 69.93s/it]Generation:  6
Best f1_score score: 0.863964196562379
Generation:  24%|██▍       | 6/25 [05:17<16:47, 53.03s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554659a0f10> 
 The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead. 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1466, in wrapper
    estimator._validate_params()
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 666, in _validate_params
    validate_parameter_constraints(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 95, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.

Generation:  7
Best f1_score score: 0.8664960199123175
Generation:  28%|██▊       | 7/25 [05:33<12:15, 40.88s/it]Generation:  8
Best f1_score score: 0.8664960199123175
Generation:  32%|███▏      | 8/25 [05:50<09:26, 33.30s/it]Generation:  9
Best f1_score score: 0.8664960199123175
Generation:  36%|███▌      | 9/25 [06:06<07:29, 28.09s/it]Generation:  10
Best f1_score score: 0.867634043140639
Generation:  40%|████      | 10/25 [06:28<06:32, 26.16s/it]Generation:  11
Best f1_score score: 0.867634043140639
Generation:  44%|████▍     | 11/25 [06:50<05:47, 24.84s/it]Generation:  12
Best f1_score score: 0.867634043140639
Generation:  48%|████▊     | 12/25 [07:05<04:44, 21.91s/it]Generation:  13
Best f1_score score: 0.867634043140639
Generation:  52%|█████▏    | 13/25 [07:23<04:06, 20.50s/it]Generation:  14
Best f1_score score: 0.8676489163031091
Generation:  56%|█████▌    | 14/25 [10:58<14:32, 79.31s/it]Generation:  15
Best f1_score score: 0.8696907994080254
Generation:  60%|██████    | 15/25 [14:54<21:05, 126.50s/it]Generation:  16
Best f1_score score: 0.8696907994080254
Generation:  64%|██████▍   | 16/25 [15:14<14:11, 94.66s/it] Generation:  17
Best f1_score score: 0.8696907994080254
Generation:  68%|██████▊   | 17/25 [15:28<09:22, 70.35s/it]Generation:  18
Best f1_score score: 0.8696907994080254
Generation:  72%|███████▏  | 18/25 [15:58<06:48, 58.29s/it]Generation:  19
Best f1_score score: 0.8696907994080254
Generation:  76%|███████▌  | 19/25 [16:13<04:30, 45.06s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554651b2cb0> 
 The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead. 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1466, in wrapper
    estimator._validate_params()
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 666, in _validate_params
    validate_parameter_constraints(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 95, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.

Generation:  20
Best f1_score score: 0.8696907994080254
Generation:  80%|████████  | 20/25 [16:29<03:01, 36.34s/it]Generation:  21
Best f1_score score: 0.8696907994080254
Generation:  84%|████████▍ | 21/25 [16:59<02:17, 34.48s/it]Generation:  22
Best f1_score score: 0.8698619220031208
Generation:  88%|████████▊ | 22/25 [18:46<02:49, 56.38s/it]Generation:  23
Best f1_score score: 0.8698619220031208
Generation:  92%|█████████▏| 23/25 [20:08<02:08, 64.03s/it]Generation:  24
Best f1_score score: 0.8698619220031208
Generation:  96%|█████████▌| 24/25 [20:36<00:53, 53.12s/it]Generation:  25
Best f1_score score: 0.8698619220031208
Generation: 100%|██████████| 25/25 [20:55<00:00, 42.92s/it]Generation: 100%|██████████| 25/25 [20:58<00:00, 50.35s/it]
2024-10-30 02:48:01,533 - distributed.scheduler - ERROR - Removing worker 'tcp://127.0.0.1:42453' caused the cluster to lose scattered data, which can't be recovered: {'ndarray-75556c436dd71b66be36a6e72cf17556', 'ndarray-37f394b4e7ca3c996b1974b9fc3696c0'} (stimulus_id='handle-worker-cleanup-1730281681.5333776')
Fitted
Pipeline(steps=[('histgradientboostingclassifier',
                 HistGradientBoostingClassifier(early_stopping=True,
                                                l2_regularization=1.6336e-09,
                                                learning_rate=0.0479602797126,
                                                max_features=0.1785445438616,
                                                max_leaf_nodes=211,
                                                min_samples_leaf=113,
                                                n_iter_no_change=6, tol=0.0001,
                                                validation_fraction=0.2016144766349))])
score start
train score: {'auroc': 0.9628889949509489, 'accuracy': 0.8961779806046777, 'balanced_accuracy': 0.8956348119307618, 'logloss': 0.2584786546597611, 'f1': 0.8957134265130813}
original test score: {'auroc': 0.9382764227642277, 'accuracy': 0.8600760456273764, 'balanced_accuracy': 0.859883855981417, 'logloss': 0.3171150193542329, 'f1': 0.8595946372532635}
imputed test score: {'auroc': 0.9381881533101046, 'accuracy': 0.8593155893536122, 'balanced_accuracy': 0.8590708478513356, 'logloss': 0.3174231280371922, 'f1': 0.8588188662448402}
score end
check intended
EXP2 Finished
running experiment 3/3 - What is the best automl settings?
<tpot2.search_spaces.pipelines.sequential.SequentialPipeline object at 0x155435014610>
Start tpot fit
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
  0%|          | 0/25 [00:00<?, ?it/s]Generation:   0%|          | 0/25 [00:00<?, ?it/s]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474cd4dc0> 

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474cd4e80> 

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474cd47f0> 

Generation:  1
Best f1_score score: 0.8587693469488858
Generation:   4%|▍         | 1/25 [10:03<4:01:31, 603.82s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15547459a5c0> 
 Negative values in data passed to MultinomialNB (input X) 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 759, in fit
    self._count(X, Y)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 881, in _count
    check_non_negative(X, "MultinomialNB (input X)")
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1689, in check_non_negative
    raise ValueError("Negative values in data passed to %s" % whom)
ValueError: Negative values in data passed to MultinomialNB (input X)

WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15547464d060> 
 Negative values in data passed to MultinomialNB (input X) 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 759, in fit
    self._count(X, Y)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 881, in _count
    check_non_negative(X, "MultinomialNB (input X)")
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1689, in check_non_negative
    raise ValueError("Negative values in data passed to %s" % whom)
ValueError: Negative values in data passed to MultinomialNB (input X)

Generation:  2
Best f1_score score: 0.8587693469488858
Generation:   8%|▊         | 2/25 [10:46<1:44:54, 273.69s/it]Generation:  3
Best f1_score score: 0.8608844259195536
Generation:  12%|█▏        | 3/25 [15:32<1:42:28, 279.48s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474d41fc0> 

Generation:  4
Best f1_score score: 0.8608844259195536
Generation:  16%|█▌        | 4/25 [25:38<2:22:50, 408.11s/it]Generation:  5
Best f1_score score: 0.8666288433081112
Generation:  20%|██        | 5/25 [26:52<1:35:56, 287.85s/it]Generation:  6
Best f1_score score: 0.8666288433081112
Generation:  24%|██▍       | 6/25 [28:17<1:09:17, 218.81s/it]Generation:  7
Best f1_score score: 0.8666288433081112
Generation:  28%|██▊       | 7/25 [28:36<46:05, 153.62s/it]  Generation:  8
Best f1_score score: 0.8666288433081112
Generation:  32%|███▏      | 8/25 [28:56<31:25, 110.93s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554550c9090> 
 Input X contains NaN.
MLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 751, in fit
    return self._fit(X, y, incremental=False)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 441, in _fit
    X, y = self._validate_input(X, y, incremental, reset=first_pass)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 1096, in _validate_input
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
MLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  9
Best f1_score score: 0.8666288433081112
Generation:  36%|███▌      | 9/25 [29:44<24:22, 91.39s/it] Generation:  10
Best f1_score score: 0.8666288433081112
Generation:  40%|████      | 10/25 [30:41<20:09, 80.64s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155452d16da0> 

Generation:  11
Best f1_score score: 0.868549743709548
Generation:  44%|████▍     | 11/25 [40:47<56:20, 241.50s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554573e6b90> 

Generation:  12
Best f1_score score: 0.868549743709548
Generation:  48%|████▊     | 12/25 [50:54<1:16:26, 352.79s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155454bd6590> 

Generation:  13
Best f1_score score: 0.868549743709548
Generation:  52%|█████▏    | 13/25 [1:01:03<1:26:02, 430.18s/it]Generation:  14
Best f1_score score: 0.868549743709548
Generation:  56%|█████▌    | 14/25 [1:02:34<1:00:04, 327.70s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155308eb4d00> 
 Input X contains NaN.
MLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 751, in fit
    return self._fit(X, y, incremental=False)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 441, in _fit
    X, y = self._validate_input(X, y, incremental, reset=first_pass)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 1096, in _validate_input
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
MLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155454eec850> 

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155452514280> 

Generation:  15
Best f1_score score: 0.869318550967081
Generation:  60%|██████    | 15/25 [1:12:40<1:08:36, 411.62s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554568dfbe0> 

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474d5f670> 

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155454a738b0> 

Generation:  16
Best f1_score score: 0.869318550967081
Generation:  64%|██████▍   | 16/25 [1:22:48<1:10:36, 470.72s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545bcd8520> 
 Input X contains NaN.
MLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 751, in fit
    return self._fit(X, y, incremental=False)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 441, in _fit
    X, y = self._validate_input(X, y, incremental, reset=first_pass)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 1096, in _validate_input
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
MLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15544d8467a0> 
 Input X contains NaN.
MLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 751, in fit
    return self._fit(X, y, incremental=False)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 441, in _fit
    X, y = self._validate_input(X, y, incremental, reset=first_pass)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 1096, in _validate_input
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
MLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155453b12830> 

Generation:  17
Best f1_score score: 0.8695095476168546
Generation:  68%|██████▊   | 17/25 [1:32:56<1:08:15, 512.00s/it]Generation:  18
Best f1_score score: 0.8695095476168546
Generation:  72%|███████▏  | 18/25 [1:35:48<47:49, 409.94s/it]  Generation:  19
Best f1_score score: 0.8695095476168546
Generation:  76%|███████▌  | 19/25 [1:37:47<32:15, 322.50s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155452884850> 

Generation:  20
Best f1_score score: 0.8716435928858148
Generation:  80%|████████  | 20/25 [1:47:55<34:00, 408.17s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545fa3ded0> 
 Input X contains NaN.
MLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 751, in fit
    return self._fit(X, y, incremental=False)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 441, in _fit
    X, y = self._validate_input(X, y, incremental, reset=first_pass)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 1096, in _validate_input
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
MLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545da3b460> 

Generation:  21
Best f1_score score: 0.8716435928858148
Generation:  84%|████████▍ | 21/25 [1:58:01<31:10, 467.59s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155456bd9330> 

Generation:  22
Best f1_score score: 0.8716435928858148
Generation:  88%|████████▊ | 22/25 [2:08:10<25:29, 509.96s/it]Generation:  23
Best f1_score score: 0.8716435928858148
Generation:  92%|█████████▏| 23/25 [2:12:38<14:35, 437.61s/it]Generation:  24
Best f1_score score: 0.8716435928858148
Generation:  96%|█████████▌| 24/25 [2:17:26<06:32, 392.58s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474d5ce20> 
 Input X contains NaN.
MLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 751, in fit
    return self._fit(X, y, incremental=False)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 441, in _fit
    X, y = self._validate_input(X, y, incremental, reset=first_pass)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py", line 1096, in _validate_input
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
MLPClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  25
Best f1_score score: 0.8716435928858148
Generation: 100%|██████████| 25/25 [2:18:19<00:00, 290.55s/it]Generation: 100%|██████████| 25/25 [2:18:19<00:00, 331.96s/it]
2024-10-30 05:06:30,682 - distributed.scheduler - ERROR - Removing worker 'tcp://127.0.0.1:47025' caused the cluster to lose scattered data, which can't be recovered: {'ndarray-75556c436dd71b66be36a6e72cf17556', 'DataFrame-4de7ac974c9ed4ac47f6e96b9f95a6be'} (stimulus_id='handle-worker-cleanup-1730289990.6824527')
Fitted
Pipeline(steps=[('knnimputer', KNNImputer(n_neighbors=22)),
                ('mlpclassifier',
                 MLPClassifier(activation='tanh', alpha=0.0070670682108,
                               early_stopping=True,
                               hidden_layer_sizes=[66, 66, 66],
                               learning_rate_init=0.0064419452681,
                               n_iter_no_change=32))])
transform worked
try transform
transform worked
score start
train score: {'auroc': 0.9493482654253725, 'accuracy': 0.8794447613614755, 'balanced_accuracy': 0.8782820938354865, 'logloss': 0.28909904912314777, 'f1': 0.8787608010397855}
test score: {'auroc': 0.9425180023228803, 'accuracy': 0.864638783269962, 'balanced_accuracy': 0.8633797909407666, 'logloss': 0.3123742499395763, 'f1': 0.8638672020397529}
original test score: {'auroc': 0.9430267131242742, 'accuracy': 0.8653992395437262, 'balanced_accuracy': 0.8640940766550522, 'logloss': 0.3103935227813493, 'f1': 0.8646163242158531}
score end
847
lvl
0.01
type
MAR
num_run
3
class_full
finished
all finished
full run takes
11.011931649777624
hours
DONE
