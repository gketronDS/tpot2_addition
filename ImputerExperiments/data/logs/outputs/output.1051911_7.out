Run: 7
/cm/local/apps/slurm/var/spool/job1051911/slurm_script: line 26: 
pip install -e tpot2
pip install -r tpot2/ImputerExperiments/requirements_.txt
: No such file or directory
RunStart
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
starting loops
../data/c/4135/4135.pkl
working on 
../data/c/4135/class_full_MAR_0.3_1
2.5765061378479004
running experiment 1/3 - Does large hyperparameter space improve reconstruction accuracy over simple
[I 2024-11-15 19:31:32,466] A new study created in memory with name: no-name-cb8ac658-6f9a-4123-bde1-908ec65b4fe9
running
running
running
running
running
running
running
running
running
running
running
running
running
running
running
running
[I 2024-11-15 19:31:33,363] Trial 2 finished with value: 0.29860598694805385 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'most_frequent'}. Best is trial 2 with value: 0.29860598694805385.
running
[I 2024-11-15 19:31:33,598] Trial 5 finished with value: 0.2707136399303752 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'median'}. Best is trial 5 with value: 0.2707136399303752.
running
[I 2024-11-15 19:31:46,837] Trial 0 finished with value: 0.24660931626365765 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 3880, 'weights': 'uniform'}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:31:47,619] Trial 3 finished with value: inf and parameters: {'model_name': 'GAIN', 'batch_size': 1, 'hint_rate': 0.3222658208418016, 'alpha': 97, 'iterations': 21, 'learning_rate': 0.03253953132164046, 'p_miss': 0.02518072909235053}. Best is trial 0 with value: 0.24660931626365765.
[I 2024-11-15 19:31:47,804] Trial 15 finished with value: 0.3428921968104199 and parameters: {'model_name': 'VAE', 'batch_size': 1, 'iterations': 2, 'learning_rate': 0.0004049710471162611, 'p_miss': 0.18447179433787014}. Best is trial 0 with value: 0.24660931626365765.
running
running
[I 2024-11-15 19:31:48,275] Trial 18 finished with value: 0.2707136399303752 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'median'}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:31:48,739] Trial 10 finished with value: 0.2616508015984005 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 20513, 'weights': 'uniform'}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:31:49,077] Trial 11 finished with value: 0.2616508015984005 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 20017, 'weights': 'uniform'}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:31:51,307] Trial 8 finished with value: 0.30294178407808503 and parameters: {'model_name': 'VAE', 'batch_size': 60, 'iterations': 1, 'learning_rate': 0.023736602756283812, 'p_miss': 0.2345515712292351}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:31:51,893] Trial 13 finished with value: inf and parameters: {'model_name': 'GAIN', 'batch_size': 1, 'hint_rate': 0.8061627873366064, 'alpha': 76, 'iterations': 53, 'learning_rate': 0.005371410499028991, 'p_miss': 0.2771095583104595}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:31:52,480] Trial 16 finished with value: 0.24880116272007263 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 4, 'imputation_order': 'random'}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:31:52,844] Trial 21 finished with value: 0.3378224209179651 and parameters: {'model_name': 'VAE', 'batch_size': 6, 'iterations': 1, 'learning_rate': 0.00948935317329511, 'p_miss': 0.23536418362592912}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:31:54,575] Trial 9 finished with value: 0.2534482975017788 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 10315, 'weights': 'distance'}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:31:55,355] Trial 23 finished with value: 0.6014080022557758 and parameters: {'model_name': 'GAIN', 'batch_size': 10, 'hint_rate': 0.6336701404738566, 'alpha': 1, 'iterations': 1, 'learning_rate': 0.0006736940207486622, 'p_miss': 0.1459122914157258}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:32:04,201] Trial 19 finished with value: 0.2616508015984005 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 21766, 'weights': 'uniform'}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:32:10,071] Trial 22 finished with value: 0.3357138178106288 and parameters: {'model_name': 'VAE', 'batch_size': 4, 'iterations': 20, 'learning_rate': 0.0008910113983564173, 'p_miss': 0.22925875168866877}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:32:11,913] Trial 25 finished with value: 0.2584045751394254 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'constant', 'n_nearest_features': 1, 'imputation_order': 'ascending'}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:32:16,748] Trial 4 finished with value: 0.33886313365153936 and parameters: {'model_name': 'VAE', 'batch_size': 46, 'iterations': 29, 'learning_rate': 0.00018333974422440763, 'p_miss': 0.2377182227627679}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:32:19,175] Trial 27 finished with value: 0.24880116272007263 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 4, 'imputation_order': 'random'}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:32:19,399] Trial 28 finished with value: 0.24880116272007263 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 4, 'imputation_order': 'random'}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:32:19,871] Trial 26 finished with value: 0.24880116272007263 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 4, 'imputation_order': 'random'}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:32:21,153] Trial 12 finished with value: 0.3704612551258385 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'constant', 'n_nearest_features': 2, 'imputation_order': 'roman', 'sample_posterior': True}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:32:22,691] Trial 29 finished with value: 0.24880116272007263 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 4, 'imputation_order': 'random'}. Best is trial 0 with value: 0.24660931626365765.
running
[I 2024-11-15 19:32:26,562] Trial 14 finished with value: 0.24036071668593695 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'mean', 'n_nearest_features': 2, 'imputation_order': 'roman'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:32:27,426] Trial 24 finished with value: 0.34276528184297506 and parameters: {'model_name': 'VAE', 'batch_size': 2, 'iterations': 36, 'learning_rate': 0.02765153083977885, 'p_miss': 0.1954014876666073}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:32:29,107] Trial 7 finished with value: 0.3715818967180439 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'median', 'n_nearest_features': 3, 'imputation_order': 'roman', 'sample_posterior': True}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:32:45,163] Trial 30 finished with value: 0.24880116272007263 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 4, 'imputation_order': 'random'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:32:48,965] Trial 41 finished with value: 0.24939807395999586 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 567, 'weights': 'distance'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:33:01,947] Trial 31 finished with value: 0.24880116272007263 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'mean', 'n_nearest_features': 4, 'imputation_order': 'random'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:33:03,716] Trial 44 finished with value: 0.2616508015984005 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'mean'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:33:05,697] Trial 42 finished with value: 0.24762033660404179 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 183, 'weights': 'distance'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:33:10,523] Trial 43 finished with value: 0.24283508875812804 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 1449, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:33:22,741] Trial 45 finished with value: 0.24249745634377967 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 1302, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:33:24,408] Trial 46 finished with value: 0.24884945118996188 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 388, 'weights': 'distance'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:33:29,259] Trial 47 finished with value: 0.2429381879952505 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 4, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:33:43,878] Trial 48 finished with value: 0.24806335290994141 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 5818, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:33:47,197] Trial 49 finished with value: 0.2492307743549363 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 7631, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:33:51,624] Trial 50 finished with value: 0.24889877111606656 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 7159, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:33:58,906] Trial 6 finished with value: 0.6147066138542945 and parameters: {'model_name': 'GAIN', 'batch_size': 266, 'hint_rate': 0.38518397595059495, 'alpha': 16, 'iterations': 78, 'learning_rate': 0.08051771906556018, 'p_miss': 0.18459213736444438}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:07,127] Trial 51 finished with value: 0.24853651277793504 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 6599, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:11,809] Trial 52 finished with value: 0.25524111624308243 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 15455, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:13,122] Trial 34 finished with value: 0.2469762508175005 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'median', 'n_nearest_features': 9, 'imputation_order': 'arabic', 'sample_posterior': False}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:18,458] Trial 53 finished with value: 0.2548690495293111 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 15037, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:26,026] Trial 36 finished with value: 0.24279968866844714 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'mean', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:26,253] Trial 54 finished with value: 0.2539953575991341 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 14021, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:27,737] Trial 59 finished with value: 0.6362487529242203 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'constant'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:31,481] Trial 37 finished with value: 0.24407876809543172 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'most_frequent', 'n_nearest_features': 8, 'imputation_order': 'random'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:32,046] Trial 35 finished with value: 0.24424959970073803 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'median', 'n_nearest_features': 9, 'imputation_order': 'descending'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:32,601] Trial 56 finished with value: 0.24624032286484093 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 3494, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:32,966] Trial 55 finished with value: 0.2530269784953959 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 12832, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:33,577] Trial 57 finished with value: 0.24646835350645113 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 3716, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:35,323] Trial 38 finished with value: 0.2434000722778004 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'median', 'n_nearest_features': 8, 'imputation_order': 'descending'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:35,927] Trial 40 finished with value: 0.24122916264767708 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'median', 'n_nearest_features': 7, 'imputation_order': 'roman'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:36,535] Trial 39 finished with value: 0.24369019327648153 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'most_frequent', 'n_nearest_features': 8, 'imputation_order': 'arabic'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:37,968] Trial 58 finished with value: 0.24618855151631766 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 3440, 'weights': 'uniform'}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:34:39,844] Trial 32 finished with value: 0.246781744805878 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'mean', 'n_nearest_features': 6, 'imputation_order': 'random', 'sample_posterior': False}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 19:52:06,270] Trial 1 finished with value: 0.2785846593838268 and parameters: {'model_name': 'VAE', 'batch_size': 12, 'iterations': 423, 'learning_rate': 0.004272735184151015, 'p_miss': 0.1524821840014694}. Best is trial 14 with value: 0.24036071668593695.
running
[I 2024-11-15 20:00:03,291] Trial 20 finished with value: 0.22592450956162105 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 2, 'imputation_order': 'descending'}. Best is trial 20 with value: 0.22592450956162105.
running
[I 2024-11-15 20:19:04,977] Trial 33 finished with value: 0.2197840027957103 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'mean', 'n_nearest_features': 6, 'imputation_order': 'random'}. Best is trial 33 with value: 0.2197840027957103.
running
[I 2024-11-15 20:22:24,242] Trial 17 finished with value: 0.22097113827048104 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'mean', 'n_nearest_features': 7, 'imputation_order': 'random'}. Best is trial 33 with value: 0.2197840027957103.
running
[I 2024-11-15 20:50:30,291] Trial 60 finished with value: 0.6298505261042635 and parameters: {'model_name': 'GAIN', 'batch_size': 428, 'hint_rate': 0.02622160210020591, 'alpha': 42, 'iterations': 2896, 'learning_rate': 0.0013969249756925603, 'p_miss': 0.08763117685590358}. Best is trial 33 with value: 0.2197840027957103.
running
[I 2024-11-15 21:33:52,902] Trial 76 finished with value: 0.2201691211088268 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'descending'}. Best is trial 33 with value: 0.2197840027957103.
running
[I 2024-11-15 22:17:28,651] Trial 77 finished with value: 0.21802655353740721 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 22:20:42,010] Trial 65 finished with value: 0.6254563603435438 and parameters: {'model_name': 'GAIN', 'batch_size': 840, 'hint_rate': 0.036005141658594864, 'alpha': 47, 'iterations': 5814, 'learning_rate': 0.0016225470149421478, 'p_miss': 0.014404674217847163}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 22:33:28,083] Trial 75 finished with value: 0.6234716678639787 and parameters: {'model_name': 'GAIN', 'batch_size': 845, 'hint_rate': 0.027210872641729056, 'alpha': 43, 'iterations': 4555, 'learning_rate': 0.00011087784896982385, 'p_miss': 0.011549445523467733}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 22:35:16,132] Trial 63 finished with value: 0.6259240269025772 and parameters: {'model_name': 'GAIN', 'batch_size': 929, 'hint_rate': 0.023120208120801677, 'alpha': 42, 'iterations': 6057, 'learning_rate': 0.0020342057969593187, 'p_miss': 0.07941607166156227}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 22:50:50,748] Trial 71 finished with value: 0.6246855950463217 and parameters: {'model_name': 'GAIN', 'batch_size': 673, 'hint_rate': 0.02706739274969122, 'alpha': 49, 'iterations': 7056, 'learning_rate': 0.0019424982426718706, 'p_miss': 0.020067370182448335}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:00:28,054] Trial 78 finished with value: 0.2212870681069854 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:03:40,876] Trial 79 finished with value: 0.22233584046568824 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:10:52,144] Trial 72 finished with value: 0.623931624712469 and parameters: {'model_name': 'GAIN', 'batch_size': 659, 'hint_rate': 0.03592063597290285, 'alpha': 41, 'iterations': 7119, 'learning_rate': 0.0001129123901221043, 'p_miss': 0.011235891053900326}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:13:44,172] Trial 67 finished with value: 0.6241646717408365 and parameters: {'model_name': 'GAIN', 'batch_size': 601, 'hint_rate': 0.09739045565705778, 'alpha': 47, 'iterations': 8111, 'learning_rate': 0.0023992812582063444, 'p_miss': 0.08654428355005034}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:14:21,569] Trial 73 finished with value: 0.6245568169643129 and parameters: {'model_name': 'GAIN', 'batch_size': 569, 'hint_rate': 0.0831241386402301, 'alpha': 45, 'iterations': 6828, 'learning_rate': 0.0019569811568976255, 'p_miss': 0.025513840712327512}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:16:01,311] Trial 80 finished with value: 0.2211193802258257 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:18:30,380] Trial 81 finished with value: 0.2202657581587415 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:21:40,591] Trial 64 finished with value: 0.6248399207309342 and parameters: {'model_name': 'GAIN', 'batch_size': 695, 'hint_rate': 0.06471644538076704, 'alpha': 45, 'iterations': 7903, 'learning_rate': 0.0017441535268008168, 'p_miss': 0.023269188699829912}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:22:31,051] Trial 61 finished with value: 0.6239826388198253 and parameters: {'model_name': 'GAIN', 'batch_size': 657, 'hint_rate': 0.014352717198808707, 'alpha': 44, 'iterations': 8228, 'learning_rate': 0.001689775481469234, 'p_miss': 0.01822806112955208}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:27:46,975] Trial 69 finished with value: 0.627576441658442 and parameters: {'model_name': 'GAIN', 'batch_size': 778, 'hint_rate': 0.023507751007965705, 'alpha': 43, 'iterations': 8306, 'learning_rate': 0.0021801667527933436, 'p_miss': 0.05394741069819459}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:32:09,900] Trial 70 finished with value: 0.6250003553614895 and parameters: {'model_name': 'GAIN', 'batch_size': 948, 'hint_rate': 0.03905021165699302, 'alpha': 44, 'iterations': 8008, 'learning_rate': 0.0015842130039556124, 'p_miss': 0.016643024590846517}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:32:25,190] Trial 82 finished with value: 0.2203018607410699 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:41:46,860] Trial 83 finished with value: 0.22232448747998554 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:42:41,657] Trial 66 finished with value: 0.624967583073761 and parameters: {'model_name': 'GAIN', 'batch_size': 405, 'hint_rate': 0.05583370423415729, 'alpha': 40, 'iterations': 9409, 'learning_rate': 0.0016587600021870743, 'p_miss': 0.015658193341629323}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:44:59,828] Trial 84 finished with value: 0.2211088827280286 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:47:55,479] Trial 62 finished with value: 0.6219483405373183 and parameters: {'model_name': 'GAIN', 'batch_size': 862, 'hint_rate': 0.04785635331668736, 'alpha': 42, 'iterations': 9202, 'learning_rate': 0.002218018643838854, 'p_miss': 0.08731549681548803}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:49:05,087] Trial 68 finished with value: 0.6258839755789471 and parameters: {'model_name': 'GAIN', 'batch_size': 432, 'hint_rate': 0.011147470400733273, 'alpha': 42, 'iterations': 9810, 'learning_rate': 0.0024167034349291566, 'p_miss': 0.08805457674747094}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:51:43,542] Trial 85 finished with value: 0.22188079060332697 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:53:14,333] Trial 86 finished with value: 0.22183345746106314 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:54:44,752] Trial 87 finished with value: 0.22029903760435166 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:55:28,827] Trial 88 finished with value: 0.21919576729732312 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-15 23:58:34,976] Trial 89 finished with value: 0.22032006404452317 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:01:09,915] Trial 91 finished with value: 0.22074388225860758 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:01:21,447] Trial 90 finished with value: 0.2207145750812904 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:06:07,713] Trial 92 finished with value: 0.22020347612548333 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:10:31,860] Trial 94 finished with value: 0.2203114925847121 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:11:10,309] Trial 93 finished with value: 0.22094892075669265 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:19:32,103] Trial 95 finished with value: 0.22111354046714018 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:20:25,495] Trial 96 finished with value: 0.22044791976955125 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:23:00,392] Trial 97 finished with value: 0.22008017270243743 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:25:32,504] Trial 98 finished with value: 0.22088422986017847 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:27:02,846] Trial 101 finished with value: 0.22090095470033028 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 5, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:27:05,513] Trial 99 finished with value: 0.22123470013949237 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:29:39,064] Trial 103 finished with value: 0.2210405851812039 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 5, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:29:43,475] Trial 102 finished with value: 0.22080074344382344 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 5, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:30:30,245] Trial 100 finished with value: 0.22034896958202146 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 6, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:31:13,239] Trial 74 finished with value: 0.6225591609373679 and parameters: {'model_name': 'GAIN', 'batch_size': 913, 'hint_rate': 0.03460324698750267, 'alpha': 49, 'iterations': 9380, 'learning_rate': 0.00012309128528840824, 'p_miss': 0.02594280459102946}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:34:26,109] Trial 104 finished with value: 0.22058816830554023 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 5, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:36:51,450] Trial 105 finished with value: 0.21878354835569463 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 5, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:37:59,199] Trial 106 finished with value: 0.22032532473116223 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 5, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:42:52,238] Trial 107 finished with value: 0.22179824176945662 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 5, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:48:47,717] Trial 108 finished with value: 0.22107337332934382 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 5, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:50:53,087] Trial 109 finished with value: 0.22070644685147797 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 5, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 00:50:54,062] Trial 125 finished with value: 0.6362487529242203 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'constant'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:00:21,536] Trial 110 finished with value: 0.22029211163234175 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'median', 'n_nearest_features': 5, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:02:03,689] Trial 111 finished with value: 0.21902763774471873 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 5, 'imputation_order': 'roman'}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:02:59,146] Trial 121 finished with value: 0.2661039981227309 and parameters: {'model_name': 'VAE', 'batch_size': 125, 'iterations': 403, 'learning_rate': 0.005929198521101906, 'p_miss': 0.29902220879141683}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:03:26,232] Trial 112 finished with value: 0.26770126450710263 and parameters: {'model_name': 'VAE', 'batch_size': 121, 'iterations': 633, 'learning_rate': 0.0960536003129857, 'p_miss': 0.13512817742233701}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:03:47,010] Trial 129 finished with value: 0.24678855774954372 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending', 'sample_posterior': False}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:13:38,695] Trial 117 finished with value: 0.2661427864013599 and parameters: {'model_name': 'VAE', 'batch_size': 137, 'iterations': 658, 'learning_rate': 0.007403351626320446, 'p_miss': 0.13028383779628053}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:15:10,177] Trial 127 finished with value: 0.26539110713178515 and parameters: {'model_name': 'VAE', 'batch_size': 151, 'iterations': 215, 'learning_rate': 0.005878356026046802, 'p_miss': 0.1255499125656191}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:17:16,258] Trial 115 finished with value: 0.26613597370566044 and parameters: {'model_name': 'VAE', 'batch_size': 127, 'iterations': 615, 'learning_rate': 0.09381080808272026, 'p_miss': 0.29842169789782125}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:17:50,718] Trial 114 finished with value: 0.2672728883518717 and parameters: {'model_name': 'VAE', 'batch_size': 133, 'iterations': 679, 'learning_rate': 0.009535072767185352, 'p_miss': 0.128571893252038}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:20:54,961] Trial 116 finished with value: 0.26658108750421244 and parameters: {'model_name': 'VAE', 'batch_size': 31, 'iterations': 898, 'learning_rate': 0.0966410297904264, 'p_miss': 0.2960380435699586}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:23:08,589] Trial 118 finished with value: 0.26512163308904063 and parameters: {'model_name': 'VAE', 'batch_size': 90, 'iterations': 916, 'learning_rate': 0.08621590279820225, 'p_miss': 0.12538002548901164}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:28:28,301] Trial 119 finished with value: 0.2672820961117862 and parameters: {'model_name': 'VAE', 'batch_size': 174, 'iterations': 896, 'learning_rate': 0.09959124420986994, 'p_miss': 0.12528143691755547}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:29:44,468] Trial 113 finished with value: 0.2678718817060906 and parameters: {'model_name': 'VAE', 'batch_size': 122, 'iterations': 1027, 'learning_rate': 0.006433959005769685, 'p_miss': 0.1348292790970501}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:31:27,709] Trial 123 finished with value: 0.26600970086153763 and parameters: {'model_name': 'VAE', 'batch_size': 137, 'iterations': 821, 'learning_rate': 0.09639504296619718, 'p_miss': 0.13462417721991493}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:31:49,802] Trial 122 finished with value: 0.26517654532048246 and parameters: {'model_name': 'VAE', 'batch_size': 99, 'iterations': 1016, 'learning_rate': 0.0859583726422155, 'p_miss': 0.2981955741986074}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:40:26,342] Trial 120 finished with value: 0.26948469929883495 and parameters: {'model_name': 'VAE', 'batch_size': 27, 'iterations': 1172, 'learning_rate': 0.08839202833184073, 'p_miss': 0.29831732811676803}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:47:35,992] Trial 126 finished with value: 0.2672215684099328 and parameters: {'model_name': 'VAE', 'batch_size': 162, 'iterations': 1057, 'learning_rate': 0.08857425943124544, 'p_miss': 0.12278067811256488}. Best is trial 77 with value: 0.21802655353740721.
running
[I 2024-11-16 01:50:03,946] Trial 130 finished with value: 0.2176905220827901 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 130 with value: 0.2176905220827901.
running
[I 2024-11-16 01:50:28,652] Trial 131 finished with value: 0.2177169810330474 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 130 with value: 0.2176905220827901.
running
[I 2024-11-16 01:50:53,065] Trial 124 finished with value: 0.26802830762460167 and parameters: {'model_name': 'VAE', 'batch_size': 161, 'iterations': 1145, 'learning_rate': 0.00689331516473824, 'p_miss': 0.14048172944494453}. Best is trial 130 with value: 0.2176905220827901.
running
[I 2024-11-16 01:52:23,905] Trial 128 finished with value: 0.26567983027449027 and parameters: {'model_name': 'VAE', 'batch_size': 147, 'iterations': 1186, 'learning_rate': 0.006548907924238049, 'p_miss': 0.1242191794012521}. Best is trial 130 with value: 0.2176905220827901.
running
[I 2024-11-16 01:57:06,914] Trial 132 finished with value: 0.21807731104876274 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 130 with value: 0.2176905220827901.
running
[I 2024-11-16 01:57:36,946] Trial 133 finished with value: 0.21762821752999378 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 133 with value: 0.21762821752999378.
running
[I 2024-11-16 01:58:51,790] Trial 134 finished with value: 0.2236575377939684 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'most_frequent', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 133 with value: 0.21762821752999378.
running
[I 2024-11-16 01:59:09,963] Trial 135 finished with value: 0.21874873801862246 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 133 with value: 0.21762821752999378.
running
[I 2024-11-16 02:01:09,847] Trial 136 finished with value: 0.21837744711539436 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 133 with value: 0.21762821752999378.
running
[I 2024-11-16 02:03:06,052] Trial 137 finished with value: 0.21770886280231405 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 133 with value: 0.21762821752999378.
running
[I 2024-11-16 02:06:41,040] Trial 138 finished with value: 0.2182834276008415 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 133 with value: 0.21762821752999378.
running
[I 2024-11-16 02:07:09,038] Trial 139 finished with value: 0.21934820812832445 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 133 with value: 0.21762821752999378.
running
[I 2024-11-16 02:08:21,783] Trial 140 finished with value: 0.2183324055178854 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 133 with value: 0.21762821752999378.
running
[I 2024-11-16 02:08:54,406] Trial 141 finished with value: 0.2199322783843683 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 133 with value: 0.21762821752999378.
running
[I 2024-11-16 02:14:59,870] Trial 142 finished with value: 0.2187328598190137 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 133 with value: 0.21762821752999378.
running
[I 2024-11-16 02:20:26,517] Trial 143 finished with value: 0.2177961597385058 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 133 with value: 0.21762821752999378.
running
[I 2024-11-16 02:22:08,028] Trial 144 finished with value: 0.21727522410927186 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 144 with value: 0.21727522410927186.
running
[I 2024-11-16 02:22:19,763] Trial 145 finished with value: 0.21880295250487078 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 144 with value: 0.21727522410927186.
running
[I 2024-11-16 02:22:43,283] Trial 146 finished with value: 0.21937514725995594 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 144 with value: 0.21727522410927186.
running
[I 2024-11-16 02:22:43,573] Trial 162 finished with value: 0.2616508015984005 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'mean'}. Best is trial 144 with value: 0.21727522410927186.
running
[I 2024-11-16 02:23:41,103] Trial 163 finished with value: 0.35564667914173764 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending', 'sample_posterior': True}. Best is trial 144 with value: 0.21727522410927186.
running
[I 2024-11-16 02:24:58,887] Trial 147 finished with value: 0.218659711951004 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 144 with value: 0.21727522410927186.
running
[I 2024-11-16 02:25:16,062] Trial 165 finished with value: 0.24698712941745296 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'descending'}. Best is trial 144 with value: 0.21727522410927186.
running
[I 2024-11-16 02:29:54,347] Trial 148 finished with value: 0.21934749898993838 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 144 with value: 0.21727522410927186.
running
[I 2024-11-16 02:30:20,893] Trial 149 finished with value: 0.21919279854269508 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 144 with value: 0.21727522410927186.
running
[I 2024-11-16 02:31:26,995] Trial 150 finished with value: 0.21796662892469526 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 144 with value: 0.21727522410927186.
running
[I 2024-11-16 02:31:43,385] Trial 151 finished with value: 0.21841295976942304 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 144 with value: 0.21727522410927186.
running
[I 2024-11-16 02:33:48,428] Trial 152 finished with value: 0.21701273304216376 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 152 with value: 0.21701273304216376.
running
[I 2024-11-16 02:35:38,003] Trial 153 finished with value: 0.21825004702709766 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 152 with value: 0.21701273304216376.
running
[I 2024-11-16 02:39:09,434] Trial 154 finished with value: 0.2188486984649698 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 152 with value: 0.21701273304216376.
running
[I 2024-11-16 02:39:24,662] Trial 155 finished with value: 0.2173422984901204 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 152 with value: 0.21701273304216376.
running
[I 2024-11-16 02:40:27,814] Trial 156 finished with value: 0.21822715854373181 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 152 with value: 0.21701273304216376.
running
[I 2024-11-16 02:41:08,217] Trial 157 finished with value: 0.21780924406919935 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 152 with value: 0.21701273304216376.
running
[I 2024-11-16 02:46:52,662] Trial 158 finished with value: 0.21754361147126441 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 152 with value: 0.21701273304216376.
running
[I 2024-11-16 02:52:15,786] Trial 159 finished with value: 0.21838047210858907 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 152 with value: 0.21701273304216376.
running
[I 2024-11-16 02:53:49,351] Trial 160 finished with value: 0.21692264174741363 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 02:54:03,339] Trial 161 finished with value: 0.21714426039111956 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 02:56:06,258] Trial 166 finished with value: 0.2178963308139156 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 02:59:05,578] Trial 164 finished with value: 0.2194405468289986 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:00:29,636] Trial 167 finished with value: 0.21704141168345453 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:00:56,360] Trial 183 finished with value: 0.2444807448910054 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:01:03,528] Trial 168 finished with value: 0.2177430185139131 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:02:26,445] Trial 170 finished with value: 0.2180210051548163 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:06:33,360] Trial 169 finished with value: 0.21851048010294388 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:08:50,550] Trial 171 finished with value: 0.218288534287633 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:10:40,728] Trial 172 finished with value: 0.21894169953414497 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:14:11,608] Trial 174 finished with value: 0.21796987132613405 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:14:12,926] Trial 173 finished with value: 0.2177458263341177 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:14:27,109] Trial 191 finished with value: 0.2469871397053282 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'constant', 'n_nearest_features': 9, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:14:27,458] Trial 192 finished with value: 0.29860598694805385 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'most_frequent'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:15:32,366] Trial 175 finished with value: 0.21956169398825326 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:16:14,070] Trial 176 finished with value: 0.21759477209106387 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:22:13,883] Trial 177 finished with value: 0.21794348361720975 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:27:13,041] Trial 178 finished with value: 0.21768833278821234 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:29:04,281] Trial 179 finished with value: 0.2175288960396419 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'descending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:29:08,343] Trial 180 finished with value: 0.218588735444274 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
running
[I 2024-11-16 03:30:59,564] Trial 181 finished with value: 0.217815639132648 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
[I 2024-11-16 03:33:58,734] Trial 182 finished with value: 0.2178493622091131 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
[I 2024-11-16 03:35:33,459] Trial 184 finished with value: 0.21780688588546301 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
[I 2024-11-16 03:35:55,870] Trial 185 finished with value: 0.2184239838946053 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
[I 2024-11-16 03:36:39,064] Trial 187 finished with value: 0.21824792770962329 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
[I 2024-11-16 03:36:51,862] Trial 186 finished with value: 0.21936619784463812 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
[I 2024-11-16 03:38:41,161] Trial 188 finished with value: 0.2195500861314459 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
[I 2024-11-16 03:40:20,370] Trial 189 finished with value: 0.21912917196650933 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
[I 2024-11-16 03:43:16,716] Trial 190 finished with value: 0.21718522640827498 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
[I 2024-11-16 03:44:17,052] Trial 194 finished with value: 0.21800250702415389 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
[I 2024-11-16 03:45:01,738] Trial 195 finished with value: 0.21928289241423782 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
[I 2024-11-16 03:47:22,145] Trial 193 finished with value: 0.21793043735920117 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'ascending'}. Best is trial 160 with value: 0.21692264174741363.
[I 2024-11-16 03:49:58,851] Trial 196 finished with value: 0.2167796158190532 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 196 with value: 0.2167796158190532.
[I 2024-11-16 03:55:39,733] Trial 199 finished with value: 0.21809017596356153 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}. Best is trial 196 with value: 0.2167796158190532.
[I 2024-11-16 03:58:10,780] Trial 197 finished with value: 0.21813654126636384 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 8, 'imputation_order': 'ascending'}. Best is trial 196 with value: 0.2167796158190532.
[I 2024-11-16 03:59:46,253] Trial 198 finished with value: 0.21778885237833162 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 9, 'imputation_order': 'ascending'}. Best is trial 196 with value: 0.2167796158190532.
fit
auto fit
auto transform
0    0
1    0
2    0
3    0
4    0
5    0
6    0
7    0
8    0
dtype: int64
0    0
1    0
2    0
3    0
4    0
5    0
6    0
7    0
8    0
dtype: int64
0.2167796158190532
{'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 7, 'imputation_order': 'ascending'}
running experiment 2/3 - Does reconstruction give good automl predictions
Start est fit
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
  0%|          | 0/25 [00:00<?, ?it/s]Generation:   0%|          | 0/25 [00:00<?, ?it/s]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554746fe560> 

Generation:  1
Best f1_score score: 0.6553738644830431
Generation:   4%|▍         | 1/25 [10:03<4:01:12, 603.03s/it]Generation:  2
Best f1_score score: 0.664842005684017
Generation:   8%|▊         | 2/25 [10:33<1:41:57, 265.98s/it]Generation:  3
Best f1_score score: 0.664842005684017
Generation:  12%|█▏        | 3/25 [11:02<57:55, 157.96s/it]  WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155467f096c0> 
 The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead. 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1466, in wrapper
    estimator._validate_params()
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 666, in _validate_params
    validate_parameter_constraints(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 95, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.

Generation:  4
Best f1_score score: 0.664842005684017
Generation:  16%|█▌        | 4/25 [11:31<37:28, 107.06s/it]Generation:  5
Best f1_score score: 0.664842005684017
Generation:  20%|██        | 5/25 [14:38<45:17, 135.86s/it]Generation:  6
Best f1_score score: 0.664842005684017
Generation:  24%|██▍       | 6/25 [15:16<32:26, 102.47s/it]Generation:  7
Best f1_score score: 0.664842005684017
Generation:  28%|██▊       | 7/25 [16:02<25:15, 84.22s/it] Generation:  8
Best f1_score score: 0.664842005684017
Generation:  32%|███▏      | 8/25 [16:32<18:56, 66.88s/it]Generation:  9
Best f1_score score: 0.664842005684017
Generation:  36%|███▌      | 9/25 [16:56<14:18, 53.63s/it]Generation:  10
Best f1_score score: 0.664842005684017
Generation:  40%|████      | 10/25 [17:36<12:19, 49.28s/it]Generation:  11
Best f1_score score: 0.664842005684017
Generation:  44%|████▍     | 11/25 [18:09<10:18, 44.21s/it]Generation:  12
Best f1_score score: 0.664842005684017
Generation:  48%|████▊     | 12/25 [18:56<09:45, 45.06s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155467fc9210> 
 The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead. 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1466, in wrapper
    estimator._validate_params()
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 666, in _validate_params
    validate_parameter_constraints(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 95, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.

Generation:  13
Best f1_score score: 0.664842005684017
Generation:  52%|█████▏    | 13/25 [22:14<18:18, 91.51s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474715150> 

Generation:  14
Best f1_score score: 0.664842005684017
Generation:  56%|█████▌    | 14/25 [32:24<45:27, 247.98s/it]Generation:  15
Best f1_score score: 0.664842005684017
Generation:  60%|██████    | 15/25 [33:13<31:21, 188.14s/it]Generation:  16
Best f1_score score: 0.664842005684017
Generation:  64%|██████▍   | 16/25 [33:30<20:28, 136.46s/it]Generation:  17
Best f1_score score: 0.664842005684017
Generation:  68%|██████▊   | 17/25 [34:19<14:43, 110.40s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155467d100d0> 

Generation:  18
Best f1_score score: 0.664842005684017
Generation:  72%|███████▏  | 18/25 [44:30<30:25, 260.81s/it]Generation:  19
Best f1_score score: 0.664842005684017
Generation:  76%|███████▌  | 19/25 [45:20<19:44, 197.50s/it]Generation:  20
Best f1_score score: 0.664842005684017
Generation:  80%|████████  | 20/25 [45:48<12:11, 146.39s/it]Generation:  21
Best f1_score score: 0.664842005684017
Generation:  84%|████████▍ | 21/25 [46:23<07:31, 112.96s/it]Generation:  22
Best f1_score score: 0.664842005684017
Generation:  88%|████████▊ | 22/25 [47:09<04:38, 92.96s/it] Generation:  23
Best f1_score score: 0.664842005684017
Generation:  92%|█████████▏| 23/25 [49:03<03:18, 99.44s/it]Generation:  24
Best f1_score score: 0.664842005684017
Generation:  96%|█████████▌| 24/25 [49:33<01:18, 78.39s/it]Generation:  25
Best f1_score score: 0.664842005684017
Generation: 100%|██████████| 25/25 [50:20<00:00, 68.92s/it]Generation: 100%|██████████| 25/25 [50:23<00:00, 120.93s/it]
2024-11-16 04:55:16,031 - distributed.scheduler - ERROR - Removing worker 'tcp://127.0.0.1:40719' caused the cluster to lose scattered data, which can't be recovered: {'ndarray-5834cf524820a0a6041715632e69b001', 'ndarray-b7e4639ccb3c3412fc3ae1fd38edaaae'} (stimulus_id='handle-worker-cleanup-1731761716.0311263')
Fitted
Pipeline(steps=[('randomforestclassifier',
                 RandomForestClassifier(bootstrap=False, criterion='entropy',
                                        max_features=0.5927775006315,
                                        n_estimators=128))])
score start
train score: {'auroc': 0.9906303529208793, 'accuracy': 0.975548350181194, 'balanced_accuracy': 0.8667690485095697, 'logloss': 0.0519573732719604, 'f1': 0.8829739409611796}
original test score: {'auroc': 0.7637424289361521, 'accuracy': 0.9391211473909064, 'balanced_accuracy': 0.6395455759349663, 'logloss': 0.7676615821292552, 'f1': 0.6658338856880641}
imputed test score: {'auroc': 0.761951865659684, 'accuracy': 0.9366798901434239, 'balanced_accuracy': 0.6246288442844476, 'logloss': 0.7282918133865787, 'f1': 0.6492442317143484}
score end
check intended
EXP2 Finished
running experiment 3/3 - What is the best automl settings?
<tpot2.search_spaces.pipelines.sequential.SequentialPipeline object at 0x155435014580>
Start tpot fit
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
  0%|          | 0/25 [00:00<?, ?it/s]Generation:   0%|          | 0/25 [00:00<?, ?it/s]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474d27e20> 
 The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead. 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1466, in wrapper
    estimator._validate_params()
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 666, in _validate_params
    validate_parameter_constraints(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 95, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.

Generation:  1
Best f1_score score: 0.5877797921823437
Generation:   4%|▍         | 1/25 [02:01<48:46, 121.96s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474cd5750> 
 Input X contains NaN.
GaussianNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 263, in fit
    return self._partial_fit(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 423, in _partial_fit
    X, y = self._validate_data(X, y, reset=first_call)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
GaussianNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474cd6200> 

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15547459f730> 

Generation:  2
Best f1_score score: 0.6260725447523221
Generation:   8%|▊         | 2/25 [12:06<2:35:29, 405.64s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474d5d420> 

Generation:  3
Best f1_score score: 0.6291992961633828
Generation:  12%|█▏        | 3/25 [22:12<3:02:16, 497.10s/it]Generation:  4
Best f1_score score: 0.6300426837407744
Generation:  16%|█▌        | 4/25 [24:14<2:02:13, 349.23s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15541016f970> 

Generation:  5
Best f1_score score: 0.6300426837407744
Generation:  20%|██        | 5/25 [34:20<2:27:14, 441.71s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474c83e80> 

Generation:  6
Best f1_score score: 0.6325181515512701
Generation:  24%|██▍       | 6/25 [44:25<2:37:30, 497.41s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545ff8a7a0> 
 Input X contains NaN.
ExtraTreesClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_forest.py", line 377, in fit
    estimator._compute_missing_values_in_feature_mask(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/tree/_classes.py", line 214, in _compute_missing_values_in_feature_mask
    assert_all_finite(X, **common_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 213, in assert_all_finite
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
ExtraTreesClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  7
Best f1_score score: 0.6325181515512701
Generation:  28%|██▊       | 7/25 [47:04<1:55:59, 386.66s/it]Generation:  8
Best f1_score score: 0.6325181515512701
Generation:  32%|███▏      | 8/25 [49:42<1:28:54, 313.78s/it]Generation:  9
Best f1_score score: 0.6325181515512701
Generation:  36%|███▌      | 9/25 [58:43<1:42:37, 384.83s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155455bc1a80> 

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545f949f00> 

Generation:  10
Best f1_score score: 0.6325181515512701
Generation:  40%|████      | 10/25 [1:08:48<1:53:11, 452.78s/it]Generation:  11
Best f1_score score: 0.6330050958282556
Generation:  44%|████▍     | 11/25 [1:12:05<1:27:23, 374.52s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545a2d07f0> 

Generation:  12
Best f1_score score: 0.6356938594727513
Generation:  48%|████▊     | 12/25 [1:22:10<1:36:20, 444.69s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1553d7d95960> 

Generation:  13
Best f1_score score: 0.6356938594727513
Generation:  52%|█████▏    | 13/25 [1:32:14<1:38:35, 492.96s/it]Generation:  14
Best f1_score score: 0.637905102472853
Generation:  56%|█████▌    | 14/25 [1:35:31<1:13:59, 403.57s/it]Generation:  15
Best f1_score score: 0.6417454912178562
Generation:  60%|██████    | 15/25 [1:38:48<56:51, 341.15s/it]  Generation:  16
Best f1_score score: 0.6417454912178562
Generation:  64%|██████▍   | 16/25 [1:42:16<45:10, 301.16s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554523d3070> 
 Input X contains NaN.
ExtraTreesClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_forest.py", line 377, in fit
    estimator._compute_missing_values_in_feature_mask(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/tree/_classes.py", line 214, in _compute_missing_values_in_feature_mask
    assert_all_finite(X, **common_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 213, in assert_all_finite
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
ExtraTreesClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545b5abeb0> 

Generation:  17
Best f1_score score: 0.6417454912178562
Generation:  68%|██████▊   | 17/25 [1:52:21<52:19, 392.46s/it]Generation:  18
Best f1_score score: 0.6430099410641861
Generation:  72%|███████▏  | 18/25 [1:55:14<38:06, 326.61s/it]Generation:  19
Best f1_score score: 0.6484290392074118
Generation:  76%|███████▌  | 19/25 [1:58:27<28:38, 286.49s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554423dd0c0> 

Generation:  20
Best f1_score score: 0.6484290392074118
Generation:  80%|████████  | 20/25 [2:08:31<31:48, 381.75s/it]Generation:  21
Best f1_score score: 0.6484290392074118
Generation:  84%|████████▍ | 21/25 [2:11:44<21:40, 325.22s/it]WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545664c850> 

Generation:  22
Best f1_score score: 0.6484290392074118
Generation:  88%|████████▊ | 22/25 [2:21:49<20:27, 409.06s/it]Generation:  23
Best f1_score score: 0.6484290392074118
Generation:  92%|█████████▏| 23/25 [2:25:02<11:28, 344.44s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15547507fdf0> 
 Input X contains NaN.
ExtraTreesClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_forest.py", line 377, in fit
    estimator._compute_missing_values_in_feature_mask(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/tree/_classes.py", line 214, in _compute_missing_values_in_feature_mask
    assert_all_finite(X, **common_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 213, in assert_all_finite
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
ExtraTreesClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554745e7dc0> 

Generation:  24
Best f1_score score: 0.6484290392074118
Generation:  96%|█████████▌| 24/25 [2:35:09<07:03, 423.04s/it]Generation:  25
Best f1_score score: 0.6484290392074118
Generation: 100%|██████████| 25/25 [2:38:23<00:00, 354.27s/it]Generation: 100%|██████████| 25/25 [2:38:23<00:00, 380.13s/it]
2024-11-16 07:34:19,335 - distributed.scheduler - ERROR - Removing worker 'tcp://127.0.0.1:45571' caused the cluster to lose scattered data, which can't be recovered: {'ndarray-b7e4639ccb3c3412fc3ae1fd38edaaae', 'DataFrame-e81f56b284a86b1d2aede5f96318c6c3'} (stimulus_id='handle-worker-cleanup-1731771259.3353045')
Fitted
Pipeline(steps=[('knnimputer', KNNImputer(n_neighbors=77, weights='distance')),
                ('extratreesclassifier',
                 ExtraTreesClassifier(bootstrap=True, class_weight='balanced',
                                      max_features=0.5119934841301,
                                      min_samples_split=9, n_jobs=1))])
transform worked
try transform
transform worked
score start
train score: {'auroc': 0.9780485465395267, 'accuracy': 0.9448025939347702, 'balanced_accuracy': 0.9336086970925562, 'logloss': 0.16821364836438496, 'f1': 0.8144694327852807}
test score: {'auroc': 0.7852449980237787, 'accuracy': 0.9218797680805615, 'balanced_accuracy': 0.6291574888103149, 'logloss': 0.2788414272400529, 'f1': 0.632421300769192}
original test score: {'auroc': 0.8078296817749671, 'accuracy': 0.9177601464754348, 'balanced_accuracy': 0.6455455972995203, 'logloss': 0.27287255545172834, 'f1': 0.6391188532649698}
score end
4135
lvl
0.3
type
MAR
num_run
1
class_full
finished
all finished
full run takes
12.105364359484778
hours
DONE
