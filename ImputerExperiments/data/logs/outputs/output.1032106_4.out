Run: 4
/cm/local/apps/slurm/var/spool/job1032106/slurm_script: line 26: 
pip install -e tpot2
pip install -r tpot2/ImputerExperiments/requirements_.txt
: No such file or directory
RunStart
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
starting loops
../data/c/871/871.pkl
working on 
../data/c/871/class_full_MCAR_0.5_1
0.21633219718933105
running experiment 1/3 - Does large hyperparameter space improve reconstruction accuracy over simple
[I 2024-10-30 00:46:27,899] A new study created in memory with name: no-name-e9f3db0e-9b4b-4856-b41a-a2abdf9846df
running
running
running
running
running
running
running
running
running
running
running
running
running
running
running
running
[I 2024-10-30 00:46:28,117] Trial 2 finished with value: 0.32225132611977736 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'median'}. Best is trial 2 with value: 0.32225132611977736.
running
[I 2024-10-30 00:46:28,297] Trial 10 finished with value: 0.1991022136428366 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'mean'}. Best is trial 10 with value: 0.1991022136428366.
running
[I 2024-10-30 00:46:28,777] Trial 9 finished with value: 0.203307298042527 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 44, 'weights': 'uniform'}. Best is trial 10 with value: 0.1991022136428366.
running
[I 2024-10-30 00:46:29,002] Trial 13 finished with value: 0.25058822568603967 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 1580, 'weights': 'distance'}. Best is trial 10 with value: 0.1991022136428366.
running
[I 2024-10-30 00:46:30,117] Trial 19 finished with value: 0.19859989364168382 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'most_frequent', 'n_nearest_features': 4, 'imputation_order': 'descending'}. Best is trial 19 with value: 0.19859989364168382.
running
[I 2024-10-30 00:46:35,255] Trial 8 finished with value: 0.32527007813724673 and parameters: {'model_name': 'GAIN', 'batch_size': 7, 'hint_rate': 0.39330254277561094, 'alpha': 83, 'iterations': 1, 'learning_rate': 0.005318766170336597, 'p_miss': 0.25066152164769545}. Best is trial 19 with value: 0.19859989364168382.
running
[I 2024-10-30 00:46:35,516] Trial 1 finished with value: 0.22688925177386765 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'median', 'n_nearest_features': 1, 'imputation_order': 'arabic'}. Best is trial 19 with value: 0.19859989364168382.
running
[I 2024-10-30 00:46:35,793] Trial 5 finished with value: 0.3430439400138493 and parameters: {'model_name': 'GAIN', 'batch_size': 468, 'hint_rate': 0.08096365089581405, 'alpha': 1, 'iterations': 1, 'learning_rate': 0.06321017870944146, 'p_miss': 0.14824458911154262}. Best is trial 19 with value: 0.19859989364168382.
running
[I 2024-10-30 00:46:36,123] Trial 22 finished with value: 0.32225132611977736 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'median'}. Best is trial 19 with value: 0.19859989364168382.
running
[I 2024-10-30 00:46:36,340] Trial 0 finished with value: 0.22697485626646757 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'mean', 'n_nearest_features': 1, 'imputation_order': 'arabic'}. Best is trial 19 with value: 0.19859989364168382.
running
[I 2024-10-30 00:46:37,477] Trial 11 finished with value: 0.32848288374289986 and parameters: {'model_name': 'GAIN', 'batch_size': 21, 'hint_rate': 0.9413558545586356, 'alpha': 29, 'iterations': 2, 'learning_rate': 0.0008630231522388043, 'p_miss': 0.23621633077664161}. Best is trial 19 with value: 0.19859989364168382.
running
[I 2024-10-30 00:46:38,880] Trial 16 finished with value: 0.11732861908489593 and parameters: {'model_name': 'VAE', 'batch_size': 4, 'iterations': 1, 'learning_rate': 0.06663042004092418, 'p_miss': 0.2438431842104901}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:46:41,078] Trial 18 finished with value: 0.2771533348819043 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'constant', 'n_nearest_features': 2, 'imputation_order': 'ascending', 'sample_posterior': True}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:46:45,431] Trial 20 finished with value: 0.28254200512941335 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'median', 'n_nearest_features': 3, 'imputation_order': 'random', 'sample_posterior': True}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:46:47,407] Trial 21 finished with value: 0.2093902215831022 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'mean', 'n_nearest_features': 3, 'imputation_order': 'descending'}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:46:47,840] Trial 23 finished with value: 0.22095037964147748 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'KNN', 'initial_strategy': 'median', 'n_nearest_features': 1, 'imputation_order': 'ascending'}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:47:02,918] Trial 17 finished with value: 0.16308864032619214 and parameters: {'model_name': 'VAE', 'batch_size': 660, 'iterations': 5, 'learning_rate': 0.02442665438087198, 'p_miss': 0.22889691740335855}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:48:05,292] Trial 14 finished with value: 0.3486360431653968 and parameters: {'model_name': 'GAIN', 'batch_size': 403, 'hint_rate': 0.05814659145116584, 'alpha': 44, 'iterations': 51, 'learning_rate': 0.0002956707513385968, 'p_miss': 0.06245778904209243}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:50:12,161] Trial 33 finished with value: 0.20942913643812244 and parameters: {'model_name': 'VAE', 'batch_size': 1, 'iterations': 38, 'learning_rate': 0.06352568860198751, 'p_miss': 0.29764397252773067}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:50:26,417] Trial 6 finished with value: 0.20807511239185197 and parameters: {'model_name': 'VAE', 'batch_size': 1, 'iterations': 87, 'learning_rate': 0.015563251185932577, 'p_miss': 0.17024221834697664}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:52:14,173] Trial 32 finished with value: 0.22956402041598434 and parameters: {'model_name': 'VAE', 'batch_size': 1, 'iterations': 99, 'learning_rate': 0.0745351678229215, 'p_miss': 0.29599492146254425}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:57:00,030] Trial 15 finished with value: 0.21178453724514962 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'mean', 'n_nearest_features': 4, 'imputation_order': 'ascending'}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:57:29,170] Trial 37 finished with value: 0.14236294277543834 and parameters: {'model_name': 'VAE', 'batch_size': 103, 'iterations': 6, 'learning_rate': 0.011848465291281677, 'p_miss': 0.1960658671666921}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:57:54,234] Trial 3 finished with value: inf and parameters: {'model_name': 'GAIN', 'batch_size': 7, 'hint_rate': 0.8294549386224614, 'alpha': 44, 'iterations': 2320, 'learning_rate': 0.00012195425662006235, 'p_miss': 0.20178539248530353}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:58:02,427] Trial 38 finished with value: 0.15972375565268304 and parameters: {'model_name': 'VAE', 'batch_size': 107, 'iterations': 7, 'learning_rate': 0.01178976324237667, 'p_miss': 0.19699966922481055}. Best is trial 16 with value: 0.11732861908489593.
running
[I 2024-10-30 00:58:33,133] Trial 40 finished with value: 0.11574594590765667 and parameters: {'model_name': 'VAE', 'batch_size': 92, 'iterations': 10, 'learning_rate': 0.0062947525370270485, 'p_miss': 0.13399476997487447}. Best is trial 40 with value: 0.11574594590765667.
running
[I 2024-10-30 00:58:43,750] Trial 39 finished with value: 0.19239177523292955 and parameters: {'model_name': 'VAE', 'batch_size': 116, 'iterations': 11, 'learning_rate': 0.013685458961444094, 'p_miss': 0.20768580202268302}. Best is trial 40 with value: 0.11574594590765667.
running
[I 2024-10-30 00:59:15,603] Trial 41 finished with value: 0.10721534147269873 and parameters: {'model_name': 'VAE', 'batch_size': 51, 'iterations': 12, 'learning_rate': 0.002531563792213041, 'p_miss': 0.11805395608389142}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 00:59:28,973] Trial 42 finished with value: 0.10836018377916026 and parameters: {'model_name': 'VAE', 'batch_size': 71, 'iterations': 13, 'learning_rate': 0.002286119417099119, 'p_miss': 0.09916517720003075}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:00:55,088] Trial 44 finished with value: 0.1115458424318114 and parameters: {'model_name': 'VAE', 'batch_size': 36, 'iterations': 24, 'learning_rate': 0.0020205042503303045, 'p_miss': 0.10063788478983898}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:13:50,227] Trial 43 finished with value: 0.20093287588425057 and parameters: {'model_name': 'VAE', 'batch_size': 26, 'iterations': 347, 'learning_rate': 0.0021770648663900685, 'p_miss': 0.11032338667148317}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:14:50,529] Trial 46 finished with value: 0.109402802136308 and parameters: {'model_name': 'VAE', 'batch_size': 37, 'iterations': 18, 'learning_rate': 0.0022368290863716517, 'p_miss': 0.10139921319376258}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:16:10,383] Trial 47 finished with value: 0.10962312321475469 and parameters: {'model_name': 'VAE', 'batch_size': 35, 'iterations': 24, 'learning_rate': 0.001942560734906842, 'p_miss': 0.08307691553191912}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:17:26,907] Trial 48 finished with value: 0.10779639381638739 and parameters: {'model_name': 'VAE', 'batch_size': 44, 'iterations': 22, 'learning_rate': 0.0010007649149747168, 'p_miss': 0.06181615732335162}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:21:43,865] Trial 45 finished with value: 0.20059294837255354 and parameters: {'model_name': 'VAE', 'batch_size': 35, 'iterations': 390, 'learning_rate': 0.0016743784129045684, 'p_miss': 0.0855943208279237}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:21:45,033] Trial 50 finished with value: 0.25086165136977195 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 2769, 'weights': 'distance'}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:23:03,547] Trial 51 finished with value: 0.10934012998768824 and parameters: {'model_name': 'VAE', 'batch_size': 63, 'iterations': 21, 'learning_rate': 0.0007986637685629523, 'p_miss': 0.04409137778575328}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:23:04,134] Trial 52 finished with value: 0.36598537688813837 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'most_frequent'}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:23:17,722] Trial 53 finished with value: 0.11098460356837883 and parameters: {'model_name': 'VAE', 'batch_size': 230, 'iterations': 3, 'learning_rate': 0.0007118121810587716, 'p_miss': 0.011224094885206481}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:30:14,165] Trial 54 finished with value: 0.1322007534396316 and parameters: {'model_name': 'VAE', 'batch_size': 14, 'iterations': 126, 'learning_rate': 0.0007772338029577892, 'p_miss': 0.03828602493065221}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:30:14,995] Trial 55 finished with value: 0.20303328720965436 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 45, 'weights': 'uniform'}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:31:30,484] Trial 56 finished with value: 0.13069277010599056 and parameters: {'model_name': 'VAE', 'batch_size': 56, 'iterations': 19, 'learning_rate': 0.0042406728075376566, 'p_miss': 0.05593769991856154}. Best is trial 41 with value: 0.10721534147269873.
running
[I 2024-10-30 01:32:23,591] Trial 57 finished with value: 0.1064615866123958 and parameters: {'model_name': 'VAE', 'batch_size': 70, 'iterations': 16, 'learning_rate': 0.0009991929936111821, 'p_miss': 0.12570265625224852}. Best is trial 57 with value: 0.1064615866123958.
running
[I 2024-10-30 01:33:11,827] Trial 58 finished with value: 0.1074088807969235 and parameters: {'model_name': 'VAE', 'batch_size': 64, 'iterations': 14, 'learning_rate': 0.0010703131350291363, 'p_miss': 0.12696803535842788}. Best is trial 57 with value: 0.1064615866123958.
running
[I 2024-10-30 01:33:26,716] Trial 59 finished with value: 0.10341325365723315 and parameters: {'model_name': 'VAE', 'batch_size': 193, 'iterations': 3, 'learning_rate': 0.0004209625113438305, 'p_miss': 0.134090281990192}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:33:33,212] Trial 60 finished with value: 0.3365625197676928 and parameters: {'model_name': 'GAIN', 'batch_size': 187, 'hint_rate': 0.5447850846175446, 'alpha': 89, 'iterations': 3, 'learning_rate': 0.000480013034116738, 'p_miss': 0.1303330223961465}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:33:48,888] Trial 61 finished with value: 0.10641717303644364 and parameters: {'model_name': 'VAE', 'batch_size': 198, 'iterations': 3, 'learning_rate': 0.0003058839790958538, 'p_miss': 0.16134266591682267}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:33:49,304] Trial 62 finished with value: 0.36598537688813837 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'constant'}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:33:56,295] Trial 63 finished with value: 0.10443404769418145 and parameters: {'model_name': 'VAE', 'batch_size': 210, 'iterations': 2, 'learning_rate': 0.00017158120600123397, 'p_miss': 0.12713010512936448}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:33:57,633] Trial 64 finished with value: 0.1991022136428366 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 3064, 'weights': 'uniform'}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:34:02,215] Trial 65 finished with value: 0.3381956517849876 and parameters: {'model_name': 'GAIN', 'batch_size': 242, 'hint_rate': 0.5942029314407525, 'alpha': 7, 'iterations': 2, 'learning_rate': 0.00011651507299367482, 'p_miss': 0.1519358518466707}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:34:15,990] Trial 66 finished with value: 0.11967859327484436 and parameters: {'model_name': 'VAE', 'batch_size': 160, 'iterations': 3, 'learning_rate': 0.00024408415820949516, 'p_miss': 0.12274477164975331}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:34:39,936] Trial 67 finished with value: 0.11553964886206161 and parameters: {'model_name': 'VAE', 'batch_size': 327, 'iterations': 5, 'learning_rate': 0.00018169543620127565, 'p_miss': 0.14586752786724108}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:34:49,797] Trial 68 finished with value: 0.10944868861002 and parameters: {'model_name': 'VAE', 'batch_size': 634, 'iterations': 2, 'learning_rate': 0.0004148206275798692, 'p_miss': 0.16450765832499079}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:34:55,370] Trial 69 finished with value: 0.11543947714442777 and parameters: {'model_name': 'VAE', 'batch_size': 980, 'iterations': 1, 'learning_rate': 0.0001859510561213464, 'p_miss': 0.16709195807277497}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:35:34,913] Trial 70 finished with value: 0.11313850286930671 and parameters: {'model_name': 'VAE', 'batch_size': 156, 'iterations': 9, 'learning_rate': 0.00046332091674834746, 'p_miss': 0.11954486976995687}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:36:12,415] Trial 24 finished with value: 0.20127787110891743 and parameters: {'model_name': 'VAE', 'batch_size': 31, 'iterations': 980, 'learning_rate': 0.00769872741817132, 'p_miss': 0.01664685881756315}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:36:31,191] Trial 72 finished with value: 0.10804368046136936 and parameters: {'model_name': 'VAE', 'batch_size': 338, 'iterations': 4, 'learning_rate': 0.0012123414508346025, 'p_miss': 0.14143342144627083}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:36:31,687] Trial 73 finished with value: 0.36598537688813837 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'most_frequent'}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:36:36,911] Trial 74 finished with value: 0.11415357881780948 and parameters: {'model_name': 'VAE', 'batch_size': 80, 'iterations': 1, 'learning_rate': 0.00030737593834642403, 'p_miss': 0.15693910020206062}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:37:52,066] Trial 75 finished with value: 0.33519459223808223 and parameters: {'model_name': 'GAIN', 'batch_size': 11, 'hint_rate': 0.2965173629878387, 'alpha': 70, 'iterations': 47, 'learning_rate': 0.0001638642634944045, 'p_miss': 0.1741366188671958}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:38:02,415] Trial 76 finished with value: 0.10663838669148178 and parameters: {'model_name': 'VAE', 'batch_size': 56, 'iterations': 2, 'learning_rate': 0.00115629619292727, 'p_miss': 0.18224378650504083}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:38:03,863] Trial 49 finished with value: 0.20350123133325462 and parameters: {'model_name': 'VAE', 'batch_size': 51, 'iterations': 395, 'learning_rate': 0.0008491262139344182, 'p_miss': 0.02359740566882986}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:38:09,094] Trial 78 finished with value: 0.10379235383506595 and parameters: {'model_name': 'VAE', 'batch_size': 140, 'iterations': 1, 'learning_rate': 0.0005074747709957962, 'p_miss': 0.18622810598393408}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:38:14,331] Trial 77 finished with value: 0.10551146155491813 and parameters: {'model_name': 'VAE', 'batch_size': 138, 'iterations': 2, 'learning_rate': 0.0005583099851697769, 'p_miss': 0.11527630808093767}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:38:20,325] Trial 79 finished with value: 0.115342802367009 and parameters: {'model_name': 'VAE', 'batch_size': 139, 'iterations': 2, 'learning_rate': 0.0005403239571564615, 'p_miss': 0.17629761007819658}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:38:22,395] Trial 80 finished with value: 0.111356664295807 and parameters: {'model_name': 'VAE', 'batch_size': 147, 'iterations': 2, 'learning_rate': 0.0005809203829383252, 'p_miss': 0.1855762355103815}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:38:23,146] Trial 81 finished with value: 0.19859989120471613 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'most_frequent', 'n_nearest_features': 5, 'imputation_order': 'roman'}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:38:25,714] Trial 82 finished with value: 0.19859989120471613 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Ridge', 'initial_strategy': 'most_frequent', 'n_nearest_features': 5, 'imputation_order': 'roman'}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:38:29,055] Trial 83 finished with value: 0.10373648107491378 and parameters: {'model_name': 'VAE', 'batch_size': 477, 'iterations': 1, 'learning_rate': 0.0003378011580481155, 'p_miss': 0.18696942108990736}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 01:38:35,451] Trial 84 finished with value: 0.10837963483143978 and parameters: {'model_name': 'VAE', 'batch_size': 238, 'iterations': 1, 'learning_rate': 0.00028875802612163157, 'p_miss': 0.21710243961212694}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 02:33:47,203] Trial 4 finished with value: 0.34508842618827235 and parameters: {'model_name': 'GAIN', 'batch_size': 151, 'hint_rate': 0.9660399401721214, 'alpha': 35, 'iterations': 4213, 'learning_rate': 0.004004566175630263, 'p_miss': 0.0874929831184801}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 02:33:52,410] Trial 87 finished with value: 0.10972784338902766 and parameters: {'model_name': 'VAE', 'batch_size': 419, 'iterations': 1, 'learning_rate': 0.0003049828472314173, 'p_miss': 0.18756663664582124}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 02:34:11,882] Trial 88 finished with value: 0.10655315315591593 and parameters: {'model_name': 'VAE', 'batch_size': 576, 'iterations': 4, 'learning_rate': 0.0003706045548037072, 'p_miss': 0.13814293131339317}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 02:34:30,845] Trial 89 finished with value: 0.10649794764372564 and parameters: {'model_name': 'VAE', 'batch_size': 826, 'iterations': 4, 'learning_rate': 0.0003587455145836859, 'p_miss': 0.13723567279995644}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 03:45:04,632] Trial 34 finished with value: 0.1889693235027127 and parameters: {'model_name': 'VAE', 'batch_size': 2, 'iterations': 3966, 'learning_rate': 0.012347941299973467, 'p_miss': 0.18486916546129012}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 03:45:19,183] Trial 91 finished with value: 0.11334965216164483 and parameters: {'model_name': 'VAE', 'batch_size': 970, 'iterations': 3, 'learning_rate': 0.0001012387912521364, 'p_miss': 0.1566122438910686}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 03:45:37,752] Trial 92 finished with value: 0.10348384882211395 and parameters: {'model_name': 'VAE', 'batch_size': 289, 'iterations': 5, 'learning_rate': 0.00023384358476768305, 'p_miss': 0.11038182351977038}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 03:45:39,311] Trial 93 finished with value: 0.250695946460642 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 1833, 'weights': 'distance'}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 03:46:02,722] Trial 94 finished with value: 0.10646350049564424 and parameters: {'model_name': 'VAE', 'batch_size': 280, 'iterations': 7, 'learning_rate': 0.00022696454307708636, 'p_miss': 0.10909933898830651}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 03:46:08,834] Trial 95 finished with value: 0.11188377960586512 and parameters: {'model_name': 'VAE', 'batch_size': 447, 'iterations': 1, 'learning_rate': 0.0001449696812817824, 'p_miss': 0.075945276270848}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 03:46:36,506] Trial 96 finished with value: 0.10689313831063632 and parameters: {'model_name': 'VAE', 'batch_size': 306, 'iterations': 7, 'learning_rate': 0.0002605750730160362, 'p_miss': 0.10719801474011728}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 03:47:00,589] Trial 97 finished with value: 0.11706709497320031 and parameters: {'model_name': 'VAE', 'batch_size': 202, 'iterations': 7, 'learning_rate': 0.0002259472719341793, 'p_miss': 0.11442251294697861}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 03:47:07,717] Trial 98 finished with value: 0.11641106157838299 and parameters: {'model_name': 'VAE', 'batch_size': 112, 'iterations': 2, 'learning_rate': 0.00021176058562752102, 'p_miss': 0.09355536970332082}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 03:47:42,095] Trial 99 finished with value: 0.11261404345253478 and parameters: {'model_name': 'VAE', 'batch_size': 298, 'iterations': 6, 'learning_rate': 0.0006282700297528473, 'p_miss': 0.2654040018517035}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 03:48:05,179] Trial 100 finished with value: 0.2078927894282317 and parameters: {'model_name': 'VAE', 'batch_size': 505, 'iterations': 3, 'learning_rate': 0.04304240618273998, 'p_miss': 0.20535675176919496}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 03:48:05,482] Trial 101 finished with value: 0.36598537688813837 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'constant'}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 04:39:55,273] Trial 7 finished with value: 0.20473520451196553 and parameters: {'model_name': 'VAE', 'batch_size': 181, 'iterations': 4520, 'learning_rate': 0.0019532188015426283, 'p_miss': 0.2324110368481137}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 04:40:07,710] Trial 103 finished with value: 0.11177618407277956 and parameters: {'model_name': 'VAE', 'batch_size': 121, 'iterations': 4, 'learning_rate': 0.0003496965267643753, 'p_miss': 0.10976252255470163}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 04:40:39,221] Trial 104 finished with value: 0.11245349590691074 and parameters: {'model_name': 'VAE', 'batch_size': 250, 'iterations': 8, 'learning_rate': 0.00045526826096466735, 'p_miss': 0.12936135337592225}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 04:40:47,468] Trial 105 finished with value: 0.10774845247411262 and parameters: {'model_name': 'VAE', 'batch_size': 86, 'iterations': 2, 'learning_rate': 0.00014483081392197965, 'p_miss': 0.0775661082300308}. Best is trial 59 with value: 0.10341325365723315.
running
[I 2024-10-30 04:41:14,824] Trial 106 finished with value: 0.10126171639455535 and parameters: {'model_name': 'VAE', 'batch_size': 784, 'iterations': 5, 'learning_rate': 0.0003918393864172943, 'p_miss': 0.14593801975426235}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 04:41:34,348] Trial 107 finished with value: 0.10502707580571898 and parameters: {'model_name': 'VAE', 'batch_size': 180, 'iterations': 5, 'learning_rate': 0.00019929476236568694, 'p_miss': 0.14579861602515998}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 04:42:02,052] Trial 108 finished with value: 0.1079172760155308 and parameters: {'model_name': 'VAE', 'batch_size': 189, 'iterations': 5, 'learning_rate': 0.0005780629101574842, 'p_miss': 0.16190484598928676}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 04:42:17,364] Trial 109 finished with value: 0.10478765117749597 and parameters: {'model_name': 'VAE', 'batch_size': 378, 'iterations': 3, 'learning_rate': 0.00038212683871587485, 'p_miss': 0.1475470937425104}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 04:42:23,298] Trial 110 finished with value: 0.3399746020609796 and parameters: {'model_name': 'GAIN', 'batch_size': 384, 'hint_rate': 0.7025178787238389, 'alpha': 65, 'iterations': 3, 'learning_rate': 0.00038908706145965983, 'p_miss': 0.1473806609096663}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:02:37,518] Trial 31 finished with value: 0.20691093078758632 and parameters: {'model_name': 'VAE', 'batch_size': 1, 'iterations': 4687, 'learning_rate': 0.05725602670291587, 'p_miss': 0.027750827709037362}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:02:58,963] Trial 112 finished with value: 0.1087024280781262 and parameters: {'model_name': 'VAE', 'batch_size': 710, 'iterations': 5, 'learning_rate': 0.00030110334149033275, 'p_miss': 0.1492659532211903}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:03:00,040] Trial 113 finished with value: 0.19744156090837356 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 805, 'weights': 'uniform'}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:03:14,235] Trial 114 finished with value: 0.10601672895173345 and parameters: {'model_name': 'VAE', 'batch_size': 525, 'iterations': 3, 'learning_rate': 0.00013952897277168464, 'p_miss': 0.19603374974193735}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:03:54,497] Trial 115 finished with value: 0.11418425373094498 and parameters: {'model_name': 'VAE', 'batch_size': 504, 'iterations': 10, 'learning_rate': 0.00013798647538355388, 'p_miss': 0.13565591377629263}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:04:07,209] Trial 116 finished with value: 0.10183739002488364 and parameters: {'model_name': 'VAE', 'batch_size': 376, 'iterations': 3, 'learning_rate': 0.00019641166140029539, 'p_miss': 0.20260161305799076}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:04:15,933] Trial 117 finished with value: 0.10499427490243385 and parameters: {'model_name': 'VAE', 'batch_size': 354, 'iterations': 2, 'learning_rate': 0.0001960035847452141, 'p_miss': 0.2230084434467834}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:04:25,122] Trial 118 finished with value: 0.12234326869961618 and parameters: {'model_name': 'VAE', 'batch_size': 346, 'iterations': 2, 'learning_rate': 0.00017997018472614332, 'p_miss': 0.21169464935058657}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:04:41,240] Trial 119 finished with value: 0.10349573792626832 and parameters: {'model_name': 'VAE', 'batch_size': 384, 'iterations': 4, 'learning_rate': 0.00020846966556817333, 'p_miss': 0.22570217581813298}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:04:58,440] Trial 120 finished with value: 0.10439254997144772 and parameters: {'model_name': 'VAE', 'batch_size': 633, 'iterations': 4, 'learning_rate': 0.00021026151000855153, 'p_miss': 0.2252296989658063}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:19:45,873] Trial 25 finished with value: 0.19421945630625806 and parameters: {'model_name': 'VAE', 'batch_size': 1, 'iterations': 5223, 'learning_rate': 0.00012288347249401154, 'p_miss': 0.01619086059284003}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:24:26,778] Trial 28 finished with value: 0.20142785868053728 and parameters: {'model_name': 'VAE', 'batch_size': 1, 'iterations': 5195, 'learning_rate': 0.07527277990263483, 'p_miss': 0.017832199555721018}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:29:12,721] Trial 122 finished with value: 0.21889599001480037 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 2, 'imputation_order': 'random'}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:29:34,361] Trial 124 finished with value: 0.11316028161575069 and parameters: {'model_name': 'VAE', 'batch_size': 767, 'iterations': 4, 'learning_rate': 0.00024749181325011175, 'p_miss': 0.22672854876713283}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:29:34,682] Trial 125 finished with value: 0.1991022136428366 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'mean'}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:29:51,921] Trial 126 finished with value: 0.10708099236601439 and parameters: {'model_name': 'VAE', 'batch_size': 401, 'iterations': 4, 'learning_rate': 0.0001901340733142096, 'p_miss': 0.2417525750130454}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:30:19,633] Trial 127 finished with value: 0.10782631333255602 and parameters: {'model_name': 'VAE', 'batch_size': 665, 'iterations': 6, 'learning_rate': 0.00020700368059337404, 'p_miss': 0.255519222451943}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:30:32,514] Trial 128 finished with value: 0.10700401783781688 and parameters: {'model_name': 'VAE', 'batch_size': 575, 'iterations': 3, 'learning_rate': 0.00011361477523369043, 'p_miss': 0.22024332344946926}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:30:56,475] Trial 129 finished with value: 0.10367515686521907 and parameters: {'model_name': 'VAE', 'batch_size': 268, 'iterations': 5, 'learning_rate': 0.0001607658901825064, 'p_miss': 0.23473924432605717}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:31:16,894] Trial 130 finished with value: 0.10341631179908692 and parameters: {'model_name': 'VAE', 'batch_size': 273, 'iterations': 5, 'learning_rate': 0.00017027160716635056, 'p_miss': 0.23494328712018883}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:33:34,648] Trial 123 finished with value: 0.21844129422493513 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'RFR', 'initial_strategy': 'constant', 'n_nearest_features': 2, 'imputation_order': 'random'}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:36:30,813] Trial 71 finished with value: 0.20587943809526674 and parameters: {'model_name': 'VAE', 'batch_size': 289, 'iterations': 4656, 'learning_rate': 0.001257002875771033, 'p_miss': 0.1386601555851628}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:36:49,308] Trial 133 finished with value: 0.10788157194075057 and parameters: {'model_name': 'VAE', 'batch_size': 228, 'iterations': 5, 'learning_rate': 0.00016582436996389217, 'p_miss': 0.23589078983969555}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:37:21,675] Trial 134 finished with value: 0.10529716731670417 and parameters: {'model_name': 'VAE', 'batch_size': 441, 'iterations': 9, 'learning_rate': 0.0002541890605203302, 'p_miss': 0.19981584289639057}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:46:23,834] Trial 132 finished with value: 0.11402241250191432 and parameters: {'model_name': 'VAE', 'batch_size': 267, 'iterations': 181, 'learning_rate': 0.00015984321774932563, 'p_miss': 0.23426100687082357}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:58:50,273] Trial 135 finished with value: 0.3234272947536537 and parameters: {'model_name': 'GAIN', 'batch_size': 283, 'hint_rate': 0.26316736706594723, 'alpha': 100, 'iterations': 861, 'learning_rate': 0.00016441914570375168, 'p_miss': 0.24853049677717076}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 05:59:14,771] Trial 137 finished with value: 0.11101715618150546 and parameters: {'model_name': 'VAE', 'batch_size': 363, 'iterations': 6, 'learning_rate': 0.0001231942337411083, 'p_miss': 0.22190071217874216}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 06:01:17,904] Trial 29 finished with value: 0.2126469530389712 and parameters: {'model_name': 'VAE', 'batch_size': 1, 'iterations': 5639, 'learning_rate': 0.06833203834170958, 'p_miss': 0.09282060302112044}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 06:01:32,185] Trial 139 finished with value: 0.10916891367874688 and parameters: {'model_name': 'VAE', 'batch_size': 3, 'iterations': 4, 'learning_rate': 0.0002684124538249226, 'p_miss': 0.21194253215780484}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 06:01:46,192] Trial 140 finished with value: 0.10642988128138883 and parameters: {'model_name': 'VAE', 'batch_size': 364, 'iterations': 3, 'learning_rate': 0.000454657024822291, 'p_miss': 0.2579556138920571}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 06:02:49,929] Trial 141 finished with value: 0.10213988745127904 and parameters: {'model_name': 'VAE', 'batch_size': 818, 'iterations': 13, 'learning_rate': 0.0003480874143848536, 'p_miss': 0.2281487821435131}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 06:03:52,929] Trial 142 finished with value: 0.10320457252452638 and parameters: {'model_name': 'VAE', 'batch_size': 772, 'iterations': 12, 'learning_rate': 0.00033783667920358123, 'p_miss': 0.2435146304809384}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 06:04:34,399] Trial 143 finished with value: 0.10359838791788775 and parameters: {'model_name': 'VAE', 'batch_size': 889, 'iterations': 9, 'learning_rate': 0.0003254163388603268, 'p_miss': 0.24417171560004103}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 06:04:39,288] Trial 36 finished with value: 0.20622605860375454 and parameters: {'model_name': 'VAE', 'batch_size': 84, 'iterations': 7188, 'learning_rate': 0.011159737668437416, 'p_miss': 0.18767071492932969}. Best is trial 106 with value: 0.10126171639455535.
running
[I 2024-10-30 06:05:52,119] Trial 145 finished with value: 0.10082850849975082 and parameters: {'model_name': 'VAE', 'batch_size': 777, 'iterations': 15, 'learning_rate': 0.0003202569030955398, 'p_miss': 0.24286712330310573}. Best is trial 145 with value: 0.10082850849975082.
running
[I 2024-10-30 06:06:36,290] Trial 146 finished with value: 0.10566745578219572 and parameters: {'model_name': 'VAE', 'batch_size': 873, 'iterations': 11, 'learning_rate': 0.0003244367248386237, 'p_miss': 0.2686235848857711}. Best is trial 145 with value: 0.10082850849975082.
running
[I 2024-10-30 06:06:57,340] Trial 144 finished with value: 0.1003923838472834 and parameters: {'model_name': 'VAE', 'batch_size': 888, 'iterations': 27, 'learning_rate': 0.0003269573969767164, 'p_miss': 0.24148462547548113}. Best is trial 144 with value: 0.1003923838472834.
running
[I 2024-10-30 06:07:44,648] Trial 147 finished with value: 0.10751294602436683 and parameters: {'model_name': 'VAE', 'batch_size': 611, 'iterations': 16, 'learning_rate': 0.00033305014825319357, 'p_miss': 0.22963480617648308}. Best is trial 144 with value: 0.1003923838472834.
running
[I 2024-10-30 06:08:58,573] Trial 148 finished with value: 0.10498494744097449 and parameters: {'model_name': 'VAE', 'batch_size': 827, 'iterations': 29, 'learning_rate': 0.0004218421660429914, 'p_miss': 0.24073729741964747}. Best is trial 144 with value: 0.1003923838472834.
running
[I 2024-10-30 06:10:03,869] Trial 150 finished with value: 0.10604969571648097 and parameters: {'model_name': 'VAE', 'batch_size': 495, 'iterations': 13, 'learning_rate': 0.0006929392100468491, 'p_miss': 0.24719295543553088}. Best is trial 144 with value: 0.1003923838472834.
running
[I 2024-10-30 06:10:22,447] Trial 149 finished with value: 0.10606808197230322 and parameters: {'model_name': 'VAE', 'batch_size': 834, 'iterations': 37, 'learning_rate': 0.00041823058527963976, 'p_miss': 0.24069171665987282}. Best is trial 144 with value: 0.1003923838472834.
running
[I 2024-10-30 06:10:24,536] Trial 152 finished with value: 0.2508265547612572 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 2228, 'weights': 'distance'}. Best is trial 144 with value: 0.1003923838472834.
running
[I 2024-10-30 06:11:17,629] Trial 151 finished with value: 0.09996016949820001 and parameters: {'model_name': 'VAE', 'batch_size': 967, 'iterations': 15, 'learning_rate': 0.0005037938032143827, 'p_miss': 0.25826882805599577}. Best is trial 151 with value: 0.09996016949820001.
running
[I 2024-10-30 06:11:43,976] Trial 153 finished with value: 0.10509739523998704 and parameters: {'model_name': 'VAE', 'batch_size': 963, 'iterations': 15, 'learning_rate': 0.000512583653144538, 'p_miss': 0.278672710339988}. Best is trial 151 with value: 0.09996016949820001.
running
[I 2024-10-30 06:12:26,600] Trial 155 finished with value: 0.10826644837861088 and parameters: {'model_name': 'VAE', 'batch_size': 778, 'iterations': 12, 'learning_rate': 0.00026330167756276855, 'p_miss': 0.23419501002062507}. Best is trial 151 with value: 0.09996016949820001.
running
[I 2024-10-30 06:12:58,038] Trial 154 finished with value: 0.10777361630673706 and parameters: {'model_name': 'VAE', 'batch_size': 733, 'iterations': 28, 'learning_rate': 0.000281473917229305, 'p_miss': 0.27316153104102264}. Best is trial 151 with value: 0.09996016949820001.
running
[I 2024-10-30 06:13:38,884] Trial 157 finished with value: 0.11299137238708043 and parameters: {'model_name': 'VAE', 'batch_size': 953, 'iterations': 8, 'learning_rate': 0.0003509651857958728, 'p_miss': 0.2531775648536848}. Best is trial 151 with value: 0.09996016949820001.
running
[I 2024-10-30 06:14:09,065] Trial 156 finished with value: 0.10727033594444538 and parameters: {'model_name': 'VAE', 'batch_size': 700, 'iterations': 26, 'learning_rate': 0.0003470384942750933, 'p_miss': 0.24489491492962578}. Best is trial 151 with value: 0.09996016949820001.
running
[I 2024-10-30 06:14:54,342] Trial 158 finished with value: 0.11518629198641298 and parameters: {'model_name': 'VAE', 'batch_size': 570, 'iterations': 18, 'learning_rate': 0.00046942609535324434, 'p_miss': 0.24680232688094642}. Best is trial 151 with value: 0.09996016949820001.
running
[I 2024-10-30 06:15:31,305] Trial 160 finished with value: 0.09988730366247614 and parameters: {'model_name': 'VAE', 'batch_size': 486, 'iterations': 9, 'learning_rate': 0.00023406404188249958, 'p_miss': 0.2604566228392476}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:15:40,570] Trial 159 finished with value: 0.10525229069594177 and parameters: {'model_name': 'VAE', 'batch_size': 562, 'iterations': 20, 'learning_rate': 0.00023425501924804882, 'p_miss': 0.2628627215553616}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:15:40,981] Trial 162 finished with value: 0.32225132611977736 and parameters: {'model_name': 'SimpleImputer', 'strategy': 'median'}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:16:21,953] Trial 161 finished with value: 0.12256107934625113 and parameters: {'model_name': 'VAE', 'batch_size': 437, 'iterations': 9, 'learning_rate': 0.00022445152157959716, 'p_miss': 0.28168168463887483}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:16:27,954] Trial 163 finished with value: 0.10853953751523164 and parameters: {'model_name': 'VAE', 'batch_size': 451, 'iterations': 9, 'learning_rate': 0.003154886918318527, 'p_miss': 0.2587674172286698}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:17:14,915] Trial 164 finished with value: 0.10338124466466447 and parameters: {'model_name': 'VAE', 'batch_size': 994, 'iterations': 11, 'learning_rate': 0.00028959355771762685, 'p_miss': 0.25919200843626017}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:17:42,018] Trial 165 finished with value: 0.10105155255316642 and parameters: {'model_name': 'VAE', 'batch_size': 683, 'iterations': 14, 'learning_rate': 0.0002923077875894437, 'p_miss': 0.25128285987014004}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:18:19,238] Trial 166 finished with value: 0.1004865852964727 and parameters: {'model_name': 'VAE', 'batch_size': 690, 'iterations': 12, 'learning_rate': 0.0003007671279802901, 'p_miss': 0.238880961484717}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:19:00,063] Trial 167 finished with value: 0.10941840200093658 and parameters: {'model_name': 'VAE', 'batch_size': 689, 'iterations': 14, 'learning_rate': 0.0002881970770734636, 'p_miss': 0.23045598656570818}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:19:37,682] Trial 168 finished with value: 0.10943913743053638 and parameters: {'model_name': 'VAE', 'batch_size': 692, 'iterations': 13, 'learning_rate': 0.00029103023234226485, 'p_miss': 0.2542894051521625}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:19:49,801] Trial 169 finished with value: 0.10432539989449334 and parameters: {'model_name': 'VAE', 'batch_size': 844, 'iterations': 11, 'learning_rate': 0.0003912152910300532, 'p_miss': 0.25314673095387186}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:20:30,105] Trial 170 finished with value: 0.10444715561066698 and parameters: {'model_name': 'VAE', 'batch_size': 871, 'iterations': 11, 'learning_rate': 0.0002442347465513541, 'p_miss': 0.26124377072630556}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:21:03,220] Trial 172 finished with value: 0.32445412043843624 and parameters: {'model_name': 'GAIN', 'batch_size': 616, 'hint_rate': 0.726797413502307, 'alpha': 17, 'iterations': 18, 'learning_rate': 0.0003142511246776052, 'p_miss': 0.241217271154222}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:23:38,308] Trial 173 finished with value: 0.10693003154488245 and parameters: {'model_name': 'VAE', 'batch_size': 990, 'iterations': 35, 'learning_rate': 0.00039379252426741156, 'p_miss': 0.2687360572416242}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:24:45,074] Trial 171 finished with value: 0.10760416913701479 and parameters: {'model_name': 'VAE', 'batch_size': 978, 'iterations': 69, 'learning_rate': 0.00024235609819255657, 'p_miss': 0.24093719242581516}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:25:21,976] Trial 175 finished with value: 0.10599858223604777 and parameters: {'model_name': 'VAE', 'batch_size': 750, 'iterations': 7, 'learning_rate': 0.0002654705773629106, 'p_miss': 0.24951916930091475}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:25:52,243] Trial 176 finished with value: 0.10419585892932251 and parameters: {'model_name': 'VAE', 'batch_size': 523, 'iterations': 6, 'learning_rate': 0.00022470732922531434, 'p_miss': 0.21581579978258955}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:26:53,174] Trial 177 finished with value: 0.10332349531818459 and parameters: {'model_name': 'VAE', 'batch_size': 770, 'iterations': 15, 'learning_rate': 0.0003209088106894267, 'p_miss': 0.23549745076302012}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:27:42,511] Trial 174 finished with value: 0.11085497636498527 and parameters: {'model_name': 'VAE', 'batch_size': 761, 'iterations': 54, 'learning_rate': 0.00026266499672498674, 'p_miss': 0.250326228713143}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:28:14,817] Trial 179 finished with value: 0.11012837623275853 and parameters: {'model_name': 'VAE', 'batch_size': 610, 'iterations': 8, 'learning_rate': 0.00032024794327494173, 'p_miss': 0.23082691555941973}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:28:35,514] Trial 178 finished with value: 0.11212097402795378 and parameters: {'model_name': 'VAE', 'batch_size': 783, 'iterations': 22, 'learning_rate': 0.0003289304344258151, 'p_miss': 0.22845881505101176}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:28:40,873] Trial 181 finished with value: 0.19858134563079713 and parameters: {'model_name': 'IterativeImputer', 'estimator': 'Bayesian', 'initial_strategy': 'mean', 'n_nearest_features': 4, 'imputation_order': 'roman', 'sample_posterior': False}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:30:03,422] Trial 182 finished with value: 0.11489198600534492 and parameters: {'model_name': 'VAE', 'batch_size': 642, 'iterations': 16, 'learning_rate': 0.0001951274058533089, 'p_miss': 0.23705364091260434}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:30:04,316] Trial 180 finished with value: 0.10827606945909722 and parameters: {'model_name': 'VAE', 'batch_size': 793, 'iterations': 23, 'learning_rate': 0.0003568097111933923, 'p_miss': 0.23794815150694798}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:30:48,405] Trial 183 finished with value: 0.10519995479951379 and parameters: {'model_name': 'VAE', 'batch_size': 880, 'iterations': 10, 'learning_rate': 0.0004315932238898887, 'p_miss': 0.245519812175082}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:30:49,542] Trial 185 finished with value: 0.1973958385837163 and parameters: {'model_name': 'KNNImputer', 'n_neighbors': 801, 'weights': 'uniform'}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:30:51,950] Trial 184 finished with value: 0.1048040110293085 and parameters: {'model_name': 'VAE', 'batch_size': 529, 'iterations': 10, 'learning_rate': 0.0004955844170850942, 'p_miss': 0.260030251850055}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:31:48,030] Trial 187 finished with value: 0.10517466267901013 and parameters: {'model_name': 'VAE', 'batch_size': 994, 'iterations': 14, 'learning_rate': 0.00015281686852565042, 'p_miss': 0.2350016286708939}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:31:48,854] Trial 186 finished with value: 0.10340583961843422 and parameters: {'model_name': 'VAE', 'batch_size': 530, 'iterations': 15, 'learning_rate': 0.00018322888211386266, 'p_miss': 0.23389189817773826}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:32:14,895] Trial 188 finished with value: 0.10582204919417337 and parameters: {'model_name': 'VAE', 'batch_size': 311, 'iterations': 6, 'learning_rate': 0.00018897074544104025, 'p_miss': 0.27386334734664414}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:33:25,717] Trial 190 finished with value: 0.1122763756040708 and parameters: {'model_name': 'VAE', 'batch_size': 672, 'iterations': 18, 'learning_rate': 0.00028653410445591437, 'p_miss': 0.2519748290018886}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:33:27,825] Trial 189 finished with value: 0.10955760406234656 and parameters: {'model_name': 'VAE', 'batch_size': 672, 'iterations': 19, 'learning_rate': 0.0001909752303792394, 'p_miss': 0.2210341016298626}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:34:00,961] Trial 191 finished with value: 0.13819583884293307 and parameters: {'model_name': 'VAE', 'batch_size': 20, 'iterations': 12, 'learning_rate': 0.009212828033195848, 'p_miss': 0.22489963851539713}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:34:07,388] Trial 192 finished with value: 0.10999101158490276 and parameters: {'model_name': 'VAE', 'batch_size': 431, 'iterations': 8, 'learning_rate': 0.00021348741920169367, 'p_miss': 0.24476951331096622}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:34:42,215] Trial 193 finished with value: 0.11514017312689526 and parameters: {'model_name': 'VAE', 'batch_size': 469, 'iterations': 8, 'learning_rate': 0.0002230043821697755, 'p_miss': 0.2667451187544171}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:35:07,887] Trial 194 finished with value: 0.10309762410760737 and parameters: {'model_name': 'VAE', 'batch_size': 548, 'iterations': 15, 'learning_rate': 0.0003641071523628432, 'p_miss': 0.2657710890557367}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:36:11,320] Trial 196 finished with value: 0.10734829201485092 and parameters: {'model_name': 'VAE', 'batch_size': 568, 'iterations': 14, 'learning_rate': 0.00037626220921411206, 'p_miss': 0.25673905412704995}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:36:14,396] Trial 195 finished with value: 0.20752329032101743 and parameters: {'model_name': 'VAE', 'batch_size': 588, 'iterations': 15, 'learning_rate': 0.024235915717584808, 'p_miss': 0.2145701968694787}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:37:03,113] Trial 198 finished with value: 0.10528861833551642 and parameters: {'model_name': 'VAE', 'batch_size': 780, 'iterations': 12, 'learning_rate': 0.0003025476477131604, 'p_miss': 0.24904313928268385}. Best is trial 160 with value: 0.09988730366247614.
running
[I 2024-10-30 06:37:21,386] Trial 197 finished with value: 0.11081035856660079 and parameters: {'model_name': 'VAE', 'batch_size': 842, 'iterations': 16, 'learning_rate': 0.00030867216608245764, 'p_miss': 0.2434889031280631}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 06:38:08,029] Trial 199 finished with value: 0.10377724550328274 and parameters: {'model_name': 'VAE', 'batch_size': 882, 'iterations': 16, 'learning_rate': 0.00041659565894812774, 'p_miss': 0.28179142882681135}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 07:08:18,734] Trial 85 finished with value: 0.20673757562403408 and parameters: {'model_name': 'VAE', 'batch_size': 514, 'iterations': 5350, 'learning_rate': 0.00034555256271419485, 'p_miss': 0.21258681137283417}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 07:12:30,624] Trial 30 finished with value: 0.20144962467115177 and parameters: {'model_name': 'VAE', 'batch_size': 1, 'iterations': 6910, 'learning_rate': 0.0663096679621249, 'p_miss': 0.025034438145722065}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 07:15:20,804] Trial 35 finished with value: 0.20549036038929086 and parameters: {'model_name': 'VAE', 'batch_size': 84, 'iterations': 9336, 'learning_rate': 0.01571459184660423, 'p_miss': 0.1994092184427277}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 07:37:39,608] Trial 26 finished with value: 0.2074362954703159 and parameters: {'model_name': 'VAE', 'batch_size': 1, 'iterations': 8051, 'learning_rate': 0.00010127086615471068, 'p_miss': 0.03399349291101193}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 07:40:45,283] Trial 27 finished with value: 0.20391924299512296 and parameters: {'model_name': 'VAE', 'batch_size': 1, 'iterations': 7134, 'learning_rate': 0.09798933814032672, 'p_miss': 0.012745169862811156}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 07:42:16,168] Trial 12 finished with value: 0.20451300497518926 and parameters: {'model_name': 'VAE', 'batch_size': 330, 'iterations': 7924, 'learning_rate': 0.059081390451264114, 'p_miss': 0.1303145810699228}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 07:59:09,290] Trial 131 finished with value: 0.20628883497852923 and parameters: {'model_name': 'VAE', 'batch_size': 289, 'iterations': 3032, 'learning_rate': 0.00026046270845685263, 'p_miss': 0.23270999437492051}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 08:30:50,140] Trial 86 finished with value: 0.20661926149954463 and parameters: {'model_name': 'VAE', 'batch_size': 526, 'iterations': 8249, 'learning_rate': 0.00037549941302417845, 'p_miss': 0.1944226775121254}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 08:52:36,395] Trial 111 finished with value: 0.2071290455337899 and parameters: {'model_name': 'VAE', 'batch_size': 547, 'iterations': 6608, 'learning_rate': 0.0002852371709529617, 'p_miss': 0.15074730094665478}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 08:56:54,896] Trial 102 finished with value: 0.205534762086614 and parameters: {'model_name': 'VAE', 'batch_size': 267, 'iterations': 8525, 'learning_rate': 0.00036756644693714435, 'p_miss': 0.11152805474514772}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 09:05:38,784] Trial 121 finished with value: 0.2081049381605375 and parameters: {'model_name': 'VAE', 'batch_size': 645, 'iterations': 7162, 'learning_rate': 0.0002493243703153849, 'p_miss': 0.2273031696802771}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 09:05:50,497] Trial 138 finished with value: 0.21320517937098424 and parameters: {'model_name': 'VAE', 'batch_size': 4, 'iterations': 6815, 'learning_rate': 0.00033379082292647294, 'p_miss': 0.2272627347033706}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 09:06:36,239] Trial 136 finished with value: 0.2048044028721768 and parameters: {'model_name': 'VAE', 'batch_size': 372, 'iterations': 6952, 'learning_rate': 0.0001244063336758156, 'p_miss': 0.22419112736277735}. Best is trial 160 with value: 0.09988730366247614.
[I 2024-10-30 09:08:19,415] Trial 90 finished with value: 0.2067348041367511 and parameters: {'model_name': 'VAE', 'batch_size': 944, 'iterations': 9102, 'learning_rate': 0.0002315096681514329, 'p_miss': 0.15909191260125363}. Best is trial 160 with value: 0.09988730366247614.
fit
auto fit
auto transform
0    0
1    0
2    0
3    0
4    0
dtype: int64
0    0
1    0
2    0
3    0
4    0
dtype: int64
0.09988730366247614
{'model_name': 'VAE', 'batch_size': 486, 'iterations': 9, 'learning_rate': 0.00023406404188249958, 'p_miss': 0.2604566228392476}
running experiment 2/3 - Does reconstruction give good automl predictions
Start est fit
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
  0%|          | 0/25 [00:00<?, ?it/s]Generation:   0%|          | 0/25 [00:00<?, ?it/s]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554746f7e20> 
 Negative values in data passed to MultinomialNB (input X) 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 759, in fit
    self._count(X, Y)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 881, in _count
    check_non_negative(X, "MultinomialNB (input X)")
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1689, in check_non_negative
    raise ValueError("Negative values in data passed to %s" % whom)
ValueError: Negative values in data passed to MultinomialNB (input X)

Generation:  1
Best f1_score score: 0.5203554990494932
Generation:   4%|         | 1/25 [01:23<33:35, 83.99s/it]Generation:  2
Best f1_score score: 0.5203554990494932
Generation:   8%|         | 2/25 [09:47<2:06:46, 330.70s/it]Generation:  3
Best f1_score score: 0.5203554990494932
Generation:  12%|        | 3/25 [12:11<1:30:00, 245.49s/it]Generation:  4
Best f1_score score: 0.5203554990494932
Generation:  16%|        | 4/25 [12:20<53:17, 152.28s/it]  Generation:  5
Best f1_score score: 0.5203554990494932
Generation:  20%|        | 5/25 [12:47<35:41, 107.07s/it]Generation:  6
Best f1_score score: 0.5203554990494932
Generation:  24%|       | 6/25 [13:41<28:06, 88.79s/it] Generation:  7
Best f1_score score: 0.5203554990494932
Generation:  28%|       | 7/25 [13:53<19:09, 63.86s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155467a88370> 
 Negative values in data passed to MultinomialNB (input X) 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 759, in fit
    self._count(X, Y)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 881, in _count
    check_non_negative(X, "MultinomialNB (input X)")
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1689, in check_non_negative
    raise ValueError("Negative values in data passed to %s" % whom)
ValueError: Negative values in data passed to MultinomialNB (input X)

WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554651144f0> 
 Negative values in data passed to MultinomialNB (input X) 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 759, in fit
    self._count(X, Y)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 881, in _count
    check_non_negative(X, "MultinomialNB (input X)")
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1689, in check_non_negative
    raise ValueError("Negative values in data passed to %s" % whom)
ValueError: Negative values in data passed to MultinomialNB (input X)

WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155465b517e0> 
 Negative values in data passed to MultinomialNB (input X) 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 759, in fit
    self._count(X, Y)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 881, in _count
    check_non_negative(X, "MultinomialNB (input X)")
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1689, in check_non_negative
    raise ValueError("Negative values in data passed to %s" % whom)
ValueError: Negative values in data passed to MultinomialNB (input X)

Generation:  8
Best f1_score score: 0.5203554990494932
Generation:  32%|      | 8/25 [14:16<14:22, 50.71s/it]Generation:  9
Best f1_score score: 0.5203554990494932
Generation:  36%|      | 9/25 [14:26<10:11, 38.23s/it]Generation:  10
Best f1_score score: 0.5203554990494932
Generation:  40%|      | 10/25 [14:42<07:48, 31.24s/it]Generation:  11
Best f1_score score: 0.5203554990494932
Generation:  44%|     | 11/25 [15:09<06:57, 29.85s/it]Generation:  12
Best f1_score score: 0.5203554990494932
Generation:  48%|     | 12/25 [15:21<05:18, 24.53s/it]Generation:  13
Best f1_score score: 0.5203554990494932
Generation:  52%|    | 13/25 [15:37<04:22, 21.89s/it]Generation:  14
Best f1_score score: 0.5222663439911117
Generation:  56%|    | 14/25 [15:57<03:53, 21.20s/it]Generation:  15
Best f1_score score: 0.5222663439911117
Generation:  60%|    | 15/25 [16:11<03:12, 19.24s/it]Generation:  16
Best f1_score score: 0.5222663439911117
Generation:  64%|   | 16/25 [16:29<02:50, 18.93s/it]Generation:  17
Best f1_score score: 0.5222663439911117
Generation:  68%|   | 17/25 [16:43<02:18, 17.37s/it]Generation:  18
Best f1_score score: 0.5222663439911117
Generation:  72%|  | 18/25 [16:58<01:57, 16.73s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155467a055a0> 
 Negative values in data passed to MultinomialNB (input X) 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 759, in fit
    self._count(X, Y)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 881, in _count
    check_non_negative(X, "MultinomialNB (input X)")
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1689, in check_non_negative
    raise ValueError("Negative values in data passed to %s" % whom)
ValueError: Negative values in data passed to MultinomialNB (input X)

Generation:  19
Best f1_score score: 0.5232212629473892
Generation:  76%|  | 19/25 [17:17<01:43, 17.17s/it]Generation:  20
Best f1_score score: 0.5235647248928512
Generation:  80%|  | 20/25 [17:33<01:24, 16.84s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155467ba6710> 
 Negative values in data passed to MultinomialNB (input X) 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 759, in fit
    self._count(X, Y)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 881, in _count
    check_non_negative(X, "MultinomialNB (input X)")
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1689, in check_non_negative
    raise ValueError("Negative values in data passed to %s" % whom)
ValueError: Negative values in data passed to MultinomialNB (input X)

Generation:  21
Best f1_score score: 0.5235647248928512
Generation:  84%| | 21/25 [17:54<01:12, 18.17s/it]Generation:  22
Best f1_score score: 0.5235647248928512
Generation:  88%| | 22/25 [18:12<00:54, 18.19s/it]Generation:  23
Best f1_score score: 0.5235647248928512
Generation:  92%|| 23/25 [18:55<00:51, 25.57s/it]Generation:  24
Best f1_score score: 0.5235647248928512
Generation:  96%|| 24/25 [19:10<00:22, 22.49s/it]Generation:  25
Best f1_score score: 0.5235647248928512
Generation: 100%|| 25/25 [19:34<00:00, 22.78s/it]Generation: 100%|| 25/25 [19:37<00:00, 47.11s/it]
2024-10-30 09:28:07,677 - distributed.scheduler - ERROR - Removing worker 'tcp://127.0.0.1:46813' caused the cluster to lose scattered data, which can't be recovered: {'ndarray-2c00092ba1cd743626b416db9c976854', 'ndarray-e1048b2ec0241b6d2cbe3c734586cc88'} (stimulus_id='handle-worker-cleanup-1730305687.6779106')
Fitted
Pipeline(steps=[('xgbclassifier',
                 XGBClassifier(base_score=None, booster=None, callbacks=None,
                               colsample_bylevel=None, colsample_bynode=None,
                               colsample_bytree=None, device=None,
                               early_stopping_rounds=None,
                               enable_categorical=False, eval_metric=None,
                               feature_types=None, gamma=0.0012827569699,
                               grow_policy=None, importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.0163902946673, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=11,
                               max_leaves=None, min_child_weight=12,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=1,
                               nthread=1, num_parallel_tree=None, ...))])
score start
train score: {'auroc': 0.783157717410992, 'accuracy': 0.7147498375568551, 'balanced_accuracy': 0.7147498375568551, 'logloss': 0.6361356399313802, 'f1': 0.7147468266781958}
original test score: {'auroc': 0.5230291786135941, 'accuracy': 0.5038961038961038, 'balanced_accuracy': 0.5038961038961038, 'logloss': 0.6930874998026203, 'f1': 0.4893402777777778}
imputed test score: {'auroc': 0.4774329566537359, 'accuracy': 0.4753246753246753, 'balanced_accuracy': 0.47532467532467526, 'logloss': 0.7055926143760691, 'f1': 0.47481445965383806}
score end
check intended
EXP2 Finished
running experiment 3/3 - What is the best automl settings?
<tpot2.search_spaces.pipelines.sequential.SequentialPipeline object at 0x1554350145e0>
Start tpot fit
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
Version 0.1.7a0 of tpot2 is outdated. Version 0.1.8a0 was released Tuesday September 17, 2024.
  0%|          | 0/25 [00:00<?, ?it/s]Generation:   0%|          | 0/25 [00:00<?, ?it/s]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474cd4d90> 
 Input X contains NaN.
KNeighborsClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neighbors/_classification.py", line 238, in fit
    return self._fit(X, y)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neighbors/_base.py", line 475, in _fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
KNeighborsClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474d1e4a0> 
 Input X contains NaN.
KNeighborsClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neighbors/_classification.py", line 238, in fit
    return self._fit(X, y)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/neighbors/_base.py", line 475, in _fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
KNeighborsClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  1
Best f1_score score: 0.5078525298076118
Generation:   4%|         | 1/25 [00:27<10:55, 27.31s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554745a6b00> 
 Negative values in data passed to MultinomialNB (input X) 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 759, in fit
    self._count(X, Y)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 881, in _count
    check_non_negative(X, "MultinomialNB (input X)")
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1689, in check_non_negative
    raise ValueError("Negative values in data passed to %s" % whom)
ValueError: Negative values in data passed to MultinomialNB (input X)

Generation:  2
Best f1_score score: 0.5168901562045922
Generation:   8%|         | 2/25 [00:40<07:16, 18.96s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15547428f760> 
 Negative values in data passed to MultinomialNB (input X) 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 759, in fit
    self._count(X, Y)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 881, in _count
    check_non_negative(X, "MultinomialNB (input X)")
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1689, in check_non_negative
    raise ValueError("Negative values in data passed to %s" % whom)
ValueError: Negative values in data passed to MultinomialNB (input X)

Generation:  3
Best f1_score score: 0.5181053721384781
Generation:  12%|        | 3/25 [00:52<05:48, 15.86s/it]Generation:  4
Best f1_score score: 0.5186462013367179
Generation:  16%|        | 4/25 [02:36<17:44, 50.70s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554742e7220> 
 Input X contains NaN.
LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py", line 1223, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  5
Best f1_score score: 0.5186462013367179
Generation:  20%|        | 5/25 [02:46<12:00, 36.00s/it]Generation:  6
Best f1_score score: 0.5212577608306648
Generation:  24%|       | 6/25 [07:49<40:07, 126.72s/it]Generation:  7
Best f1_score score: 0.5212577608306648
Generation:  28%|       | 7/25 [08:03<27:00, 90.02s/it] WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545fe26f50> 
 Input X contains NaN.
GaussianNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 263, in fit
    return self._partial_fit(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 423, in _partial_fit
    X, y = self._validate_data(X, y, reset=first_call)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
GaussianNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474d38880> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING AN INDIVIDUAL TIMED OUT: 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474d39870> 

Generation:  8
Best f1_score score: 0.5212577608306648
Generation:  32%|      | 8/25 [18:08<1:11:55, 253.87s/it]Generation:  9
Best f1_score score: 0.5212577608306648
Generation:  36%|      | 9/25 [19:07<51:28, 193.01s/it]  WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545def1a20> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545316b550> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  10
Best f1_score score: 0.5212577608306648
Generation:  40%|      | 10/25 [20:52<41:24, 165.65s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545dd17c10> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155452fba1a0> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  11
Best f1_score score: 0.5306005672261255
Generation:  44%|     | 11/25 [21:39<30:13, 129.51s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155453009870> 
 Input X contains NaN.
GaussianNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 263, in fit
    return self._partial_fit(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/naive_bayes.py", line 423, in _partial_fit
    X, y = self._validate_data(X, y, reset=first_call)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
GaussianNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  12
Best f1_score score: 0.5306005672261255
Generation:  48%|     | 12/25 [24:40<31:26, 145.09s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545ff536d0> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554742ad450> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155452c7a440> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  13
Best f1_score score: 0.5306005672261255
Generation:  52%|    | 13/25 [24:48<20:43, 103.61s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545364cfa0> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  14
Best f1_score score: 0.5306005672261255
Generation:  56%|    | 14/25 [25:58<17:05, 93.26s/it] WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554529d31c0> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  15
Best f1_score score: 0.5306005672261255
Generation:  60%|    | 15/25 [27:42<16:07, 96.74s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474cd5f60> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  16
Best f1_score score: 0.5306005672261255
Generation:  64%|   | 16/25 [29:32<15:04, 100.51s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545dd32110> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15545fbce830> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  17
Best f1_score score: 0.5306005672261255
Generation:  68%|   | 17/25 [31:24<13:53, 104.22s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155452a800d0> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  18
Best f1_score score: 0.5306005672261255
Generation:  72%|  | 18/25 [32:17<10:21, 88.82s/it] Generation:  19
Best f1_score score: 0.5306005672261255
Generation:  76%|  | 19/25 [32:30<06:35, 65.86s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155474d669b0> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  20
Best f1_score score: 0.5306005672261255
Generation:  80%|  | 20/25 [33:17<05:01, 60.35s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155454a5a860> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  21
Best f1_score score: 0.5306005672261255
Generation:  84%| | 21/25 [33:41<03:17, 49.48s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x1554573958a0> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x15547431c7c0> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  22
Best f1_score score: 0.5306005672261255
Generation:  88%| | 22/25 [33:56<01:56, 38.97s/it]WARNING THIS INDIVIDUAL CAUSED AND EXCEPTION 
 <tpot2.search_spaces.pipelines.sequential.SequentialPipelineIndividual object at 0x155452836350> 
 Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values 
 Traceback (most recent call last):
  File "/common/ketrong/tpotexp/tpot2/tpot2/utils/eval_utils.py", line 53, in objective_nan_wrapper
    value = func_timeout.func_timeout(timeout, objective_function, args=[individual], kwargs=objective_kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/dafunc.py", line 108, in func_timeout
    raise_exception(exception)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/func_timeout/py3_raise.py", line 7, in raise_exception
    raise exception[0] from None
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator.py", line 623, in objective_function
    return objective_function_generator(
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/estimator_utils.py", line 55, in objective_function_generator
    cv_obj_scores = cross_val_score_objective(sklearn.base.clone(pipeline),x,y,scorers=scorers, cv=cv , fold=step)
  File "/common/ketrong/tpotexp/tpot2/tpot2/tpot_estimator/cross_val_utils.py", line 28, in cross_val_score_objective
    this_fold_pipeline.fit(X_train,y_train)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/pipeline.py", line 473, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py", line 133, in fit
    X, y = self._validate_data(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/base.py", line 650, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1301, in check_X_y
    X = check_array(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1064, in check_array
    _assert_all_finite(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 123, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/common/ketrong/tpotexp/env/lib/python3.10/site-packages/sklearn/utils/validation.py", line 172, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values

Generation:  23
Best f1_score score: 0.5306005672261255
Generation:  92%|| 23/25 [35:43<01:59, 59.54s/it]Generation:  24
Best f1_score score: 0.5306005672261255
Generation:  96%|| 24/25 [35:53<00:44, 44.62s/it]Generation:  25
Best f1_score score: 0.5306005672261255
Generation: 100%|| 25/25 [36:49<00:00, 47.99s/it]Generation: 100%|| 25/25 [36:49<00:00, 88.38s/it]
2024-10-30 10:05:07,202 - distributed.scheduler - ERROR - Removing worker 'tcp://127.0.0.1:35367' caused the cluster to lose scattered data, which can't be recovered: {'ndarray-2c00092ba1cd743626b416db9c976854', 'DataFrame-08e1b058d141830c9d7df46d3b57064c'} (stimulus_id='handle-worker-cleanup-1730307907.2027016')
Fitted
Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='most_frequent')),
                ('adaboostclassifier',
                 AdaBoostClassifier(learning_rate=0.023266330608,
                                    n_estimators=262))])
transform worked
try transform
transform worked
score start
train score: {'auroc': 0.576202828685074, 'accuracy': 0.5565302144249513, 'balanced_accuracy': 0.5565302144249513, 'logloss': 0.6910483446144323, 'f1': 0.5506196872016877}
test score: {'auroc': 0.47724742789677854, 'accuracy': 0.4922077922077922, 'balanced_accuracy': 0.4922077922077922, 'logloss': 0.6927262837965663, 'f1': 0.48295611644349645}
original test score: {'auroc': 0.4770956316410862, 'accuracy': 0.4935064935064935, 'balanced_accuracy': 0.4935064935064935, 'logloss': 0.6931173248658935, 'f1': 0.46353541416566624}
score end
871
lvl
0.5
type
MCAR
num_run
1
class_full
finished
all finished
full run takes
9.31374471174346
hours
DONE
